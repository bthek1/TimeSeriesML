{"title":"Nixtla - Neural Advanced Forecast","markdown":{"yaml":{"description":"Model training, evaluation and selection for [multiple time series](https://nixtla.github.io/neuralforecast/models.html)","output-file":"nixtla - neural_advanced.html","skip_exec":true,"skip_showdoc":true,"title":"Nixtla - Neural Advanced Forecast"},"headingText":"Prerequesites","containsRefs":false,"markdown":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n:::{.callout-warning collapse=\"true\"}\nThis Guide assumes basic familiarity with NeuralForecast. For a minimal example visit the [Quick Start](./Getting_Started.ipynb)\n:::\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series. \n\nDuring this guide you will gain familiary with the core `NueralForecast`class and some relevant methods like `NeuralForecast.fit`, `NeuralForecast.predict`, and `StatsForecast.cross_validation.`\n\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset. \n\nWe will model each time series globally Therefore, you will train a set of models for the whole dataset, and then select the best model for each individual time series. NeuralForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\n\n\n**Outline:**\n\n1. Install packages.\n1. Read the data.\n2. Explore the data.\n3. Train many models globally for the entire dataset. \n4. Evaluate the model's performance using cross-validation. \n5. Select the best model for every unique time series.\n\n:::{.callout-tip collapse=true}\n## Not Covered in this guide\n\n* Using external regressors or exogenous variables\n    * Follow this tutorial to [include exogenous variables](./Exogenous_Variables.ipynb) like weather or holidays or static variables like category or family. \n\n* Probabilistic forecasting\n    * Follow this tutorial to [generate probabilistic forecasts](./LongHorizon_Probabilistic.ipynb)\n\n* Transfer Learning\n    * Train a model and use it to forecast on different data using [this tutorial](./Transfer_Learning.ipynb)\n:::\n\n::: {.callout-tip}\nYou can use Colab to run this Notebook interactively <a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Getting_Started_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n::: \n\n::: {.callout-warning}\nTo reduce the computation time, it is recommended to use GPU. Using Colab, do not forget to activate it. Just go to `Runtime>Change runtime type` and select GPU as hardware accelerator.\n::: \n\n## 1. Install libraries\n\nWe assume you have `NeuralForecast` already installed. Check this guide for instructions on [how to install NeuralForecast](./Installation.ipynb).\n\nAdditionally, we will install `s3fs` to read from the S3 Filesystem of AWS, `statsforecast` for plotting, and `datasetsforecast` for common error metrics like MAE or MASE.\n\nInstall the necessary packages using `pip install statsforecast s3fs datasetsforecast`\n``\n\n%%capture\n! pip install statsforecast s3fs datasetsforecast\n\n%%capture\n! pip install git+https://github.com/Nixtla/neuralforecast.git@main\n\n## 2. Read the data\n\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes `.csv`. \n\nThe input to `NeuralForecast` is always a data frame in [long format](https://www.theanalysisfactor.com/wide-and-long-data/) with three columns: `unique_id`, `ds` and `y`:\n\n* The `unique_id` (string, int or category) represents an identifier for the series. \n\n* The `ds` (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n\n* The `y` (numeric) represents the measurement we wish to forecast. \nWe will rename the \n\nThis data set already satisfies the requirement.  \n\nDepending on your internet connection, this step should take around 10 seconds. \n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility's sake, we will select only 10 unique IDs. Depending on your processing infrastructure feel free to select more or less series. \n\n:::{.callout-note}\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n:::\n\n## 3. Explore Data with the plot method of StatsForecast\n\nPlot some series using the `plot` method from the `StatsForecast` class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n:::{.callout-note}\nThe `StatsForecast.plot` method uses Plotly as a defaul engine. You can change to MatPlotLib by setting `engine=\"matplotlib\"`. \n:::\n\n## 4. Train multiple models for many series\n\n`NeuralForecast` can train many models on many time series globally and efficiently. \n\nEach `Auto` model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\n\nFirst, we create a custom search space for the `AutoNHITS` and `AutoLSTM` models. Search spaces are specified with dictionaries, where keys corresponds to the model's hyperparameter and the value is a `Tune` function to specify how the hyperparameter will be sampled. For example, use `randint` to sample integers uniformly, and `choice` to sample values of a list.\n\nTo instantiate an `Auto` model you need to define:\n\n* `h`: forecasting horizon.\n* `loss`: training and validation loss from `neuralforecast.losses.pytorch`.\n* `config`: hyperparameter search space. If `None`, the `Auto` class will use a pre-defined suggested hyperparameter space.\n* `search_alg`: search algorithm (from `tune.search`), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\n* `num_samples`: number of configurations explored.\n\nIn this example we set horizon `h` as 48, use the `MQLoss` distribution loss for training and validation, and use the default search algorithm. \n\n:::{.callout-tip}\nThe number of samples, `num_samples`, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting `num_samples` higher than 20.\n:::\n\nNext, we use the `Neuralforecast` class to train the `Auto` model. In this step, `Auto` models will automatically perform hyperparameter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nNext, we use the `predict` method to forecast the next 48 days using the optimal hyperparameters.\n\nThe `StatsForecast.plot` allows for further customization. For example, plot the results of the different models and unique ids. \n\n## 5. Evaluate the model's performance\n\n\nIn previous steps, we've taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\n\nWith time series data, **Cross Validation** is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model's predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\n\nThe following graph depicts such a Cross Validation Strategy:\n\n![](https://raw.githubusercontent.com/Nixtla/statsforecast/main/nbs/imgs/ChainedWindows.gif)\n\n:::{.callout-tip}\nSetting `n_windows=1` mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set. \n:::\n\nThe `cross_validation` method from the `NeuralForecast` class takes the following arguments.\n\n- `df`: training data frame\n\n- `step_size` (int): step size between each window. In other words: how often do you want to run the forecasting processes.\n- `n_windows` (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\nThe `cv_df` object is a new data frame that includes the following columns:\n\n- `unique_id`: identifies each time series\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\nNow, let's evaluate the models' performance.\n\n:::{.callout-warning}\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely [hard to judge](\"https://blog.blueyonder.com/mean-absolute-percentage-error-mape-has-served-its-duty-and-should-now-retire/\") and not useful to assess forecasting quality.\n:::\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric. \n\nCreate a summary table with a model column and the number of series where that model performs best. \n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\n## 6. Select the best model for every unique series\n\nDefine a utility function that takes your forecast's data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nPlot the results. \n","srcMarkdownNoYaml":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n:::{.callout-warning collapse=\"true\"}\n## Prerequesites\nThis Guide assumes basic familiarity with NeuralForecast. For a minimal example visit the [Quick Start](./Getting_Started.ipynb)\n:::\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series. \n\nDuring this guide you will gain familiary with the core `NueralForecast`class and some relevant methods like `NeuralForecast.fit`, `NeuralForecast.predict`, and `StatsForecast.cross_validation.`\n\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset. \n\nWe will model each time series globally Therefore, you will train a set of models for the whole dataset, and then select the best model for each individual time series. NeuralForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\n\n\n**Outline:**\n\n1. Install packages.\n1. Read the data.\n2. Explore the data.\n3. Train many models globally for the entire dataset. \n4. Evaluate the model's performance using cross-validation. \n5. Select the best model for every unique time series.\n\n:::{.callout-tip collapse=true}\n## Not Covered in this guide\n\n* Using external regressors or exogenous variables\n    * Follow this tutorial to [include exogenous variables](./Exogenous_Variables.ipynb) like weather or holidays or static variables like category or family. \n\n* Probabilistic forecasting\n    * Follow this tutorial to [generate probabilistic forecasts](./LongHorizon_Probabilistic.ipynb)\n\n* Transfer Learning\n    * Train a model and use it to forecast on different data using [this tutorial](./Transfer_Learning.ipynb)\n:::\n\n::: {.callout-tip}\nYou can use Colab to run this Notebook interactively <a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Getting_Started_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n::: \n\n::: {.callout-warning}\nTo reduce the computation time, it is recommended to use GPU. Using Colab, do not forget to activate it. Just go to `Runtime>Change runtime type` and select GPU as hardware accelerator.\n::: \n\n## 1. Install libraries\n\nWe assume you have `NeuralForecast` already installed. Check this guide for instructions on [how to install NeuralForecast](./Installation.ipynb).\n\nAdditionally, we will install `s3fs` to read from the S3 Filesystem of AWS, `statsforecast` for plotting, and `datasetsforecast` for common error metrics like MAE or MASE.\n\nInstall the necessary packages using `pip install statsforecast s3fs datasetsforecast`\n``\n\n%%capture\n! pip install statsforecast s3fs datasetsforecast\n\n%%capture\n! pip install git+https://github.com/Nixtla/neuralforecast.git@main\n\n## 2. Read the data\n\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes `.csv`. \n\nThe input to `NeuralForecast` is always a data frame in [long format](https://www.theanalysisfactor.com/wide-and-long-data/) with three columns: `unique_id`, `ds` and `y`:\n\n* The `unique_id` (string, int or category) represents an identifier for the series. \n\n* The `ds` (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n\n* The `y` (numeric) represents the measurement we wish to forecast. \nWe will rename the \n\nThis data set already satisfies the requirement.  \n\nDepending on your internet connection, this step should take around 10 seconds. \n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility's sake, we will select only 10 unique IDs. Depending on your processing infrastructure feel free to select more or less series. \n\n:::{.callout-note}\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n:::\n\n## 3. Explore Data with the plot method of StatsForecast\n\nPlot some series using the `plot` method from the `StatsForecast` class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n:::{.callout-note}\nThe `StatsForecast.plot` method uses Plotly as a defaul engine. You can change to MatPlotLib by setting `engine=\"matplotlib\"`. \n:::\n\n## 4. Train multiple models for many series\n\n`NeuralForecast` can train many models on many time series globally and efficiently. \n\nEach `Auto` model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\n\nFirst, we create a custom search space for the `AutoNHITS` and `AutoLSTM` models. Search spaces are specified with dictionaries, where keys corresponds to the model's hyperparameter and the value is a `Tune` function to specify how the hyperparameter will be sampled. For example, use `randint` to sample integers uniformly, and `choice` to sample values of a list.\n\nTo instantiate an `Auto` model you need to define:\n\n* `h`: forecasting horizon.\n* `loss`: training and validation loss from `neuralforecast.losses.pytorch`.\n* `config`: hyperparameter search space. If `None`, the `Auto` class will use a pre-defined suggested hyperparameter space.\n* `search_alg`: search algorithm (from `tune.search`), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\n* `num_samples`: number of configurations explored.\n\nIn this example we set horizon `h` as 48, use the `MQLoss` distribution loss for training and validation, and use the default search algorithm. \n\n:::{.callout-tip}\nThe number of samples, `num_samples`, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting `num_samples` higher than 20.\n:::\n\nNext, we use the `Neuralforecast` class to train the `Auto` model. In this step, `Auto` models will automatically perform hyperparameter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nNext, we use the `predict` method to forecast the next 48 days using the optimal hyperparameters.\n\nThe `StatsForecast.plot` allows for further customization. For example, plot the results of the different models and unique ids. \n\n## 5. Evaluate the model's performance\n\n\nIn previous steps, we've taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\n\nWith time series data, **Cross Validation** is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model's predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\n\nThe following graph depicts such a Cross Validation Strategy:\n\n![](https://raw.githubusercontent.com/Nixtla/statsforecast/main/nbs/imgs/ChainedWindows.gif)\n\n:::{.callout-tip}\nSetting `n_windows=1` mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set. \n:::\n\nThe `cross_validation` method from the `NeuralForecast` class takes the following arguments.\n\n- `df`: training data frame\n\n- `step_size` (int): step size between each window. In other words: how often do you want to run the forecasting processes.\n- `n_windows` (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\nThe `cv_df` object is a new data frame that includes the following columns:\n\n- `unique_id`: identifies each time series\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\nNow, let's evaluate the models' performance.\n\n:::{.callout-warning}\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely [hard to judge](\"https://blog.blueyonder.com/mean-absolute-percentage-error-mape-has-served-its-duty-and-should-now-retire/\") and not useful to assess forecasting quality.\n:::\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric. \n\nCreate a summary table with a model column and the number of series where that model performs best. \n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\n## 6. Select the best model for every unique series\n\nDefine a utility function that takes your forecast's data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nPlot the results. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"nixtla - neural_advanced.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"author":"Benedict Thekkel","theme":{"light":"flatly","dark":"darkly"},"description":"Model training, evaluation and selection for [multiple time series](https://nixtla.github.io/neuralforecast/models.html)","skip_exec":true,"skip_showdoc":true,"title":"Nixtla - Neural Advanced Forecast"},"extensions":{"book":{"multiFile":true}}},"gfm":{"identifier":{"display-name":"Github (GFM)","target-format":"gfm","base-format":"gfm"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"md","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"variant":"+autolink_bare_uris+emoji+footnotes+gfm_auto_identifiers+pipe_tables+strikeout+task_lists+tex_math_dollars"},"pandoc":{"standalone":true,"default-image-extension":"png","to":"commonmark","output-file":"nixtla - neural_advanced.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"description":"Model training, evaluation and selection for [multiple time series](https://nixtla.github.io/neuralforecast/models.html)","skip_exec":true,"skip_showdoc":true,"title":"Nixtla - Neural Advanced Forecast"}}},"projectFormats":["html","gfm"]}