{"title":"Statistical, Machine Learning and Neural Forecasting methods","markdown":{"yaml":{"description":"In this notebook, you will make forecasts for the M5 dataset choosing the best model for each time series using cross validation.","output-file":"nixtla - all_methods.html","skip_exec":true,"skip_showdoc":true,"title":"Statistical, Machine Learning and Neural Forecasting methods"},"headingText":"Installing Libraries","containsRefs":false,"markdown":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\nStatistical, Machine Learning, and Neural Forecasting Methods\nIn this tutorial, we will explore the process of forecasting on the M5 dataset by utilizing the most suitable model for each time series. We'll accomplish this through an essential technique known as cross-validation. This approach helps us in estimating the predictive performance of our models, and in selecting the model that yields the best performance for each time series.\n\nThe M5 dataset comprises of hierarchical sales data, spanning five years, from Walmart. The aim is to forecast daily sales for the next 28 days. The dataset is broken down into the 50 states of America, with 10 stores in each state.\n\nIn the realm of time series forecasting and analysis, one of the more complex tasks is identifying the model that is optimally suited for a specific group of series. Quite often, this selection process leans heavily on intuition, which may not necessarily align with the empirical reality of our dataset.\n\nIn this tutorial, we aim to provide a more structured, data-driven approach to model selection for different groups of series within the M5 benchmark dataset. This dataset, well-known in the field of forecasting, allows us to showcase the versatility and power of our methodology.\n\nWe will train an assortment of models from various forecasting paradigms:\n\n*[StatsForecast]((https://github.com/Nixtla/statsforecast))*\n\n- Baseline models: These models are simple yet often highly effective for providing an initial perspective on the forecasting problem. We will use `SeasonalNaive` and `HistoricAverage` models for this category.\n- Intermittent models: For series with sporadic, non-continuous demand, we will utilize models like `CrostonOptimized`, `IMAPA`, and `ADIDA`. These models are particularly suited for handling zero-inflated series.\n- State Space Models: These are statistical models that use mathematical descriptions of a system to make predictions. The `AutoETS` model from the statsforecast library falls under this category.\n\n*[MLForecast](https://github.com/Nixtla/mlforecast)*\n\nMachine Learning: Leveraging ML models like `LightGBM`, `XGBoost`, and `LinearRegression` can be advantageous due to their capacity to uncover intricate patterns in data. We'll use the MLForecast library for this purpose.\n\n*[NeuralForecast](https://github.com/Nixtla/neuralforecast)*\n\nDeep Learning: DL models, such as Transformers (`AutoTFT`) and Neural Networks (`AutoNHITS`), allow us to handle complex non-linear dependencies in time series data. We'll utilize the NeuralForecast library for these models.\n\nUsing the Nixtla suite of libraries, we'll be able to drive our model selection process with data, ensuring we utilize the most suitable models for specific groups of series in our dataset.\n\nOutline:\n\n* Reading Data: In this initial step, we load our dataset into memory, making it available for our subsequent analysis and forecasting. It is important to understand the structure and nuances of the dataset at this stage.\n\n* Forecasting Using Statistical and Deep Learning Methods: We apply a wide range of forecasting methods from basic statistical techniques to advanced deep learning models. The aim is to generate predictions for the next 28 days based on our dataset.\n\n* Model Performance Evaluation on Different Windows: We assess the performance of our models on distinct windows.\n\n* Selecting the Best Model for a Group of Series: Using the performance evaluation, we identify the optimal model for each group of series. This step ensures that the chosen model is tailored to the unique characteristics of each group.\n\n* Filtering the Best Possible Forecast: Finally, we filter the forecasts generated by our chosen models to obtain the most promising predictions. This is our final output and represents the best possible forecast for each series according to our models.\n\n::: {.callout-warning collapse=\"false\"}\nThis tutorial was originally executed using a `c5d.24xlarge` EC2 instance.\n:::\n\n\n```sh\n!pip install statsforecast mlforecast neuralforecast datasetforecast s3fs pyarrow\n```\n\n## Download and prepare data\n\nThe example uses the [M5 dataset](https://github.com/Mcompetitions/M5-methods/blob/master/M5-Competitors-Guide.pdf). It consists of `30,490` bottom time series. \n\nFor simplicity sake we will keep just one category\n\n# Basic Plotting\n\nPlot some series using the plot method from the `StatsForecast` class. This method prints 8 random series from the dataset and is useful for basic [EDA](https://nixtla.github.io/statsforecast/core.html#statsforecast.plot).\n\n# Create forecasts with Stats, Ml and Neural methods.\n\n## StatsForecast\n\n`StatsForecast` is a comprehensive library providing a suite of popular univariate time series forecasting models, all designed with a focus on high performance and scalability.\n\nHere's what makes StatsForecast a powerful tool for time series forecasting:\n\n- **Collection of Local Models**: StatsForecast provides a diverse collection of local models that can be applied to each time series individually, allowing us to capture unique patterns within each series.\n\n- **Simplicity**: With StatsForecast, training, forecasting, and backtesting multiple models become a straightforward process, requiring only a few lines of code. This simplicity makes it a convenient tool for both beginners and experienced practitioners.\n\n- **Optimized for Speed**: The implementation of the models in StatsForecast is optimized for speed, ensuring that large-scale computations are performed efficiently, thereby reducing the overall time for model training and prediction.\n\n- **Horizontal Scalability**: One of the distinguishing features of StatsForecast is its ability to scale horizontally. It is compatible with distributed computing frameworks such as Spark, Dask, and Ray. This feature allows it to handle large datasets by distributing the computations across multiple nodes in a cluster, making it a go-to solution for large-scale time series forecasting tasks.\n\n`StatsForecast` receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\n- `models`: a list of models. Select the models you want from models and import them.\n- `freq`: a string indicating the frequency of the data. (See panda’s available frequencies.)\n- `n_jobs`: int, number of jobs used in the parallel processing, use -1 for all cores.\n- `fallback_model`: a model to be used if a model fails.\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\n- `h` (int): represents the forecast h steps into the future. In this case, 12 months ahead.\n- `level` (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nThis block of code times how long it takes to run the forecasting function of the StatsForecast class, which predicts the next 28 days (h=28). The level is set to [90], meaning it will compute the 90% prediction interval. The time is calculated in minutes and printed out at the end.\n\n## MLForecast\n\n`MLForecast` is a powerful library that provides automated feature creation for time series forecasting, facilitating the use of global machine learning models. It is designed for high performance and scalability.\n\nKey features of MLForecast include:\n\n* **Support for sklearn models**: MLForecast is compatible with models that follow the scikit-learn API. This makes it highly flexible and allows it to seamlessly integrate with a wide variety of machine learning algorithms.\n\n* **Simplicity**: With MLForecast, the tasks of training, forecasting, and backtesting models can be accomplished in just a few lines of code. This streamlined simplicity makes it user-friendly for practitioners at all levels of expertise.\n\n* **Optimized for speed:** MLForecast is engineered to execute tasks rapidly, which is crucial when handling large datasets and complex models.\n\n* **Horizontal Scalability:** MLForecast is capable of horizontal scaling using distributed computing frameworks such as Spark, Dask, and Ray. This feature enables it to efficiently process massive datasets by distributing the computations across multiple nodes in a cluster, making it ideal for large-scale time series forecasting tasks.\n\n```sh\n!pip install lightgbm xgboost\n```\n\nJust call the `fit` models to train the select models. In this case we are generating conformal prediction intervals. \n\nAfter that, just call `predict` to generate forecasts.\n\n## NeuralForecast\n\n`NeuralForecast` is a robust collection of neural forecasting models that focuses on usability and performance. It includes a variety of model architectures, from classic networks such as Multilayer Perceptrons (MLP) and Recurrent Neural Networks (RNN) to novel contributions like N-BEATS, N-HITS, Temporal Fusion Transformers (TFT), and more.\n\nKey features of `NeuralForecast` include:\n\n- A broad collection of global models. Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, TFT, Informer, PatchTST and HINT. \n- A simple and intuitive interface that allows training, forecasting, and backtesting of various models in a few lines of code.\n- Support for GPU acceleration to improve computational speed.\n\nThis machine doesn't have GPU, but Google Colabs offers some for free. \n\nUsing [Colab's GPU to train NeuralForecast](https://nixtla.github.io/neuralforecast/examples/intermittentdata.html).\n\n## Forecast plots\n\nUse the plot function to explore models and ID's\n\n# Validate Model's Performance\n\nThe three libraries - `StatsForecast`, `MLForecast`, and `NeuralForecast` - offer out-of-the-box cross-validation capabilities specifically designed for time series. This allows us to evaluate the model's performance using historical data to obtain an unbiased assessment of how well each model is likely to perform on unseen data.\n\n![From the course of Modern Forecasting in Practice](../imgs/cv-sphere.png)\n\n## Cross Validation in StatsForecast\n\nThe `cross_validation` method from the `StatsForecast` class accepts the following arguments:\n\n- `df`: A DataFrame representing the training data.\n- `h` (int): The forecast horizon, represented as the number of steps into the future that we wish to predict. For example, if we're forecasting hourly data, `h=24` would represent a 24-hour forecast.\n- `step_size` (int): The step size between each cross-validation window. This parameter determines how often we want to run the forecasting process.\n- `n_windows` (int): The number of windows used for cross validation. This parameter defines how many past forecasting processes we want to evaluate.\n\nThese parameters allow us to control the extent and granularity of our cross-validation process. By tuning these settings, we can balance between computational cost and the thoroughness of the cross-validation.\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\n- `unique_id` index: (If you dont like working with index just run forecasts_cv_df.resetindex())\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\n## MLForecast\n\nThe `cross_validation` method from the `MLForecast` class takes the following arguments.\n\n- `data`: training data frame\n- `window_size` (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\n- `step_size` (int): step size between each window. In other words: how often do you want to run the forecasting processes.\n- `n_windows` (int): number of windows used for cross-validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n- `prediction_intervals`: class to compute conformal intervals.\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\n- `unique_id` index: (If you dont like working with index just run forecasts_cv_df.resetindex())\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\n## NeuralForecast\n\nThis machine doesn't have GPU, but Google Colabs offers some for free. \n\nUsing [Colab's GPU to train NeuralForecast](https://nixtla.github.io/neuralforecast/examples/intermittentdata.html).\n\n## Merge cross validation forecasts\n\n## Plots CV\n\n### Aggregate Demand\n\n## Evaluation per series and CV window\n\nIn this section, we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use `dask` to parallelize the evaluation. The parallelization will be done using `fugue`.\n\nThe `evaluate` function receives a unique combination of a time series and a window, and calculates different `metrics` for each model in `df`.\n\nLet's cleate a `dask` client.\n\nThe `transform` function takes the `evaluate` functions and applies it to each combination of time series (`unique_id`) and cross validation window (`cutoff`) using the `dask` client we created before.\n\nResults showed in previous experiments.\n\n| model | MSE | \n|:-------|-----:|\n|MQCNN | 10.09 |\n|DeepAR-student_t | 10.11 |\n|DeepAR-lognormal | 30.20 |\n|DeepAR| 9.13 |\n|NPTS |  11.53 |\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\n### Distribution of errors\n\n#### SMAPE\n\n### Choose models for groups of series\n\nFeature: \n\n* A unified dataframe with forecasts for all different models\n* Easy Ensamble\n* E.g. Average predictions\n* Or MinMax (Choosing is ensembling)\n\n### Et pluribus unum: an inclusive forecasting Pie.\n\n# Choose Forecasting method for different groups of series\n\n# Technical Debt\n\n- Train the statistical models in the full dataset.\n- Increase the number of `num_samples` in the neural auto models.\n- Include other models such as `Theta`, `ARIMA`, `RNN`, `LSTM`, ...\n\n# Further materials\n\n- [Available Models StatsForecast](https://nixtla.github.io/statsforecast/examples/models_intro.html)\n- [Available Models NeuralForecast](https://nixtla.github.io/neuralforecast/models.html)\n- [Scalers and Loss Functions](https://nixtla.github.io/neuralforecast/losses.pytorch.html)\n- [Getting Started NeuralForecast](https://nixtla.github.io/neuralforecast/examples/getting_started.html)\n- [Hierarchical Reconciliation](https://nixtla.github.io/hierarchicalforecast/examples/tourismsmall.html)\n- [Distributed ML Forecast (trees)](https://nixtla.github.io/mlforecast/docs/quick_start_distributed.html)\n- [Using StatsForecast to train millions of time series](https://www.anyscale.com/blog/how-nixtla-uses-ray-to-accurately-predict-more-than-a-million-time-series)\n- [Intermittent Demand Forecasting With Nixtla on Databricks](https://www.databricks.com/blog/2022/12/06/intermittent-demand-forecasting-nixtla-databricks.html)\n","srcMarkdownNoYaml":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\nStatistical, Machine Learning, and Neural Forecasting Methods\nIn this tutorial, we will explore the process of forecasting on the M5 dataset by utilizing the most suitable model for each time series. We'll accomplish this through an essential technique known as cross-validation. This approach helps us in estimating the predictive performance of our models, and in selecting the model that yields the best performance for each time series.\n\nThe M5 dataset comprises of hierarchical sales data, spanning five years, from Walmart. The aim is to forecast daily sales for the next 28 days. The dataset is broken down into the 50 states of America, with 10 stores in each state.\n\nIn the realm of time series forecasting and analysis, one of the more complex tasks is identifying the model that is optimally suited for a specific group of series. Quite often, this selection process leans heavily on intuition, which may not necessarily align with the empirical reality of our dataset.\n\nIn this tutorial, we aim to provide a more structured, data-driven approach to model selection for different groups of series within the M5 benchmark dataset. This dataset, well-known in the field of forecasting, allows us to showcase the versatility and power of our methodology.\n\nWe will train an assortment of models from various forecasting paradigms:\n\n*[StatsForecast]((https://github.com/Nixtla/statsforecast))*\n\n- Baseline models: These models are simple yet often highly effective for providing an initial perspective on the forecasting problem. We will use `SeasonalNaive` and `HistoricAverage` models for this category.\n- Intermittent models: For series with sporadic, non-continuous demand, we will utilize models like `CrostonOptimized`, `IMAPA`, and `ADIDA`. These models are particularly suited for handling zero-inflated series.\n- State Space Models: These are statistical models that use mathematical descriptions of a system to make predictions. The `AutoETS` model from the statsforecast library falls under this category.\n\n*[MLForecast](https://github.com/Nixtla/mlforecast)*\n\nMachine Learning: Leveraging ML models like `LightGBM`, `XGBoost`, and `LinearRegression` can be advantageous due to their capacity to uncover intricate patterns in data. We'll use the MLForecast library for this purpose.\n\n*[NeuralForecast](https://github.com/Nixtla/neuralforecast)*\n\nDeep Learning: DL models, such as Transformers (`AutoTFT`) and Neural Networks (`AutoNHITS`), allow us to handle complex non-linear dependencies in time series data. We'll utilize the NeuralForecast library for these models.\n\nUsing the Nixtla suite of libraries, we'll be able to drive our model selection process with data, ensuring we utilize the most suitable models for specific groups of series in our dataset.\n\nOutline:\n\n* Reading Data: In this initial step, we load our dataset into memory, making it available for our subsequent analysis and forecasting. It is important to understand the structure and nuances of the dataset at this stage.\n\n* Forecasting Using Statistical and Deep Learning Methods: We apply a wide range of forecasting methods from basic statistical techniques to advanced deep learning models. The aim is to generate predictions for the next 28 days based on our dataset.\n\n* Model Performance Evaluation on Different Windows: We assess the performance of our models on distinct windows.\n\n* Selecting the Best Model for a Group of Series: Using the performance evaluation, we identify the optimal model for each group of series. This step ensures that the chosen model is tailored to the unique characteristics of each group.\n\n* Filtering the Best Possible Forecast: Finally, we filter the forecasts generated by our chosen models to obtain the most promising predictions. This is our final output and represents the best possible forecast for each series according to our models.\n\n::: {.callout-warning collapse=\"false\"}\nThis tutorial was originally executed using a `c5d.24xlarge` EC2 instance.\n:::\n\n## Installing Libraries\n\n```sh\n!pip install statsforecast mlforecast neuralforecast datasetforecast s3fs pyarrow\n```\n\n## Download and prepare data\n\nThe example uses the [M5 dataset](https://github.com/Mcompetitions/M5-methods/blob/master/M5-Competitors-Guide.pdf). It consists of `30,490` bottom time series. \n\nFor simplicity sake we will keep just one category\n\n# Basic Plotting\n\nPlot some series using the plot method from the `StatsForecast` class. This method prints 8 random series from the dataset and is useful for basic [EDA](https://nixtla.github.io/statsforecast/core.html#statsforecast.plot).\n\n# Create forecasts with Stats, Ml and Neural methods.\n\n## StatsForecast\n\n`StatsForecast` is a comprehensive library providing a suite of popular univariate time series forecasting models, all designed with a focus on high performance and scalability.\n\nHere's what makes StatsForecast a powerful tool for time series forecasting:\n\n- **Collection of Local Models**: StatsForecast provides a diverse collection of local models that can be applied to each time series individually, allowing us to capture unique patterns within each series.\n\n- **Simplicity**: With StatsForecast, training, forecasting, and backtesting multiple models become a straightforward process, requiring only a few lines of code. This simplicity makes it a convenient tool for both beginners and experienced practitioners.\n\n- **Optimized for Speed**: The implementation of the models in StatsForecast is optimized for speed, ensuring that large-scale computations are performed efficiently, thereby reducing the overall time for model training and prediction.\n\n- **Horizontal Scalability**: One of the distinguishing features of StatsForecast is its ability to scale horizontally. It is compatible with distributed computing frameworks such as Spark, Dask, and Ray. This feature allows it to handle large datasets by distributing the computations across multiple nodes in a cluster, making it a go-to solution for large-scale time series forecasting tasks.\n\n`StatsForecast` receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\n- `models`: a list of models. Select the models you want from models and import them.\n- `freq`: a string indicating the frequency of the data. (See panda’s available frequencies.)\n- `n_jobs`: int, number of jobs used in the parallel processing, use -1 for all cores.\n- `fallback_model`: a model to be used if a model fails.\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\n- `h` (int): represents the forecast h steps into the future. In this case, 12 months ahead.\n- `level` (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nThis block of code times how long it takes to run the forecasting function of the StatsForecast class, which predicts the next 28 days (h=28). The level is set to [90], meaning it will compute the 90% prediction interval. The time is calculated in minutes and printed out at the end.\n\n## MLForecast\n\n`MLForecast` is a powerful library that provides automated feature creation for time series forecasting, facilitating the use of global machine learning models. It is designed for high performance and scalability.\n\nKey features of MLForecast include:\n\n* **Support for sklearn models**: MLForecast is compatible with models that follow the scikit-learn API. This makes it highly flexible and allows it to seamlessly integrate with a wide variety of machine learning algorithms.\n\n* **Simplicity**: With MLForecast, the tasks of training, forecasting, and backtesting models can be accomplished in just a few lines of code. This streamlined simplicity makes it user-friendly for practitioners at all levels of expertise.\n\n* **Optimized for speed:** MLForecast is engineered to execute tasks rapidly, which is crucial when handling large datasets and complex models.\n\n* **Horizontal Scalability:** MLForecast is capable of horizontal scaling using distributed computing frameworks such as Spark, Dask, and Ray. This feature enables it to efficiently process massive datasets by distributing the computations across multiple nodes in a cluster, making it ideal for large-scale time series forecasting tasks.\n\n```sh\n!pip install lightgbm xgboost\n```\n\nJust call the `fit` models to train the select models. In this case we are generating conformal prediction intervals. \n\nAfter that, just call `predict` to generate forecasts.\n\n## NeuralForecast\n\n`NeuralForecast` is a robust collection of neural forecasting models that focuses on usability and performance. It includes a variety of model architectures, from classic networks such as Multilayer Perceptrons (MLP) and Recurrent Neural Networks (RNN) to novel contributions like N-BEATS, N-HITS, Temporal Fusion Transformers (TFT), and more.\n\nKey features of `NeuralForecast` include:\n\n- A broad collection of global models. Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, TFT, Informer, PatchTST and HINT. \n- A simple and intuitive interface that allows training, forecasting, and backtesting of various models in a few lines of code.\n- Support for GPU acceleration to improve computational speed.\n\nThis machine doesn't have GPU, but Google Colabs offers some for free. \n\nUsing [Colab's GPU to train NeuralForecast](https://nixtla.github.io/neuralforecast/examples/intermittentdata.html).\n\n## Forecast plots\n\nUse the plot function to explore models and ID's\n\n# Validate Model's Performance\n\nThe three libraries - `StatsForecast`, `MLForecast`, and `NeuralForecast` - offer out-of-the-box cross-validation capabilities specifically designed for time series. This allows us to evaluate the model's performance using historical data to obtain an unbiased assessment of how well each model is likely to perform on unseen data.\n\n![From the course of Modern Forecasting in Practice](../imgs/cv-sphere.png)\n\n## Cross Validation in StatsForecast\n\nThe `cross_validation` method from the `StatsForecast` class accepts the following arguments:\n\n- `df`: A DataFrame representing the training data.\n- `h` (int): The forecast horizon, represented as the number of steps into the future that we wish to predict. For example, if we're forecasting hourly data, `h=24` would represent a 24-hour forecast.\n- `step_size` (int): The step size between each cross-validation window. This parameter determines how often we want to run the forecasting process.\n- `n_windows` (int): The number of windows used for cross validation. This parameter defines how many past forecasting processes we want to evaluate.\n\nThese parameters allow us to control the extent and granularity of our cross-validation process. By tuning these settings, we can balance between computational cost and the thoroughness of the cross-validation.\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\n- `unique_id` index: (If you dont like working with index just run forecasts_cv_df.resetindex())\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\n## MLForecast\n\nThe `cross_validation` method from the `MLForecast` class takes the following arguments.\n\n- `data`: training data frame\n- `window_size` (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\n- `step_size` (int): step size between each window. In other words: how often do you want to run the forecasting processes.\n- `n_windows` (int): number of windows used for cross-validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n- `prediction_intervals`: class to compute conformal intervals.\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\n- `unique_id` index: (If you dont like working with index just run forecasts_cv_df.resetindex())\n- `ds`: datestamp or temporal index\n- `cutoff`: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\n- `y`: true value\n- `\"model\"`: columns with the model’s name and fitted value.\n\n## NeuralForecast\n\nThis machine doesn't have GPU, but Google Colabs offers some for free. \n\nUsing [Colab's GPU to train NeuralForecast](https://nixtla.github.io/neuralforecast/examples/intermittentdata.html).\n\n## Merge cross validation forecasts\n\n## Plots CV\n\n### Aggregate Demand\n\n## Evaluation per series and CV window\n\nIn this section, we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use `dask` to parallelize the evaluation. The parallelization will be done using `fugue`.\n\nThe `evaluate` function receives a unique combination of a time series and a window, and calculates different `metrics` for each model in `df`.\n\nLet's cleate a `dask` client.\n\nThe `transform` function takes the `evaluate` functions and applies it to each combination of time series (`unique_id`) and cross validation window (`cutoff`) using the `dask` client we created before.\n\nResults showed in previous experiments.\n\n| model | MSE | \n|:-------|-----:|\n|MQCNN | 10.09 |\n|DeepAR-student_t | 10.11 |\n|DeepAR-lognormal | 30.20 |\n|DeepAR| 9.13 |\n|NPTS |  11.53 |\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\n### Distribution of errors\n\n#### SMAPE\n\n### Choose models for groups of series\n\nFeature: \n\n* A unified dataframe with forecasts for all different models\n* Easy Ensamble\n* E.g. Average predictions\n* Or MinMax (Choosing is ensembling)\n\n### Et pluribus unum: an inclusive forecasting Pie.\n\n# Choose Forecasting method for different groups of series\n\n# Technical Debt\n\n- Train the statistical models in the full dataset.\n- Increase the number of `num_samples` in the neural auto models.\n- Include other models such as `Theta`, `ARIMA`, `RNN`, `LSTM`, ...\n\n# Further materials\n\n- [Available Models StatsForecast](https://nixtla.github.io/statsforecast/examples/models_intro.html)\n- [Available Models NeuralForecast](https://nixtla.github.io/neuralforecast/models.html)\n- [Scalers and Loss Functions](https://nixtla.github.io/neuralforecast/losses.pytorch.html)\n- [Getting Started NeuralForecast](https://nixtla.github.io/neuralforecast/examples/getting_started.html)\n- [Hierarchical Reconciliation](https://nixtla.github.io/hierarchicalforecast/examples/tourismsmall.html)\n- [Distributed ML Forecast (trees)](https://nixtla.github.io/mlforecast/docs/quick_start_distributed.html)\n- [Using StatsForecast to train millions of time series](https://www.anyscale.com/blog/how-nixtla-uses-ray-to-accurately-predict-more-than-a-million-time-series)\n- [Intermittent Demand Forecasting With Nixtla on Databricks](https://www.databricks.com/blog/2022/12/06/intermittent-demand-forecasting-nixtla-databricks.html)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"nixtla - all_methods.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"author":"Benedict Thekkel","theme":{"light":"flatly","dark":"darkly"},"description":"In this notebook, you will make forecasts for the M5 dataset choosing the best model for each time series using cross validation.","skip_exec":true,"skip_showdoc":true,"title":"Statistical, Machine Learning and Neural Forecasting methods"},"extensions":{"book":{"multiFile":true}}},"gfm":{"identifier":{"display-name":"Github (GFM)","target-format":"gfm","base-format":"gfm"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"md","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"variant":"+autolink_bare_uris+emoji+footnotes+gfm_auto_identifiers+pipe_tables+strikeout+task_lists+tex_math_dollars"},"pandoc":{"standalone":true,"default-image-extension":"png","to":"commonmark","output-file":"nixtla - all_methods.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"description":"In this notebook, you will make forecasts for the M5 dataset choosing the best model for each time series using cross validation.","skip_exec":true,"skip_showdoc":true,"title":"Statistical, Machine Learning and Neural Forecasting methods"}}},"projectFormats":["html","gfm"]}