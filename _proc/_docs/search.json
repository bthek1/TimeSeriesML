[
  {
    "objectID": "consumption_model_week.html",
    "href": "consumption_model_week.html",
    "title": "Consumption Model For A Week",
    "section": "",
    "text": "Energy\n\n1 hour integral of power in kWh\n2021/04/01 - 2023/11/09\n\nPower\n\n5 mins average of Iotawatt\n2022/07/21 - 2023/11/09\n\ncreate list of other variable to tune"
  },
  {
    "objectID": "consumption_model_week.html#get-data",
    "href": "consumption_model_week.html#get-data",
    "title": "Consumption Model For A Week",
    "section": "Get Data",
    "text": "Get Data\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "consumption_model_week.html#energy-in-wide-form",
    "href": "consumption_model_week.html#energy-in-wide-form",
    "title": "Consumption Model For A Week",
    "section": "Energy in Wide Form",
    "text": "Energy in Wide Form\n\nwide_df = result.pivot(index='_time', columns='sensor', values='_value')\n# Reset the index to make 'id' a regular column\nwide_df.reset_index(inplace=True)\n\nwide_df.columns\n\nIndex(['_time', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production', 'Unmonitored'],\n      dtype='object', name='sensor')\n\n\n\nwide_df['_time'] = pd.to_datetime(wide_df['_time']).dt.tz_localize(None)\nwide_df.drop(['Unmonitored'], axis = 1, inplace=True)\nwide_df.rename(columns={'_time':'ds'}, inplace=True)\n# Delete the last row\nwide_df = wide_df.drop(wide_df.index[-1])\n\n\nwide_df_new = pd.DataFrame()\nfor column in wide_df.columns:\n    wide_df_new[f'{column}'] = wide_df[f'{column}'].interpolate()\n\nwide_df = pd.DataFrame(wide_df_new)\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production'],\n      dtype='object')\n\n\n\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nConsumption\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n8.578005\n357.275503\n866.654129\n180.242247\n25.492343\n24.017087\n0.530865\n999.775552\n333.258545\n366.408449\n71.876417\n80.009421\n-108.365830\n66.086281\n176.392548\n491.748285\n\n\n241\n2023-10-29\n8.661682\n179.501779\n866.654129\n806.610093\n19.634587\n2.731102\n0.192283\n530.312474\n176.770805\n201.397413\n1.974071\n60.108551\n-804.636021\n1.253300\n103.451046\n1017.375874\n\n\n242\n2023-10-30\n8.603354\n263.812291\n866.654129\n757.601472\n27.081000\n0.983716\n0.183639\n788.486015\n262.828480\n290.616882\n3.378339\n66.827993\n-754.223132\n1.356785\n178.170521\n1052.272015\n\n\n243\n2023-10-31\n9.032917\n311.552394\n866.654129\n803.841970\n35.649472\n11.163953\n0.269313\n901.165305\n300.388379\n319.795146\n5.027126\n67.972347\n-798.814844\n30.217549\n166.649785\n1142.661342\n\n\n244\n2023-11-01\n8.743938\n211.339011\n866.654129\n898.594991\n30.389375\n3.655479\n0.189410\n623.050504\n207.683462\n245.486118\n2.309883\n50.624125\n-896.285107\n0.213375\n145.941312\n1134.978184\n\n\n\n\n\n\n\n\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n    return df\n\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='y')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='y:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nwide_df = data_scaler(wide_df)\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nConsumption\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n0.000327\n0.295413\n0.639383\n0.087269\n0.054518\n0.475787\n0.000988\n0.441317\n0.270647\n0.263152\n0.098826\n0.397064\n0.702175\n0.217335\n0.451213\n0.164710\n\n\n241\n2023-10-29\n0.000459\n0.096052\n0.639383\n0.390543\n0.008991\n0.049113\n0.000167\n0.155476\n0.095349\n0.096613\n0.001842\n0.287229\n0.452268\n0.003559\n0.195780\n0.385757\n\n\n242\n2023-10-30\n0.000367\n0.190600\n0.639383\n0.366814\n0.066865\n0.014087\n0.000146\n0.312670\n0.191751\n0.186659\n0.003790\n0.324314\n0.470363\n0.003900\n0.457440\n0.400433\n\n\n243\n2023-10-31\n0.001045\n0.244137\n0.639383\n0.389203\n0.133461\n0.218148\n0.000354\n0.381277\n0.233826\n0.216107\n0.006078\n0.330630\n0.454358\n0.099064\n0.417095\n0.438445\n\n\n244\n2023-11-01\n0.000589\n0.131755\n0.639383\n0.435080\n0.092579\n0.067642\n0.000160\n0.211942\n0.129978\n0.141110\n0.002308\n0.234883\n0.419373\n0.000130\n0.344577\n0.435214\n\n\n\n\n\n\n\n\ndf = long_form(wide_df)\nwide_df.rename(columns={'Consumption': 'y'}, inplace=True)\nwide_df['unique_id'] = 'Consumption'\ndf.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-03-02\nAircon\n0.007134\n\n\n1\n2023-03-03\nAircon\n0.178748\n\n\n2\n2023-03-04\nAircon\n0.338990\n\n\n3\n2023-03-05\nAircon\n0.173844\n\n\n4\n2023-03-06\nAircon\n0.363831"
  },
  {
    "objectID": "consumption_model_week.html#weather-data",
    "href": "consumption_model_week.html#weather-data",
    "title": "Consumption Model For A Week",
    "section": "Weather data",
    "text": "Weather data\n\nimport pandas as pd\npath = 'Data'\nname = 'weather'\n\nweather_df = pd.read_csv(f'{path}/{name}.csv')\nweather_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01 00:00\n21.6\n0\n12.6\n126\n1013.0\n0.0\n84\n73\n21.6\n21.6\n24.4\n18.7\n0\n10.0\n19.8\n1.0\n\n\n1\n2023-01-01 01:00\n21.3\n0\n11.9\n133\n1013.0\n0.0\n85\n77\n21.3\n21.3\n24.3\n18.7\n0\n10.0\n18.7\n1.0\n\n\n2\n2023-01-01 02:00\n21.1\n0\n11.2\n140\n1012.0\n0.0\n86\n80\n21.1\n21.1\n24.2\n18.7\n0\n10.0\n17.6\n1.0\n\n\n3\n2023-01-01 03:00\n20.8\n0\n10.4\n147\n1012.0\n0.0\n88\n84\n20.8\n20.8\n20.8\n18.7\n0\n10.0\n16.6\n1.0\n\n\n4\n2023-01-01 04:00\n21.0\n0\n10.6\n144\n1012.0\n0.0\n86\n84\n21.0\n21.0\n21.0\n18.6\n0\n10.0\n16.3\n1.0\n\n\n\n\n\n\n\n\nweather_df = data_scaler(weather_df)\n\n\ndef data_day_avg(weather_df):\n    from tqdm.notebook import tqdm\n    avg_df = pd.DataFrame()\n    dates = pd.to_datetime(weather_df['ds']).dt.date.unique()\n    \n    for date in tqdm(dates):\n        filtered_df = weather_df[pd.to_datetime(weather_df['ds']).dt.date == date]\n        ds = filtered_df.pop('ds')\n\n        filtered_df = pd.DataFrame(filtered_df.mean()).T\n        filtered_df\n\n        filtered_df.insert(0, 'ds', date)\n\n        avg_df = pd.concat([avg_df, filtered_df], ignore_index=True)\n    \n    return avg_df\n\n\nweather_avg_df = data_day_avg(weather_df)\n\n\n\n\n\nweather_avg_df['ds'] = pd.to_datetime(weather_avg_df['ds'])\n\n\nweather_avg_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01\n0.566503\n0.583333\n0.459098\n0.345636\n0.428728\n0.000895\n0.670886\n0.583750\n0.516791\n0.595085\n0.512881\n0.778121\n0.0\n1.000000\n0.434678\n0.422619\n\n\n1\n2023-01-02\n0.554902\n0.583333\n0.451453\n0.396936\n0.359649\n0.001253\n0.602848\n0.409167\n0.499005\n0.584249\n0.486067\n0.725707\n0.0\n1.000000\n0.434844\n0.428571\n\n\n2\n2023-01-03\n0.582516\n0.583333\n0.263889\n0.415506\n0.258772\n0.000000\n0.584916\n0.287917\n0.525746\n0.610043\n0.502366\n0.738663\n0.0\n1.000000\n0.283533\n0.452381\n\n\n3\n2023-01-04\n0.650490\n0.583333\n0.313073\n0.382660\n0.163377\n0.000895\n0.687764\n0.196250\n0.608582\n0.673535\n0.609621\n0.866902\n0.0\n0.994792\n0.305777\n0.500000\n\n\n4\n2023-01-05\n0.654575\n0.583333\n0.257518\n0.506267\n0.166667\n0.026859\n0.735232\n0.700417\n0.629726\n0.677350\n0.608701\n0.911808\n0.0\n0.989583\n0.288098\n0.416667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\n2023-12-06\n0.580556\n0.583333\n0.254077\n0.264740\n0.558114\n0.000000\n0.608650\n0.174583\n0.519527\n0.608364\n0.515116\n0.745878\n0.0\n1.000000\n0.214558\n0.470238\n\n\n340\n2023-12-07\n0.605065\n0.583333\n0.266565\n0.202066\n0.565789\n0.000000\n0.582806\n0.050000\n0.543532\n0.631258\n0.533386\n0.748233\n0.0\n1.000000\n0.222693\n0.476190\n\n\n341\n2023-12-08\n0.618954\n0.583333\n0.349261\n0.251625\n0.575658\n0.000018\n0.626582\n0.342083\n0.561816\n0.644078\n0.555205\n0.795200\n0.0\n1.000000\n0.285940\n0.476190\n\n\n342\n2023-12-09\n0.612582\n0.583333\n0.362768\n0.190460\n0.544956\n0.000107\n0.577532\n0.497500\n0.564179\n0.638584\n0.539432\n0.757509\n0.0\n1.000000\n0.308267\n0.428571\n\n\n343\n2023-12-10\n0.630556\n0.583333\n0.513507\n0.233751\n0.530702\n0.000233\n0.515295\n0.537500\n0.571269\n0.655067\n0.546924\n0.730271\n0.0\n1.000000\n0.426627\n0.452381\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n(\n    wide_df[['y']].plot(title='Consumption')\n)\n\n&lt;Axes: title={'center': 'Consumption'}&gt;\n\n\n\n\n\n\nfrom statsforecast import StatsForecast\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/core.py:25: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/_plotly_utils/basevalidators.py:105: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\n\nwide_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 245 entries, 0 to 244\nData columns (total 18 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   ds                  245 non-null    datetime64[ns]\n 1   Aircon              245 non-null    float64       \n 2   y                   245 non-null    float64       \n 3   ConsumptionNet      198 non-null    float64       \n 4   Export              245 non-null    float64       \n 5   Fridge              245 non-null    float64       \n 6   Garage              245 non-null    float64       \n 7   Hotwater            245 non-null    float64       \n 8   HouseBoardOutside   161 non-null    float64       \n 9   HouseBoardOutsideR  245 non-null    float64       \n 10  HouseBoardR         245 non-null    float64       \n 11  Import              245 non-null    float64       \n 12  Lights              245 non-null    float64       \n 13  Net                 245 non-null    float64       \n 14  OvenStove           245 non-null    float64       \n 15  Powerpoints         245 non-null    float64       \n 16  Production          245 non-null    float64       \n 17  unique_id           245 non-null    object        \ndtypes: datetime64[ns](1), float64(16), object(1)\nmemory usage: 34.6+ KB\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3920 entries, 0 to 3919\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         3920 non-null   datetime64[ns]\n 1   unique_id  3920 non-null   object        \n 2   y          3789 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 92.0+ KB\n\n\n\nweather_avg_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   ds              344 non-null    datetime64[ns]\n 1   temp_c          344 non-null    float64       \n 2   is_day          344 non-null    float64       \n 3   wind_kph        344 non-null    float64       \n 4   wind_degree     344 non-null    float64       \n 5   pressure_mb     344 non-null    float64       \n 6   precip_mm       344 non-null    float64       \n 7   humidity        344 non-null    float64       \n 8   cloud           344 non-null    float64       \n 9   feelslike_c     344 non-null    float64       \n 10  windchill_c     344 non-null    float64       \n 11  heatindex_c     344 non-null    float64       \n 12  dewpoint_c      344 non-null    float64       \n 13  chance_of_rain  344 non-null    float64       \n 14  vis_km          344 non-null    float64       \n 15  gust_kph        344 non-null    float64       \n 16  uv              344 non-null    float64       \ndtypes: datetime64[ns](1), float64(16)\nmemory usage: 45.8 KB\n\n\n\ndate_range_start, date_range_end  = wide_df['ds'].min(), wide_df['ds'].max()\n# Filter rows between the start and end dates\nweather_avg_filtered_df = weather_avg_df[(weather_avg_df['ds'] &gt;= date_range_start) & (weather_avg_df['ds'] &lt;= date_range_end)]\nweather_avg_filtered_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.0\n0.588811\n0.345238\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.0\n0.328934\n0.345238\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.0\n0.340139\n0.416667\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.0\n0.470286\n0.464286\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.0\n0.467795\n0.404762\n\n\n\n\n245 rows × 17 columns\n\n\n\n\naltair_plot(df)\n\n\n\n\n\n\n\n\nweather_avg_df_plot = long_form(weather_avg_filtered_df)\naltair_plot(weather_avg_df_plot)\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nwide_df[\"y\"]\n\n0      0.355157\n1      0.291076\n2      0.498418\n3      0.418824\n4      0.561599\n         ...   \n240    0.295413\n241    0.096052\n242    0.190600\n243    0.244137\n244    0.131755\nName: y, Length: 245, dtype: float64\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=28, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=28, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=14, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=14, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(wide_df[\"y\"], model = \"additive\", period=28)\na.plot();"
  },
  {
    "objectID": "consumption_model_week.html#designing-models",
    "href": "consumption_model_week.html#designing-models",
    "title": "Consumption Model For A Week",
    "section": "Designing Models",
    "text": "Designing Models\n\nweather_avg_filtered_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n\n\n\n\nwide_df = pd.merge(wide_df, weather_avg_filtered_df, on='ds', how='outer')\n\n\nwide_df.head()\n\n\n\n\n\n\n\n\nds\nAircon\ny\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\n...\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-03-02\n0.007134\n0.355157\nNaN\n0.016595\n0.814609\n0.003794\n0.000287\nNaN\n0.356702\n...\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n1\n2023-03-03\n0.178748\n0.291076\nNaN\n0.165011\n0.669709\n0.015031\n0.000143\nNaN\n0.292065\n...\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n2\n2023-03-04\n0.338990\n0.498418\nNaN\n0.294620\n0.686509\n0.059559\n0.000460\nNaN\n0.496691\n...\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n3\n2023-03-05\n0.173844\n0.418824\nNaN\n0.302607\n0.701288\n0.009916\n0.000256\nNaN\n0.419958\n...\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n4\n2023-03-06\n0.363831\n0.561599\nNaN\n0.381220\n0.773714\n0.008798\n0.002112\nNaN\n0.562640\n...\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n5 rows × 34 columns\n\n\n\n\nhorizon = 10\n\ntrain_size = len(wide_df) - horizon\ntrain, test = wide_df[:train_size], wide_df[train_size:]\n\n\nwide_df.shape, train.shape, test.shape\n\n((245, 34), (235, 34), (10, 34))\n\n\n\nfutr_df=weather_avg_filtered_df[train_size:]\nfutr_df['unique_id'] = 'Consumption'\nfutr_df.shape\n\n/tmp/ipykernel_1270281/3311167277.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n(10, 18)\n\n\n\nfutr_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nConsumption\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nConsumption\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nConsumption\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nConsumption\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nConsumption\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.000000\n0.588811\n0.345238\nConsumption\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.000000\n0.328934\n0.345238\nConsumption\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.000000\n0.340139\n0.416667\nConsumption\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.000000\n0.470286\n0.464286\nConsumption\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.000000\n0.467795\n0.404762\nConsumption\n\n\n\n\n\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n #FEDformer, #taken too long\n #StemGNN, #need n_series\n #HINT, #need n_series\n #TimesNet # takes too long\n             ]\n\n\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'y', 'ConsumptionNet', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'unique_id', 'temp_c', 'is_day', 'wind_kph', 'wind_degree',\n       'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n       'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km',\n       'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nmodels = [model(input_size=2 * horizon,\n                h=horizon,\n                max_steps=500,\n                hist_exog_list =['Aircon', 'Export', 'Fridge', 'Garage',\n                                 'Hotwater', \n                                 'HouseBoardR','Import', 'Lights', 'Net', 'OvenStove',\n                                 'Powerpoints', 'Production'\n                                ],\n                futr_exog_list = ['temp_c', 'is_day', 'wind_kph', 'wind_degree',\n                    'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n                    'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain',\n                    'vis_km', 'gust_kph', 'uv']\n               ) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN, GRU, LSTM, TCN, DilatedRNN, MLP, NHITS, NBEATS, NBEATSx]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\ntrain.columns\n\nIndex(['ds', 'Aircon', 'y', 'ConsumptionNet', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'unique_id', 'temp_c', 'is_day', 'wind_kph', 'wind_degree',\n       'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n       'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km',\n       'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconsumption_predict = nf.predict(futr_df=futr_df).reset_index()\nconsumption_predict.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\nConsumption\n2023-10-23\n0.187143\n0.340722\n0.270740\n0.227037\n0.262816\n0.142264\n0.266232\n0.310167\n0.164331\n\n\n1\nConsumption\n2023-10-24\n0.120339\n0.247064\n0.097423\n0.069222\n0.166623\n0.137484\n0.146339\n0.264303\n0.187077\n\n\n2\nConsumption\n2023-10-25\n0.106269\n0.185992\n0.218475\n0.100015\n0.174694\n0.203210\n0.233841\n0.362458\n0.266043\n\n\n3\nConsumption\n2023-10-26\n0.037814\n0.153535\n0.145480\n0.042539\n0.125000\n0.265789\n0.335360\n0.289754\n0.315103\n\n\n4\nConsumption\n2023-10-27\n0.021276\n0.049343\n-0.004718\n0.034827\n0.095918\n0.238553\n0.303091\n0.309346\n0.279027\n\n\n\n\n\n\n\n\nconsumption_predict.drop('unique_id', axis=1, inplace=True)\n\njust_consumption = df[df['unique_id'] == 'Consumption']\n\n\nconsumption_predict_plot = long_form(consumption_predict)\nconsumption_predict_plot = pd.concat([consumption_predict_plot, just_consumption]\n                                     , ignore_index=True)\nconsumption_predict_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-23\nRNN\n0.187143\n\n\n1\n2023-10-24\nRNN\n0.120339\n\n\n2\n2023-10-25\nRNN\n0.106269\n\n\n3\n2023-10-26\nRNN\n0.037814\n\n\n4\n2023-10-27\nRNN\n0.021276\n\n\n...\n...\n...\n...\n\n\n330\n2023-10-28\nConsumption\n0.295413\n\n\n331\n2023-10-29\nConsumption\n0.096052\n\n\n332\n2023-10-30\nConsumption\n0.190600\n\n\n333\n2023-10-31\nConsumption\n0.244137\n\n\n334\n2023-11-01\nConsumption\n0.131755\n\n\n\n\n335 rows × 3 columns\n\n\n\n\naltair_plot(consumption_predict_plot)"
  },
  {
    "objectID": "consumption_model_week.html#performance",
    "href": "consumption_model_week.html#performance",
    "title": "Consumption Model For A Week",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nconsumption_predict.info(), consumption_predict.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   ds          10 non-null     datetime64[ns]\n 1   RNN         10 non-null     float64       \n 2   GRU         10 non-null     float64       \n 3   LSTM        10 non-null     float64       \n 4   TCN         10 non-null     float64       \n 5   DilatedRNN  10 non-null     float64       \n 6   MLP         10 non-null     float64       \n 7   NHITS       10 non-null     float64       \n 8   NBEATS      10 non-null     float64       \n 9   NBEATSx     10 non-null     float64       \ndtypes: datetime64[ns](1), float64(9)\nmemory usage: 932.0 bytes\n\n\n(None,\n Index(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n        'NBEATS', 'NBEATSx'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 1: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nscore = evaluate_performace(train, test, consumption_predict)\nscore.style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\n\n\n\n\n \nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\nmase\n0.827192\n0.475914\n0.581997\n0.854392\n0.532540\n0.891489\n1.069189\n1.064738\n1.062052\n\n\nmae\n0.124269\n0.071497\n0.087433\n0.128355\n0.080004\n0.133928\n0.160624\n0.159956\n0.159552\n\n\nmape\n55.463221\n31.560946\n31.207467\n48.447932\n29.925153\n68.742079\n92.780892\n92.143272\n92.818673\n\n\nrmse\n0.164423\n0.107169\n0.156157\n0.176625\n0.128558\n0.170495\n0.195339\n0.177750\n0.192040\n\n\nsmape\n76.016902\n34.840016\n47.025939\n71.174894\n34.164025\n55.432298\n60.084229\n58.849896\n60.377130\n\n\n\n\n\n\nscore = data_scaler(score.T)\nscore.plot(kind='bar', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nresults = evaluate_performace(train, test, consumption_predict)\nresults.to_csv('Data/Neutral_results5.csv')\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')"
  },
  {
    "objectID": "consumption_model_week.html#cross_validate",
    "href": "consumption_model_week.html#cross_validate",
    "title": "Consumption Model For A Week",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df.columns\n\nIndex(['unique_id', 'ds', 'cutoff', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN',\n       'MLP', 'NHITS', 'NBEATS', 'NBEATSx', 'Aircon', 'y', 'ConsumptionNet',\n       'Export', 'Fridge', 'Garage', 'Hotwater', 'HouseBoardOutside',\n       'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'Lights', 'Net',\n       'OvenStove', 'Powerpoints', 'Production', 'temp_c', 'is_day',\n       'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',\n       'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c', 'dewpoint_c',\n       'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_df['RNN']\n\n0     0.135751\n1     0.069040\n2     0.035342\n3     0.191986\n4     0.166608\n5     0.172630\n6     0.276067\n7     0.224828\n8     0.119483\n9     0.142353\n10    0.102038\n11    0.065945\n12    0.183238\n13    0.203040\n14    0.190365\n15    0.209685\n16    0.239724\n17    0.131889\n18    0.137529\n19    0.309497\n20    0.131816\n21    0.200610\n22    0.181522\n23    0.179420\n24    0.192814\n25    0.191258\n26    0.125263\n27    0.109986\n28    0.267251\n29    0.237799\nName: RNN, dtype: float32\n\n\n\nconsumption_predict.columns\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\ncv_df_filter = cv_df[consumption_predict.columns]\ncv_df_filter\n\n\n\n\n\n\n\n\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\n2023-10-11\n0.135751\n0.067367\n0.110072\n0.087748\n0.100938\n0.102914\n0.160495\n0.176175\n0.128718\n\n\n1\n2023-10-12\n0.069040\n0.149712\n0.197463\n0.148317\n0.126475\n0.112232\n0.078474\n0.145379\n0.088011\n\n\n2\n2023-10-13\n0.035342\n0.187814\n0.194257\n0.140832\n0.177151\n0.041507\n0.077609\n0.112854\n0.052702\n\n\n3\n2023-10-14\n0.191986\n0.232270\n0.253045\n0.228376\n0.199984\n0.159770\n0.178985\n0.242857\n0.067291\n\n\n4\n2023-10-15\n0.166608\n0.128854\n0.169904\n0.126402\n0.164642\n0.215704\n0.143691\n0.212753\n0.167671\n\n\n5\n2023-10-16\n0.172630\n0.208335\n0.227233\n0.223335\n0.202316\n0.139277\n0.256585\n0.268092\n0.178975\n\n\n6\n2023-10-17\n0.276067\n0.281430\n0.284902\n0.354908\n0.246623\n0.305722\n0.252022\n0.270245\n0.243796\n\n\n7\n2023-10-18\n0.224828\n0.219993\n0.226789\n0.329371\n0.222550\n0.262789\n0.249297\n0.274197\n0.279998\n\n\n8\n2023-10-19\n0.119483\n0.204906\n0.162221\n0.165711\n0.210179\n0.112409\n0.100446\n0.252383\n0.193922\n\n\n9\n2023-10-20\n0.142353\n0.226974\n0.191427\n0.111255\n0.154943\n0.302589\n0.209240\n0.218997\n0.152117\n\n\n10\n2023-10-12\n0.102038\n0.111689\n0.155506\n0.097696\n0.139405\n0.065505\n0.063259\n0.111341\n0.082719\n\n\n11\n2023-10-13\n0.065945\n0.173949\n0.210845\n0.121179\n0.138233\n0.151942\n0.068114\n0.124901\n0.112885\n\n\n12\n2023-10-14\n0.183238\n0.209708\n0.224227\n0.221133\n0.225971\n0.141812\n0.198544\n0.221697\n0.127999\n\n\n13\n2023-10-15\n0.203040\n0.189177\n0.187111\n0.244232\n0.242181\n0.160846\n0.196839\n0.221572\n0.114414\n\n\n14\n2023-10-16\n0.190365\n0.249911\n0.216426\n0.253370\n0.215205\n0.200827\n0.154640\n0.255481\n0.214551\n\n\n15\n2023-10-17\n0.209685\n0.212194\n0.246824\n0.305890\n0.202765\n0.186043\n0.265844\n0.213276\n0.161825\n\n\n16\n2023-10-18\n0.239724\n0.239608\n0.244121\n0.271035\n0.233068\n0.306250\n0.258346\n0.316249\n0.219644\n\n\n17\n2023-10-19\n0.131889\n0.198312\n0.168622\n0.159480\n0.207734\n0.131938\n0.148374\n0.250731\n0.293880\n\n\n18\n2023-10-20\n0.137529\n0.205980\n0.201126\n0.135570\n0.194409\n0.162717\n0.189692\n0.192537\n0.217399\n\n\n19\n2023-10-21\n0.309497\n0.257568\n0.222876\n0.194708\n0.225145\n0.306709\n0.249137\n0.223473\n0.185947\n\n\n20\n2023-10-13\n0.131816\n0.154035\n0.225009\n0.131973\n0.137236\n0.070659\n0.026579\n0.064938\n-0.003257\n\n\n21\n2023-10-14\n0.200610\n0.267801\n0.222852\n0.250773\n0.230686\n0.252358\n0.116113\n0.220819\n0.064967\n\n\n22\n2023-10-15\n0.181522\n0.219067\n0.192574\n0.274661\n0.226209\n0.140444\n0.076887\n0.129147\n0.088821\n\n\n23\n2023-10-16\n0.179420\n0.209414\n0.232633\n0.256211\n0.203172\n0.187722\n0.257174\n0.165209\n0.136125\n\n\n24\n2023-10-17\n0.192814\n0.198803\n0.229261\n0.268127\n0.147232\n0.298106\n0.138862\n0.254128\n0.190610\n\n\n25\n2023-10-18\n0.191258\n0.202635\n0.200670\n0.332800\n0.234716\n0.180917\n0.157915\n0.141489\n0.126197\n\n\n26\n2023-10-19\n0.125263\n0.125340\n0.174479\n0.155649\n0.202877\n0.137405\n0.134489\n0.214591\n0.166119\n\n\n27\n2023-10-20\n0.109986\n0.172756\n0.117033\n0.113169\n0.137168\n0.212421\n0.174950\n0.158424\n0.247471\n\n\n28\n2023-10-21\n0.267251\n0.264075\n0.266374\n0.239609\n0.218864\n0.225566\n0.161354\n0.198722\n0.185505\n\n\n29\n2023-10-22\n0.237799\n0.199004\n0.225758\n0.263953\n0.253231\n0.243639\n0.234858\n0.233367\n0.161174\n\n\n\n\n\n\n\n\ncv_df_plot = long_form(cv_df_filter)\n\n\ncv_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-11\nRNN\n0.135751\n\n\n1\n2023-10-12\nRNN\n0.069040\n\n\n2\n2023-10-13\nRNN\n0.035342\n\n\n3\n2023-10-14\nRNN\n0.191986\n\n\n4\n2023-10-15\nRNN\n0.166608\n\n\n...\n...\n...\n...\n\n\n265\n2023-10-18\nNBEATSx\n0.126197\n\n\n266\n2023-10-19\nNBEATSx\n0.166119\n\n\n267\n2023-10-20\nNBEATSx\n0.247471\n\n\n268\n2023-10-21\nNBEATSx\n0.185505\n\n\n269\n2023-10-22\nNBEATSx\n0.161174\n\n\n\n\n270 rows × 3 columns\n\n\n\n\naltair_plot(cv_df_plot)"
  },
  {
    "objectID": "individual_model_lights.html",
    "href": "individual_model_lights.html",
    "title": "LightsModel",
    "section": "",
    "text": "import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nimport pandas as pd\n\nimport warnings\nfrom influxdb_client.client.warnings import MissingPivotFunction\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nwarnings.simplefilter(\"ignore\", MissingPivotFunction)\n\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "individual_model_lights.html#get-data",
    "href": "individual_model_lights.html#get-data",
    "title": "LightsModel",
    "section": "",
    "text": "import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nimport pandas as pd\n\nimport warnings\nfrom influxdb_client.client.warnings import MissingPivotFunction\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nwarnings.simplefilter(\"ignore\", MissingPivotFunction)\n\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "individual_model_lights.html#energy-in-wide-form",
    "href": "individual_model_lights.html#energy-in-wide-form",
    "title": "LightsModel",
    "section": "Energy in Wide Form",
    "text": "Energy in Wide Form\n\nwide_df = result.pivot(index='_time', columns='sensor', values='_value')\n# Reset the index to make 'id' a regular column\nwide_df.reset_index(inplace=True)\n\nwide_df.columns\n\nIndex(['_time', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production', 'Unmonitored'],\n      dtype='object', name='sensor')\n\n\n\nwide_df['_time'] = pd.to_datetime(wide_df['_time']).dt.tz_localize(None)\nwide_df.drop(['Unmonitored', 'Consumption', 'ConsumptionNet'], axis = 1, inplace=True)\nwide_df.rename(columns={'_time':'ds'}, inplace=True)\n# Delete the last row\nwide_df = wide_df.drop(wide_df.index[-1])\n\n\nwide_df_new = pd.DataFrame()\nfor column in wide_df.columns:\n    wide_df_new[f'{column}'] = wide_df[f'{column}'].interpolate()\n\nwide_df = pd.DataFrame(wide_df_new)\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import',\n       'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production'],\n      dtype='object')\n\n\n\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n8.578005\n180.242247\n25.492343\n24.017087\n0.530865\n999.775552\n333.258545\n366.408449\n71.876417\n80.009421\n-108.365830\n66.086281\n176.392548\n491.748285\n\n\n241\n2023-10-29\n8.661682\n806.610093\n19.634587\n2.731102\n0.192283\n530.312474\n176.770805\n201.397413\n1.974071\n60.108551\n-804.636021\n1.253300\n103.451046\n1017.375874\n\n\n242\n2023-10-30\n8.603354\n757.601472\n27.081000\n0.983716\n0.183639\n788.486015\n262.828480\n290.616882\n3.378339\n66.827993\n-754.223132\n1.356785\n178.170521\n1052.272015\n\n\n243\n2023-10-31\n9.032917\n803.841970\n35.649472\n11.163953\n0.269313\n901.165305\n300.388379\n319.795146\n5.027126\n67.972347\n-798.814844\n30.217549\n166.649785\n1142.661342\n\n\n244\n2023-11-01\n8.743938\n898.594991\n30.389375\n3.655479\n0.189410\n623.050504\n207.683462\n245.486118\n2.309883\n50.624125\n-896.285107\n0.213375\n145.941312\n1134.978184\n\n\n\n\n\n\n\n\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n    return df\n\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='y')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='y:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nwide_df = data_scaler(wide_df)\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n0.000327\n0.087269\n0.054518\n0.475787\n0.000988\n0.441317\n0.270647\n0.263152\n0.098826\n0.397064\n0.702175\n0.217335\n0.451213\n0.164710\n\n\n241\n2023-10-29\n0.000459\n0.390543\n0.008991\n0.049113\n0.000167\n0.155476\n0.095349\n0.096613\n0.001842\n0.287229\n0.452268\n0.003559\n0.195780\n0.385757\n\n\n242\n2023-10-30\n0.000367\n0.366814\n0.066865\n0.014087\n0.000146\n0.312670\n0.191751\n0.186659\n0.003790\n0.324314\n0.470363\n0.003900\n0.457440\n0.400433\n\n\n243\n2023-10-31\n0.001045\n0.389203\n0.133461\n0.218148\n0.000354\n0.381277\n0.233826\n0.216107\n0.006078\n0.330630\n0.454358\n0.099064\n0.417095\n0.438445\n\n\n244\n2023-11-01\n0.000589\n0.435080\n0.092579\n0.067642\n0.000160\n0.211942\n0.129978\n0.141110\n0.002308\n0.234883\n0.419373\n0.000130\n0.344577\n0.435214\n\n\n\n\n\n\n\n\ndf = long_form(wide_df)\nwide_df.rename(columns={'Lights': 'y'}, inplace=True)\nwide_df['unique_id'] = 'Lights'\ndf.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-03-02\nAircon\n0.007134\n\n\n1\n2023-03-03\nAircon\n0.178748\n\n\n2\n2023-03-04\nAircon\n0.338990\n\n\n3\n2023-03-05\nAircon\n0.173844\n\n\n4\n2023-03-06\nAircon\n0.363831"
  },
  {
    "objectID": "individual_model_lights.html#weather-data",
    "href": "individual_model_lights.html#weather-data",
    "title": "LightsModel",
    "section": "Weather data",
    "text": "Weather data\n\nimport pandas as pd\npath = 'Data'\nname = 'weather'\n\nweather_df = pd.read_csv(f'{path}/{name}.csv')\nweather_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01 00:00\n21.6\n0\n12.6\n126\n1013.0\n0.0\n84\n73\n21.6\n21.6\n24.4\n18.7\n0\n10.0\n19.8\n1.0\n\n\n1\n2023-01-01 01:00\n21.3\n0\n11.9\n133\n1013.0\n0.0\n85\n77\n21.3\n21.3\n24.3\n18.7\n0\n10.0\n18.7\n1.0\n\n\n2\n2023-01-01 02:00\n21.1\n0\n11.2\n140\n1012.0\n0.0\n86\n80\n21.1\n21.1\n24.2\n18.7\n0\n10.0\n17.6\n1.0\n\n\n3\n2023-01-01 03:00\n20.8\n0\n10.4\n147\n1012.0\n0.0\n88\n84\n20.8\n20.8\n20.8\n18.7\n0\n10.0\n16.6\n1.0\n\n\n4\n2023-01-01 04:00\n21.0\n0\n10.6\n144\n1012.0\n0.0\n86\n84\n21.0\n21.0\n21.0\n18.6\n0\n10.0\n16.3\n1.0\n\n\n\n\n\n\n\n\nweather_df = data_scaler(weather_df)\n\n\ndef data_day_avg(weather_df):\n    from tqdm.notebook import tqdm\n    avg_df = pd.DataFrame()\n    dates = pd.to_datetime(weather_df['ds']).dt.date.unique()\n    \n    for date in tqdm(dates):\n        filtered_df = weather_df[pd.to_datetime(weather_df['ds']).dt.date == date]\n        ds = filtered_df.pop('ds')\n\n        filtered_df = pd.DataFrame(filtered_df.mean()).T\n        filtered_df\n\n        filtered_df.insert(0, 'ds', date)\n\n        avg_df = pd.concat([avg_df, filtered_df], ignore_index=True)\n    \n    return avg_df\n\n\nweather_avg_df = data_day_avg(weather_df)\n\n\n\n\n\nweather_avg_df['ds'] = pd.to_datetime(weather_avg_df['ds'])\n\n\nweather_avg_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01\n0.566503\n0.583333\n0.459098\n0.345636\n0.428728\n0.000895\n0.670886\n0.583750\n0.516791\n0.595085\n0.512881\n0.778121\n0.0\n1.000000\n0.434678\n0.422619\n\n\n1\n2023-01-02\n0.554902\n0.583333\n0.451453\n0.396936\n0.359649\n0.001253\n0.602848\n0.409167\n0.499005\n0.584249\n0.486067\n0.725707\n0.0\n1.000000\n0.434844\n0.428571\n\n\n2\n2023-01-03\n0.582516\n0.583333\n0.263889\n0.415506\n0.258772\n0.000000\n0.584916\n0.287917\n0.525746\n0.610043\n0.502366\n0.738663\n0.0\n1.000000\n0.283533\n0.452381\n\n\n3\n2023-01-04\n0.650490\n0.583333\n0.313073\n0.382660\n0.163377\n0.000895\n0.687764\n0.196250\n0.608582\n0.673535\n0.609621\n0.866902\n0.0\n0.994792\n0.305777\n0.500000\n\n\n4\n2023-01-05\n0.654575\n0.583333\n0.257518\n0.506267\n0.166667\n0.026859\n0.735232\n0.700417\n0.629726\n0.677350\n0.608701\n0.911808\n0.0\n0.989583\n0.288098\n0.416667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\n2023-12-06\n0.580556\n0.583333\n0.254077\n0.264740\n0.558114\n0.000000\n0.608650\n0.174583\n0.519527\n0.608364\n0.515116\n0.745878\n0.0\n1.000000\n0.214558\n0.470238\n\n\n340\n2023-12-07\n0.605065\n0.583333\n0.266565\n0.202066\n0.565789\n0.000000\n0.582806\n0.050000\n0.543532\n0.631258\n0.533386\n0.748233\n0.0\n1.000000\n0.222693\n0.476190\n\n\n341\n2023-12-08\n0.618954\n0.583333\n0.349261\n0.251625\n0.575658\n0.000018\n0.626582\n0.342083\n0.561816\n0.644078\n0.555205\n0.795200\n0.0\n1.000000\n0.285940\n0.476190\n\n\n342\n2023-12-09\n0.612582\n0.583333\n0.362768\n0.190460\n0.544956\n0.000107\n0.577532\n0.497500\n0.564179\n0.638584\n0.539432\n0.757509\n0.0\n1.000000\n0.308267\n0.428571\n\n\n343\n2023-12-10\n0.630556\n0.583333\n0.513507\n0.233751\n0.530702\n0.000233\n0.515295\n0.537500\n0.571269\n0.655067\n0.546924\n0.730271\n0.0\n1.000000\n0.426627\n0.452381\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n(\n    wide_df[['y']].plot(title='Lights')\n)\n\n&lt;Axes: title={'center': 'Lights'}&gt;\n\n\n\n\n\n\nfrom statsforecast import StatsForecast\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nwide_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 245 entries, 0 to 244\nData columns (total 16 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   ds                  245 non-null    datetime64[ns]\n 1   Aircon              245 non-null    float64       \n 2   Export              245 non-null    float64       \n 3   Fridge              245 non-null    float64       \n 4   Garage              245 non-null    float64       \n 5   Hotwater            245 non-null    float64       \n 6   HouseBoardOutside   161 non-null    float64       \n 7   HouseBoardOutsideR  245 non-null    float64       \n 8   HouseBoardR         245 non-null    float64       \n 9   Import              245 non-null    float64       \n 10  y                   245 non-null    float64       \n 11  Net                 245 non-null    float64       \n 12  OvenStove           245 non-null    float64       \n 13  Powerpoints         245 non-null    float64       \n 14  Production          245 non-null    float64       \n 15  unique_id           245 non-null    object        \ndtypes: datetime64[ns](1), float64(14), object(1)\nmemory usage: 30.8+ KB\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3430 entries, 0 to 3429\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         3430 non-null   datetime64[ns]\n 1   unique_id  3430 non-null   object        \n 2   y          3346 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 80.5+ KB\n\n\n\nweather_avg_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   ds              344 non-null    datetime64[ns]\n 1   temp_c          344 non-null    float64       \n 2   is_day          344 non-null    float64       \n 3   wind_kph        344 non-null    float64       \n 4   wind_degree     344 non-null    float64       \n 5   pressure_mb     344 non-null    float64       \n 6   precip_mm       344 non-null    float64       \n 7   humidity        344 non-null    float64       \n 8   cloud           344 non-null    float64       \n 9   feelslike_c     344 non-null    float64       \n 10  windchill_c     344 non-null    float64       \n 11  heatindex_c     344 non-null    float64       \n 12  dewpoint_c      344 non-null    float64       \n 13  chance_of_rain  344 non-null    float64       \n 14  vis_km          344 non-null    float64       \n 15  gust_kph        344 non-null    float64       \n 16  uv              344 non-null    float64       \ndtypes: datetime64[ns](1), float64(16)\nmemory usage: 45.8 KB\n\n\n\ndate_range_start, date_range_end  = wide_df['ds'].min(), wide_df['ds'].max()\n# Filter rows between the start and end dates\nweather_avg_filtered_df = weather_avg_df[(weather_avg_df['ds'] &gt;= date_range_start) & (weather_avg_df['ds'] &lt;= date_range_end)]\nweather_avg_filtered_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.0\n0.588811\n0.345238\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.0\n0.328934\n0.345238\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.0\n0.340139\n0.416667\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.0\n0.470286\n0.464286\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.0\n0.467795\n0.404762\n\n\n\n\n245 rows × 17 columns\n\n\n\n\naltair_plot(df)\n\n\n\n\n\n\n\n\nweather_avg_df_plot = long_form(weather_avg_filtered_df)\naltair_plot(weather_avg_df_plot)\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nwide_df[\"y\"]\n\n0      1.000000\n1      0.219103\n2      0.449558\n3      0.642563\n4      0.401555\n         ...   \n240    0.397064\n241    0.287229\n242    0.324314\n243    0.330630\n244    0.234883\nName: y, Length: 245, dtype: float64\n\n\n\ndef augmented_dickey_fuller_test(series , column_name):\n    from statsmodels.tsa.stattools import adfuller\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\naugmented_dickey_fuller_test(wide_df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic                  -3.110241\np-value                          0.025805\nNo Lags Used                    12.000000\nNumber of observations used    232.000000\nCritical Value (1%)             -3.458855\nCritical Value (5%)             -2.874080\nCritical Value (10%)            -2.573453\ndtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=28, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=28, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=14, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=14, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(wide_df[\"y\"], model = \"additive\", period=28)\na.plot();"
  },
  {
    "objectID": "individual_model_lights.html#designing-models",
    "href": "individual_model_lights.html#designing-models",
    "title": "LightsModel",
    "section": "Designing Models",
    "text": "Designing Models\n\nweather_avg_filtered_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n\n\n\n\nwide_df = pd.merge(wide_df, weather_avg_filtered_df, on='ds', how='outer')\n\n\nwide_df.head()\n\n\n\n\n\n\n\n\nds\nAircon\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\n...\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-03-02\n0.007134\n0.016595\n0.814609\n0.003794\n0.000287\nNaN\n0.356702\n0.364385\n0.085067\n...\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n1\n2023-03-03\n0.178748\n0.165011\n0.669709\n0.015031\n0.000143\nNaN\n0.292065\n0.295309\n0.296304\n...\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n2\n2023-03-04\n0.338990\n0.294620\n0.686509\n0.059559\n0.000460\nNaN\n0.496691\n0.501420\n0.551547\n...\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n3\n2023-03-05\n0.173844\n0.302607\n0.701288\n0.009916\n0.000256\nNaN\n0.419958\n0.422212\n0.467784\n...\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n4\n2023-03-06\n0.363831\n0.381220\n0.773714\n0.008798\n0.002112\nNaN\n0.562640\n0.561444\n0.600856\n...\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n5 rows × 32 columns\n\n\n\n\nhorizon = 10\n\ntrain_size = len(wide_df) - horizon\ntrain, test = wide_df[:train_size], wide_df[train_size:]\n\n\nwide_df.shape, train.shape, test.shape\n\n((245, 32), (235, 32), (10, 32))\n\n\n\nfutr_df=weather_avg_filtered_df[train_size:]\nfutr_df['unique_id'] = 'Lights'\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nLights\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nLights\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nLights\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nLights\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nLights\n\n\n\n\n\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n #FEDformer, #taken too long\n #StemGNN, #need n_series\n #HINT, #need n_series\n #TimesNet # takes too long\n             ]\n\n\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'y',\n       'Net', 'OvenStove', 'Powerpoints', 'Production', 'unique_id', 'temp_c',\n       'is_day', 'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm',\n       'humidity', 'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c',\n       'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nmodels = [model(input_size=5 * horizon,\n                h=horizon,\n                max_steps=2000,\n                hist_exog_list =['Export',\n                                ],\n                futr_exog_list = ['temp_c', 'is_day', 'wind_kph', 'wind_degree',\n                    'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n                    'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain',\n                    'vis_km', 'gust_kph', 'uv']\n               ) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN, GRU, LSTM, TCN, DilatedRNN, MLP, NHITS, NBEATS, NBEATSx]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\ntrain.columns\n\nIndex(['ds', 'Aircon', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'y',\n       'Net', 'OvenStove', 'Powerpoints', 'Production', 'unique_id', 'temp_c',\n       'is_day', 'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm',\n       'humidity', 'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c',\n       'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nLights\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nLights\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nLights\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nLights\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nLights\n\n\n\n\n\n\n\n\nconsumption_predict = nf.predict(futr_df=futr_df).reset_index()\nconsumption_predict.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\nLights\n2023-10-23\n0.507807\n0.379466\n0.478897\n0.511510\n0.563464\n0.161172\n0.416092\n0.450695\n0.442487\n\n\n1\nLights\n2023-10-24\n0.393286\n0.316233\n0.490113\n0.475892\n0.473403\n0.305529\n0.480687\n0.536951\n0.494100\n\n\n2\nLights\n2023-10-25\n0.274335\n0.229077\n0.081534\n0.302572\n0.333799\n0.245025\n0.473535\n0.502184\n0.387711\n\n\n3\nLights\n2023-10-26\n0.301236\n0.221772\n0.089306\n0.287488\n0.292562\n0.086765\n0.323140\n0.422453\n0.293017\n\n\n4\nLights\n2023-10-27\n0.127637\n0.043374\n0.074683\n0.128003\n0.258646\n0.326311\n0.473880\n0.557035\n0.491085\n\n\n\n\n\n\n\n\nconsumption_predict.drop('unique_id', axis=1, inplace=True)\n\n\njust_production = df[df['unique_id'] == 'Lights']\n\n\nconsumption_predict_plot = long_form(consumption_predict)\nconsumption_predict_plot = pd.concat([consumption_predict_plot, just_production.tail(50)]\n                                     , ignore_index=True)\nconsumption_predict_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-23\nRNN\n0.435813\n\n\n1\n2023-10-24\nRNN\n0.471173\n\n\n2\n2023-10-25\nRNN\n0.488459\n\n\n3\n2023-10-26\nRNN\n0.456378\n\n\n4\n2023-10-27\nRNN\n0.370659\n\n\n...\n...\n...\n...\n\n\n135\n2023-10-28\nLights\n0.397064\n\n\n136\n2023-10-29\nLights\n0.287229\n\n\n137\n2023-10-30\nLights\n0.324314\n\n\n138\n2023-10-31\nLights\n0.330630\n\n\n139\n2023-11-01\nLights\n0.234883\n\n\n\n\n140 rows × 3 columns\n\n\n\n\naltair_plot(consumption_predict_plot)"
  },
  {
    "objectID": "individual_model_lights.html#performance",
    "href": "individual_model_lights.html#performance",
    "title": "LightsModel",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nconsumption_predict.info(), consumption_predict.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   ds          10 non-null     datetime64[ns]\n 1   RNN         10 non-null     float64       \n 2   GRU         10 non-null     float64       \n 3   LSTM        10 non-null     float64       \n 4   TCN         10 non-null     float64       \n 5   DilatedRNN  10 non-null     float64       \n 6   MLP         10 non-null     float64       \n 7   NHITS       10 non-null     float64       \n 8   NBEATS      10 non-null     float64       \n 9   NBEATSx     10 non-null     float64       \ndtypes: datetime64[ns](1), float64(9)\nmemory usage: 932.0 bytes\n\n\n(None,\n Index(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n        'NBEATS', 'NBEATSx'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nscore = evaluate_performace(train, test, consumption_predict)\nscore.style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\n\n\n\n\n \nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\nmase\n0.371110\n0.373379\n0.376333\n0.386538\n0.426027\n0.392198\n0.419957\n0.429512\n\n\nmae\n0.048412\n0.048708\n0.049094\n0.050425\n0.055577\n0.051163\n0.054785\n0.056031\n\n\nmape\n259.057162\n160.017654\n375.530638\n807.648400\n1278.895162\n616.177544\n1099.112504\n2281.618162\n\n\nrmse\n0.086880\n0.088143\n0.087256\n0.085695\n0.088678\n0.088601\n0.090735\n0.086129\n\n\nsmape\n138.050393\n137.216017\n143.938382\n171.561234\n187.530805\n164.534808\n173.732277\n187.285272\n\n\n\n\n\n\nscore = data_scaler(score.T)\nscore.plot(kind='bar', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nresults = evaluate_performace(train, test, consumption_predict)\nresults.to_csv('Data/Neutral_results6.csv')\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')"
  },
  {
    "objectID": "individual_model_lights.html#cross_validate",
    "href": "individual_model_lights.html#cross_validate",
    "title": "LightsModel",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df.columns\n\nIndex(['unique_id', 'ds', 'cutoff', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN',\n       'MLP', 'NHITS', 'NBEATS', 'NBEATSx', 'y', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'temp_c', 'is_day', 'wind_kph', 'wind_degree', 'pressure_mb',\n       'precip_mm', 'humidity', 'cloud', 'feelslike_c', 'windchill_c',\n       'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph',\n       'uv'],\n      dtype='object')\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_df['RNN'].head()\n\n0    0.003673\n1    0.003411\n2    0.005272\n3    0.004834\n4    0.003371\nName: RNN, dtype: float32\n\n\n\nconsumption_predict.columns\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\ncv_df_filter = cv_df[consumption_predict.columns]\ncv_df_filter.head()\n\n\n\n\n\n\n\n\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\n2023-10-11\n0.003673\n0.005021\n0.004987\n0.005792\n0.005489\n-0.009889\n-0.003473\n-0.001256\n-0.030136\n\n\n1\n2023-10-12\n0.003411\n0.005343\n0.005253\n0.005539\n0.006155\n-0.005851\n-0.002714\n0.004471\n-0.001906\n\n\n2\n2023-10-13\n0.005272\n0.004845\n0.005999\n0.007143\n0.004875\n-0.002195\n-0.004884\n-0.004693\n-0.008124\n\n\n3\n2023-10-14\n0.004834\n0.004222\n0.005454\n0.007220\n0.005831\n-0.001914\n-0.004632\n0.003145\n0.017054\n\n\n4\n2023-10-15\n0.003371\n0.004710\n0.005032\n0.006623\n0.005268\n-0.005891\n-0.005599\n0.005866\n-0.004327\n\n\n\n\n\n\n\n\ncv_df_plot = long_form(cv_df_filter)\n\n\ncv_df_plot.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-11\nRNN\n0.003673\n\n\n1\n2023-10-12\nRNN\n0.003411\n\n\n2\n2023-10-13\nRNN\n0.005272\n\n\n3\n2023-10-14\nRNN\n0.004834\n\n\n4\n2023-10-15\nRNN\n0.003371\n\n\n\n\n\n\n\n\naltair_plot(cv_df_plot)"
  },
  {
    "objectID": "optiver.html",
    "href": "optiver.html",
    "title": "Optiver - Trading at the Close",
    "section": "",
    "text": "download data from kaggle\nvisalize data using mitosheet\nclean up data\ncreate prediction model\ntest model\nupload submission\n\n\nfrom nbdevAuto.functions import * \nimport nbdevAuto.functions\n\n\nname = 'optiver-trading-at-the-close'\nkaggle_download(name)\n\nfile exists\n\n\n\nimport pandas as pd \nimport numpy as np\nimport plotly.express as px\nfrom fastai.vision.all import *\nset_seed(42)\n\n\npath = Path('Data/optiver-trading-at-the-close/')\ntrain = pd.read_csv(path/'train.csv')\n\n\n(\n    train.query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','bid_price','ask_price', 'wap']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the order book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the order book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train.query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','bid_price','ask_price', 'wap']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the order book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the order book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train\n    .query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','near_price','far_price','reference_price']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title = 'Stock 0 on Day 0 - How the auction & combined book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the auction & combined book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train\n    .query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','imbalance_size','matched_size']]\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the auction sizing changes during the auction period')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the auction sizing changes during the auction period'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\ntrain['target']\n\n0         -3.029704\n1         -5.519986\n2         -8.389950\n3         -4.010200\n4         -7.349849\n             ...   \n5237975    2.310276\n5237976   -8.220077\n5237977    1.169443\n5237978   -1.540184\n5237979   -6.530285\nName: target, Length: 5237980, dtype: float64\n\n\n\nsimple_mapping = {\n    1: 0.1,\n    0: 0,\n    -1: -0.1\n}\n\ntrain['baseline_prediction'] = 0\ntrain['simple_prediction'] = train['imbalance_buy_sell_flag'].map(simple_mapping)\n\n\nbaseline_mae = (train['baseline_prediction'] - train['target']).abs().mean()\nsimple_prediction_mae = (train['simple_prediction'] - train['target']).abs().mean()\n\nprint(baseline_mae, simple_prediction_mae)\nprint('MAE improvement in basis points: ', (baseline_mae-simple_prediction_mae))\n\n6.40777074811524 6.407056596608261\nMAE improvement in basis points:  0.000714151506978844\n\n\n\nimport mitosheet\n\n\nmitosheet.sheet(train, analysis_to_replay=\"id-mdlltizbxo\")\n\n\n\n        \n    \n\n\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial rn26d 128px', name)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'subm.csv'"
  },
  {
    "objectID": "optiver.html#objectives",
    "href": "optiver.html#objectives",
    "title": "Optiver - Trading at the Close",
    "section": "",
    "text": "download data from kaggle\nvisalize data using mitosheet\nclean up data\ncreate prediction model\ntest model\nupload submission\n\n\nfrom nbdevAuto.functions import * \nimport nbdevAuto.functions\n\n\nname = 'optiver-trading-at-the-close'\nkaggle_download(name)\n\nfile exists\n\n\n\nimport pandas as pd \nimport numpy as np\nimport plotly.express as px\nfrom fastai.vision.all import *\nset_seed(42)\n\n\npath = Path('Data/optiver-trading-at-the-close/')\ntrain = pd.read_csv(path/'train.csv')\n\n\n(\n    train.query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','bid_price','ask_price', 'wap']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the order book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the order book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train.query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','bid_price','ask_price', 'wap']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the order book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the order book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train\n    .query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','near_price','far_price','reference_price']]\n    .replace(0, np.nan)\n    .set_index('seconds_in_bucket')\n    .plot(title = 'Stock 0 on Day 0 - How the auction & combined book pricing changes during the auction')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the auction & combined book pricing changes during the auction'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\n(\n    train\n    .query('stock_id ==0 & date_id ==0')\n    [['seconds_in_bucket','imbalance_size','matched_size']]\n    .set_index('seconds_in_bucket')\n    .plot(title='Stock 0 on Day 0 - How the auction sizing changes during the auction period')\n)\n\n&lt;Axes: title={'center': 'Stock 0 on Day 0 - How the auction sizing changes during the auction period'}, xlabel='seconds_in_bucket'&gt;\n\n\n\n\n\n\ntrain['target']\n\n0         -3.029704\n1         -5.519986\n2         -8.389950\n3         -4.010200\n4         -7.349849\n             ...   \n5237975    2.310276\n5237976   -8.220077\n5237977    1.169443\n5237978   -1.540184\n5237979   -6.530285\nName: target, Length: 5237980, dtype: float64\n\n\n\nsimple_mapping = {\n    1: 0.1,\n    0: 0,\n    -1: -0.1\n}\n\ntrain['baseline_prediction'] = 0\ntrain['simple_prediction'] = train['imbalance_buy_sell_flag'].map(simple_mapping)\n\n\nbaseline_mae = (train['baseline_prediction'] - train['target']).abs().mean()\nsimple_prediction_mae = (train['simple_prediction'] - train['target']).abs().mean()\n\nprint(baseline_mae, simple_prediction_mae)\nprint('MAE improvement in basis points: ', (baseline_mae-simple_prediction_mae))\n\n6.40777074811524 6.407056596608261\nMAE improvement in basis points:  0.000714151506978844\n\n\n\nimport mitosheet\n\n\nmitosheet.sheet(train, analysis_to_replay=\"id-mdlltizbxo\")\n\n\n\n        \n    \n\n\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial rn26d 128px', name)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'subm.csv'"
  },
  {
    "objectID": "solar_model.html",
    "href": "solar_model.html",
    "title": "Solar Model",
    "section": "",
    "text": "result = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "solar_model.html#get-data",
    "href": "solar_model.html#get-data",
    "title": "Solar Model",
    "section": "",
    "text": "result = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "solar_model.html#energy-in-wide-form",
    "href": "solar_model.html#energy-in-wide-form",
    "title": "Solar Model",
    "section": "Energy in Wide Form",
    "text": "Energy in Wide Form\n\nwide_df = result.pivot(index='_time', columns='sensor', values='_value')\n# Reset the index to make 'id' a regular column\nwide_df.reset_index(inplace=True)\n\nwide_df.columns\n\nIndex(['_time', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production', 'Unmonitored'],\n      dtype='object', name='sensor')\n\n\n\nwide_df['_time'] = pd.to_datetime(wide_df['_time']).dt.tz_localize(None)\nwide_df.drop(['Unmonitored', 'Consumption', 'ConsumptionNet', 'Export'], axis = 1, inplace=True)\nwide_df.rename(columns={'_time':'ds'}, inplace=True)\n# Delete the last row\nwide_df = wide_df.drop(wide_df.index[-1])\n\n\nwide_df_new = pd.DataFrame()\nfor column in wide_df.columns:\n    wide_df_new[f'{column}'] = wide_df[f'{column}'].interpolate()\n\nwide_df = pd.DataFrame(wide_df_new)\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Fridge', 'Garage', 'Hotwater', 'HouseBoardOutside',\n       'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'Lights', 'Net',\n       'OvenStove', 'Powerpoints', 'Production'],\n      dtype='object')\n\n\n\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n8.578005\n25.492343\n24.017087\n0.530865\n999.775552\n333.258545\n366.408449\n71.876417\n80.009421\n-108.365830\n66.086281\n176.392548\n491.748285\n\n\n241\n2023-10-29\n8.661682\n19.634587\n2.731102\n0.192283\n530.312474\n176.770805\n201.397413\n1.974071\n60.108551\n-804.636021\n1.253300\n103.451046\n1017.375874\n\n\n242\n2023-10-30\n8.603354\n27.081000\n0.983716\n0.183639\n788.486015\n262.828480\n290.616882\n3.378339\n66.827993\n-754.223132\n1.356785\n178.170521\n1052.272015\n\n\n243\n2023-10-31\n9.032917\n35.649472\n11.163953\n0.269313\n901.165305\n300.388379\n319.795146\n5.027126\n67.972347\n-798.814844\n30.217549\n166.649785\n1142.661342\n\n\n244\n2023-11-01\n8.743938\n30.389375\n3.655479\n0.189410\n623.050504\n207.683462\n245.486118\n2.309883\n50.624125\n-896.285107\n0.213375\n145.941312\n1134.978184\n\n\n\n\n\n\n\n\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n    return df\n\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='y')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='y:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nwide_df = data_scaler(wide_df)\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n0.000327\n0.054518\n0.475787\n0.000988\n0.441317\n0.270647\n0.263152\n0.098826\n0.397064\n0.702175\n0.217335\n0.451213\n0.164710\n\n\n241\n2023-10-29\n0.000459\n0.008991\n0.049113\n0.000167\n0.155476\n0.095349\n0.096613\n0.001842\n0.287229\n0.452268\n0.003559\n0.195780\n0.385757\n\n\n242\n2023-10-30\n0.000367\n0.066865\n0.014087\n0.000146\n0.312670\n0.191751\n0.186659\n0.003790\n0.324314\n0.470363\n0.003900\n0.457440\n0.400433\n\n\n243\n2023-10-31\n0.001045\n0.133461\n0.218148\n0.000354\n0.381277\n0.233826\n0.216107\n0.006078\n0.330630\n0.454358\n0.099064\n0.417095\n0.438445\n\n\n244\n2023-11-01\n0.000589\n0.092579\n0.067642\n0.000160\n0.211942\n0.129978\n0.141110\n0.002308\n0.234883\n0.419373\n0.000130\n0.344577\n0.435214\n\n\n\n\n\n\n\n\ndf = long_form(wide_df)\nwide_df.rename(columns={'Production': 'y'}, inplace=True)\nwide_df['unique_id'] = 'Production'\ndf.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-03-02\nAircon\n0.007134\n\n\n1\n2023-03-03\nAircon\n0.178748\n\n\n2\n2023-03-04\nAircon\n0.338990\n\n\n3\n2023-03-05\nAircon\n0.173844\n\n\n4\n2023-03-06\nAircon\n0.363831"
  },
  {
    "objectID": "solar_model.html#weather-data",
    "href": "solar_model.html#weather-data",
    "title": "Solar Model",
    "section": "Weather data",
    "text": "Weather data\n\nimport pandas as pd\npath = 'Data'\nname = 'weather'\n\nweather_df = pd.read_csv(f'{path}/{name}.csv')\nweather_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01 00:00\n21.6\n0\n12.6\n126\n1013.0\n0.0\n84\n73\n21.6\n21.6\n24.4\n18.7\n0\n10.0\n19.8\n1.0\n\n\n1\n2023-01-01 01:00\n21.3\n0\n11.9\n133\n1013.0\n0.0\n85\n77\n21.3\n21.3\n24.3\n18.7\n0\n10.0\n18.7\n1.0\n\n\n2\n2023-01-01 02:00\n21.1\n0\n11.2\n140\n1012.0\n0.0\n86\n80\n21.1\n21.1\n24.2\n18.7\n0\n10.0\n17.6\n1.0\n\n\n3\n2023-01-01 03:00\n20.8\n0\n10.4\n147\n1012.0\n0.0\n88\n84\n20.8\n20.8\n20.8\n18.7\n0\n10.0\n16.6\n1.0\n\n\n4\n2023-01-01 04:00\n21.0\n0\n10.6\n144\n1012.0\n0.0\n86\n84\n21.0\n21.0\n21.0\n18.6\n0\n10.0\n16.3\n1.0\n\n\n\n\n\n\n\n\nweather_df = data_scaler(weather_df)\n\n\ndef data_day_avg(weather_df):\n    from tqdm.notebook import tqdm\n    avg_df = pd.DataFrame()\n    dates = pd.to_datetime(weather_df['ds']).dt.date.unique()\n    \n    for date in tqdm(dates):\n        filtered_df = weather_df[pd.to_datetime(weather_df['ds']).dt.date == date]\n        ds = filtered_df.pop('ds')\n\n        filtered_df = pd.DataFrame(filtered_df.mean()).T\n        filtered_df\n\n        filtered_df.insert(0, 'ds', date)\n\n        avg_df = pd.concat([avg_df, filtered_df], ignore_index=True)\n    \n    return avg_df\n\n\nweather_avg_df = data_day_avg(weather_df)\n\n\n\n\n\nweather_avg_df['ds'] = pd.to_datetime(weather_avg_df['ds'])\n\n\nweather_avg_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01\n0.566503\n0.583333\n0.459098\n0.345636\n0.428728\n0.000895\n0.670886\n0.583750\n0.516791\n0.595085\n0.512881\n0.778121\n0.0\n1.000000\n0.434678\n0.422619\n\n\n1\n2023-01-02\n0.554902\n0.583333\n0.451453\n0.396936\n0.359649\n0.001253\n0.602848\n0.409167\n0.499005\n0.584249\n0.486067\n0.725707\n0.0\n1.000000\n0.434844\n0.428571\n\n\n2\n2023-01-03\n0.582516\n0.583333\n0.263889\n0.415506\n0.258772\n0.000000\n0.584916\n0.287917\n0.525746\n0.610043\n0.502366\n0.738663\n0.0\n1.000000\n0.283533\n0.452381\n\n\n3\n2023-01-04\n0.650490\n0.583333\n0.313073\n0.382660\n0.163377\n0.000895\n0.687764\n0.196250\n0.608582\n0.673535\n0.609621\n0.866902\n0.0\n0.994792\n0.305777\n0.500000\n\n\n4\n2023-01-05\n0.654575\n0.583333\n0.257518\n0.506267\n0.166667\n0.026859\n0.735232\n0.700417\n0.629726\n0.677350\n0.608701\n0.911808\n0.0\n0.989583\n0.288098\n0.416667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\n2023-12-06\n0.580556\n0.583333\n0.254077\n0.264740\n0.558114\n0.000000\n0.608650\n0.174583\n0.519527\n0.608364\n0.515116\n0.745878\n0.0\n1.000000\n0.214558\n0.470238\n\n\n340\n2023-12-07\n0.605065\n0.583333\n0.266565\n0.202066\n0.565789\n0.000000\n0.582806\n0.050000\n0.543532\n0.631258\n0.533386\n0.748233\n0.0\n1.000000\n0.222693\n0.476190\n\n\n341\n2023-12-08\n0.618954\n0.583333\n0.349261\n0.251625\n0.575658\n0.000018\n0.626582\n0.342083\n0.561816\n0.644078\n0.555205\n0.795200\n0.0\n1.000000\n0.285940\n0.476190\n\n\n342\n2023-12-09\n0.612582\n0.583333\n0.362768\n0.190460\n0.544956\n0.000107\n0.577532\n0.497500\n0.564179\n0.638584\n0.539432\n0.757509\n0.0\n1.000000\n0.308267\n0.428571\n\n\n343\n2023-12-10\n0.630556\n0.583333\n0.513507\n0.233751\n0.530702\n0.000233\n0.515295\n0.537500\n0.571269\n0.655067\n0.546924\n0.730271\n0.0\n1.000000\n0.426627\n0.452381\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n(\n    wide_df[['y']].plot(title='Production')\n)\n\n&lt;Axes: title={'center': 'Production'}&gt;\n\n\n\n\n\n\nfrom statsforecast import StatsForecast\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nwide_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 245 entries, 0 to 244\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   ds                  245 non-null    datetime64[ns]\n 1   Aircon              245 non-null    float64       \n 2   Fridge              245 non-null    float64       \n 3   Garage              245 non-null    float64       \n 4   Hotwater            245 non-null    float64       \n 5   HouseBoardOutside   161 non-null    float64       \n 6   HouseBoardOutsideR  245 non-null    float64       \n 7   HouseBoardR         245 non-null    float64       \n 8   Import              245 non-null    float64       \n 9   Lights              245 non-null    float64       \n 10  Net                 245 non-null    float64       \n 11  OvenStove           245 non-null    float64       \n 12  Powerpoints         245 non-null    float64       \n 13  y                   245 non-null    float64       \n 14  unique_id           245 non-null    object        \ndtypes: datetime64[ns](1), float64(13), object(1)\nmemory usage: 28.8+ KB\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3185 entries, 0 to 3184\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         3185 non-null   datetime64[ns]\n 1   unique_id  3185 non-null   object        \n 2   y          3101 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 74.8+ KB\n\n\n\nweather_avg_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   ds              344 non-null    datetime64[ns]\n 1   temp_c          344 non-null    float64       \n 2   is_day          344 non-null    float64       \n 3   wind_kph        344 non-null    float64       \n 4   wind_degree     344 non-null    float64       \n 5   pressure_mb     344 non-null    float64       \n 6   precip_mm       344 non-null    float64       \n 7   humidity        344 non-null    float64       \n 8   cloud           344 non-null    float64       \n 9   feelslike_c     344 non-null    float64       \n 10  windchill_c     344 non-null    float64       \n 11  heatindex_c     344 non-null    float64       \n 12  dewpoint_c      344 non-null    float64       \n 13  chance_of_rain  344 non-null    float64       \n 14  vis_km          344 non-null    float64       \n 15  gust_kph        344 non-null    float64       \n 16  uv              344 non-null    float64       \ndtypes: datetime64[ns](1), float64(16)\nmemory usage: 45.8 KB\n\n\n\ndate_range_start, date_range_end  = wide_df['ds'].min(), wide_df['ds'].max()\n# Filter rows between the start and end dates\nweather_avg_filtered_df = weather_avg_df[(weather_avg_df['ds'] &gt;= date_range_start) & (weather_avg_df['ds'] &lt;= date_range_end)]\nweather_avg_filtered_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.0\n0.588811\n0.345238\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.0\n0.328934\n0.345238\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.0\n0.340139\n0.416667\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.0\n0.470286\n0.464286\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.0\n0.467795\n0.404762\n\n\n\n\n245 rows × 17 columns\n\n\n\n\naltair_plot(df)\n\n\n\n\n\n\n\n\nweather_avg_df_plot = long_form(weather_avg_filtered_df)\naltair_plot(weather_avg_df_plot)\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nwide_df[\"y\"]\n\n0      0.128585\n1      0.168068\n2      0.283984\n3      0.285599\n4      0.368173\n         ...   \n240    0.164710\n241    0.385757\n242    0.400433\n243    0.438445\n244    0.435214\nName: y, Length: 245, dtype: float64\n\n\n\ndef augmented_dickey_fuller_test(series , column_name):\n    from statsmodels.tsa.stattools import adfuller\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\naugmented_dickey_fuller_test(wide_df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic                  -4.863065\np-value                          0.000041\nNo Lags Used                     2.000000\nNumber of observations used    242.000000\nCritical Value (1%)             -3.457664\nCritical Value (5%)             -2.873559\nCritical Value (10%)            -2.573175\ndtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=28, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=28, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=14, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=14, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(wide_df[\"y\"], model = \"additive\", period=28)\na.plot();"
  },
  {
    "objectID": "solar_model.html#designing-models",
    "href": "solar_model.html#designing-models",
    "title": "Solar Model",
    "section": "Designing Models",
    "text": "Designing Models\n\nweather_avg_filtered_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n\n\n\n\nwide_df = pd.merge(wide_df, weather_avg_filtered_df, on='ds', how='outer')\n\n\nwide_df.head()\n\n\n\n\n\n\n\n\nds\nAircon\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\n...\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-03-02\n0.007134\n0.814609\n0.003794\n0.000287\nNaN\n0.356702\n0.364385\n0.085067\n1.000000\n...\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n1\n2023-03-03\n0.178748\n0.669709\n0.015031\n0.000143\nNaN\n0.292065\n0.295309\n0.296304\n0.219103\n...\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n2\n2023-03-04\n0.338990\n0.686509\n0.059559\n0.000460\nNaN\n0.496691\n0.501420\n0.551547\n0.449558\n...\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n3\n2023-03-05\n0.173844\n0.701288\n0.009916\n0.000256\nNaN\n0.419958\n0.422212\n0.467784\n0.642563\n...\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n4\n2023-03-06\n0.363831\n0.773714\n0.008798\n0.002112\nNaN\n0.562640\n0.561444\n0.600856\n0.401555\n...\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nhorizon = 10\n\ntrain_size = len(wide_df) - horizon\ntrain, test = wide_df[:train_size], wide_df[train_size:]\n\n\nwide_df.shape, train.shape, test.shape\n\n((245, 31), (235, 31), (10, 31))\n\n\n\nfutr_df=weather_avg_filtered_df[train_size:]\nfutr_df['unique_id'] = 'Production'\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nProduction\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nProduction\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nProduction\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nProduction\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nProduction\n\n\n\n\n\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n #FEDformer, #taken too long\n #StemGNN, #need n_series\n #HINT, #need n_series\n #TimesNet # takes too long\n             ]\n\n\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Fridge', 'Garage', 'Hotwater', 'HouseBoardOutside',\n       'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'Lights', 'Net',\n       'OvenStove', 'Powerpoints', 'y', 'unique_id', 'temp_c', 'is_day',\n       'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',\n       'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c', 'dewpoint_c',\n       'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nmodels = [model(input_size=2 * horizon,\n                h=horizon,\n                max_steps=2000,\n                hist_exog_list =['Aircon', 'Fridge', 'Garage',\n                                 'Hotwater', 'HouseBoardR', 'Lights', 'Net',\n                                 'OvenStove','Powerpoints'\n                                ],\n                futr_exog_list = ['temp_c', 'is_day', 'wind_kph', 'wind_degree',\n                    'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n                    'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain',\n                    'vis_km', 'gust_kph', 'uv']\n               ) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN, GRU, LSTM, TCN, DilatedRNN, MLP, NHITS, NBEATS, NBEATSx]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\ntrain.columns\n\nIndex(['ds', 'Aircon', 'Fridge', 'Garage', 'Hotwater', 'HouseBoardOutside',\n       'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'Lights', 'Net',\n       'OvenStove', 'Powerpoints', 'y', 'unique_id', 'temp_c', 'is_day',\n       'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',\n       'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c', 'dewpoint_c',\n       'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nProduction\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nProduction\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nProduction\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nProduction\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nProduction\n\n\n\n\n\n\n\n\nconsumption_predict = nf.predict(futr_df=futr_df).reset_index()\nconsumption_predict.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\nProduction\n2023-10-23\n0.507700\n0.449085\n0.476290\n0.490807\n0.548747\n0.382190\n0.446692\n0.384382\n0.424434\n\n\n1\nProduction\n2023-10-24\n0.525197\n0.460444\n0.494279\n0.507169\n0.497478\n0.431177\n0.445254\n0.338512\n0.455130\n\n\n2\nProduction\n2023-10-25\n0.453300\n0.369578\n0.385844\n0.400484\n0.412231\n0.411991\n0.466954\n0.277904\n0.433808\n\n\n3\nProduction\n2023-10-26\n0.366417\n0.374538\n0.348836\n0.326042\n0.442334\n0.305548\n0.374557\n0.262405\n0.404792\n\n\n4\nProduction\n2023-10-27\n0.174317\n0.246273\n0.306424\n0.160848\n0.238782\n0.205710\n0.382070\n0.221150\n0.392989\n\n\n\n\n\n\n\n\nconsumption_predict.drop('unique_id', axis=1, inplace=True)\n\n\njust_production = df[df['unique_id'] == 'Production']\n\n\nconsumption_predict_plot = long_form(consumption_predict)\nconsumption_predict_plot = pd.concat([consumption_predict_plot, just_production.tail(50)]\n                                     , ignore_index=True)\nconsumption_predict_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-23\nRNN\n0.507700\n\n\n1\n2023-10-24\nRNN\n0.525197\n\n\n2\n2023-10-25\nRNN\n0.453300\n\n\n3\n2023-10-26\nRNN\n0.366417\n\n\n4\n2023-10-27\nRNN\n0.174317\n\n\n...\n...\n...\n...\n\n\n135\n2023-10-28\nProduction\n0.164710\n\n\n136\n2023-10-29\nProduction\n0.385757\n\n\n137\n2023-10-30\nProduction\n0.400433\n\n\n138\n2023-10-31\nProduction\n0.438445\n\n\n139\n2023-11-01\nProduction\n0.435214\n\n\n\n\n140 rows × 3 columns\n\n\n\n\nplot_predict = consumption_predict.copy()\nplot_predict.pop('ds')\n\n0   2023-10-23\n1   2023-10-24\n2   2023-10-25\n3   2023-10-26\n4   2023-10-27\n5   2023-10-28\n6   2023-10-29\n7   2023-10-30\n8   2023-10-31\n9   2023-11-01\nName: ds, dtype: datetime64[ns]\n\n\n\nplot_predict.plot(kind='line', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\naltair_plot(consumption_predict_plot)"
  },
  {
    "objectID": "solar_model.html#performance",
    "href": "solar_model.html#performance",
    "title": "Solar Model",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nconsumption_predict.info(), consumption_predict.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   ds          10 non-null     datetime64[ns]\n 1   RNN         10 non-null     float64       \n 2   GRU         10 non-null     float64       \n 3   LSTM        10 non-null     float64       \n 4   TCN         10 non-null     float64       \n 5   DilatedRNN  10 non-null     float64       \n 6   MLP         10 non-null     float64       \n 7   NHITS       10 non-null     float64       \n 8   NBEATS      10 non-null     float64       \n 9   NBEATSx     10 non-null     float64       \ndtypes: datetime64[ns](1), float64(9)\nmemory usage: 932.0 bytes\n\n\n(None,\n Index(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n        'NBEATS', 'NBEATSx'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nscore = evaluate_performace(train, test, consumption_predict)\nscore.style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\n\n\n\n\n \nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\nmase\n0.569339\n0.744755\n0.822659\n0.638444\n0.759240\n0.583159\n0.968761\n0.650740\n\n\nmae\n0.061258\n0.080132\n0.088514\n0.068694\n0.081691\n0.062745\n0.104234\n0.070016\n\n\nmape\n15.308022\n21.804741\n24.603571\n20.118973\n23.460916\n22.128259\n26.571337\n24.593907\n\n\nrmse\n0.069256\n0.093225\n0.099134\n0.077298\n0.098223\n0.088908\n0.127561\n0.092721\n\n\nsmape\n15.845139\n20.093627\n26.026108\n18.411672\n23.525816\n17.278618\n29.432969\n18.838586\n\n\n\n\n\n\nscore = data_scaler(score.T)\nscore.plot(kind='bar', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nresults = evaluate_performace(train, test, consumption_predict)\nresults.to_csv('Data/Neutral_results6.csv')\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')"
  },
  {
    "objectID": "solar_model.html#cross_validate",
    "href": "solar_model.html#cross_validate",
    "title": "Solar Model",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df.columns\n\nIndex(['unique_id', 'ds', 'cutoff', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN',\n       'MLP', 'NHITS', 'NBEATS', 'NBEATSx', 'Aircon', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'y', 'temp_c',\n       'is_day', 'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm',\n       'humidity', 'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c',\n       'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\nconsumption_predict.columns\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\ncv_df_filter = cv_df[consumption_predict.columns]\ncv_df_filter.head()\n\n\n\n\n\n\n\n\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\n2023-10-11\n0.349310\n0.365361\n0.401426\n0.379176\n0.384931\n0.326775\n0.389734\n0.388676\n0.384356\n\n\n1\n2023-10-12\n0.360853\n0.377349\n0.384142\n0.383169\n0.387812\n0.377452\n0.383780\n0.432581\n0.410671\n\n\n2\n2023-10-13\n0.365891\n0.360561\n0.330846\n0.379273\n0.388241\n0.352880\n0.375882\n0.392877\n0.410011\n\n\n3\n2023-10-14\n0.440600\n0.430985\n0.386927\n0.429745\n0.445192\n0.348349\n0.413572\n0.434086\n0.456314\n\n\n4\n2023-10-15\n0.414007\n0.411958\n0.409442\n0.380501\n0.400195\n0.357064\n0.387844\n0.427270\n0.407692\n\n\n\n\n\n\n\n\ncv_df_plot = long_form(cv_df_filter)\n\n\ncv_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-11\nRNN\n0.349310\n\n\n1\n2023-10-12\nRNN\n0.360853\n\n\n2\n2023-10-13\nRNN\n0.365891\n\n\n3\n2023-10-14\nRNN\n0.440600\n\n\n4\n2023-10-15\nRNN\n0.414007\n\n\n...\n...\n...\n...\n\n\n265\n2023-10-18\nNBEATSx\n0.390957\n\n\n266\n2023-10-19\nNBEATSx\n0.334832\n\n\n267\n2023-10-20\nNBEATSx\n0.340729\n\n\n268\n2023-10-21\nNBEATSx\n0.282658\n\n\n269\n2023-10-22\nNBEATSx\n0.410265\n\n\n\n\n270 rows × 3 columns\n\n\n\n\naltair_plot(cv_df_plot)"
  },
  {
    "objectID": "neural_models.html",
    "href": "neural_models.html",
    "title": "Neural Learning Models",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\nimport matplotlib.pyplot as plt\n\n# Generate a sine wave for demonstration\n\n\nfrom tqdm.autonotebook import tqdm\nfrom statsforecast import StatsForecast\n\nfrom statsforecast.utils import AirPassengersDF, ConformalIntervals\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, \n                cycles * np.pi,\n                weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\nsine_wave = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': sine_wave})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   y          364 non-null    float64       \n 1   unique_id  364 non-null    int64         \n 2   ds         364 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 8.7 KB\n\n\n\ndf_plot = df.copy()\ndf_plot.rename(columns={'y': 'final'}, inplace=True)\ndf_plot['unique_id'] = 'final'\ndf_plot['first'] = np.sin(t)\ndf_plot['second'] = 0.3*np.sin(5*t)\ndf_plot['noise'] = np.random.normal(scale=0.2, size=len(t))\ndf_plot.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   final      364 non-null    float64       \n 1   unique_id  364 non-null    object        \n 2   ds         364 non-null    datetime64[ns]\n 3   first      364 non-null    float64       \n 4   second     364 non-null    float64       \n 5   noise      364 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1)\nmemory usage: 17.2+ KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\ndf_plot = long_form(df_plot)\naltair_plot(df_plot)\n\n\n\n\n\n\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=28)\na.plot();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "neural_models.html#data-extraction",
    "href": "neural_models.html#data-extraction",
    "title": "Neural Learning Models",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\nimport matplotlib.pyplot as plt\n\n# Generate a sine wave for demonstration\n\n\nfrom tqdm.autonotebook import tqdm\nfrom statsforecast import StatsForecast\n\nfrom statsforecast.utils import AirPassengersDF, ConformalIntervals\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, \n                cycles * np.pi,\n                weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\nsine_wave = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': sine_wave})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   y          364 non-null    float64       \n 1   unique_id  364 non-null    int64         \n 2   ds         364 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 8.7 KB\n\n\n\ndf_plot = df.copy()\ndf_plot.rename(columns={'y': 'final'}, inplace=True)\ndf_plot['unique_id'] = 'final'\ndf_plot['first'] = np.sin(t)\ndf_plot['second'] = 0.3*np.sin(5*t)\ndf_plot['noise'] = np.random.normal(scale=0.2, size=len(t))\ndf_plot.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   final      364 non-null    float64       \n 1   unique_id  364 non-null    object        \n 2   ds         364 non-null    datetime64[ns]\n 3   first      364 non-null    float64       \n 4   second     364 non-null    float64       \n 5   noise      364 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1)\nmemory usage: 17.2+ KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\ndf_plot = long_form(df_plot)\naltair_plot(df_plot)\n\n\n\n\n\n\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=28)\na.plot();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "neural_models.html#create-model",
    "href": "neural_models.html#create-model",
    "title": "Neural Learning Models",
    "section": "Create Model",
    "text": "Create Model\n\nhorizon = 28 * 2\n\ntrain_size = weeks * resolution - horizon\ntrain, test = df[:train_size], df[train_size:]\n\n\ndf.shape, train.shape, test.shape\n\n((364, 3), (308, 3), (56, 3))\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n FEDformer,\n #StemGNN, #need n_series\n #HINT, #need n_series\n TimesNet\n             ]\n\n\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_steps=50),\n          LSTM(input_size=2 * horizon, h=horizon, max_steps=50),\n          RNN(input_size=2 * horizon, h=horizon, max_steps=50),\n          #DeepAR(input_size=2 * horizon, h=horizon, max_steps=50),\n          ]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[NBEATS, NHITS, LSTM, RNN]\n\n\n\nmodels = [model(input_size=2 * horizon, h=horizon, max_steps=50) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN,\n GRU,\n LSTM,\n TCN,\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n FEDformer,\n TimesNet]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY_hat_df = nf.predict().reset_index()\nY_hat_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\nFEDformer\nTimesNet\n\n\n\n\n0\n1\n2022-02-03\n0.331584\n0.631020\n0.475472\n-0.038207\n-0.242601\n0.502359\n0.265126\n0.453357\n0.453357\n-0.182627\n0.269210\n\n\n1\n1\n2022-02-04\n0.583147\n0.764406\n0.713646\n0.352403\n0.103927\n0.741343\n0.922827\n0.823710\n0.823710\n-0.162300\n0.336149\n\n\n2\n1\n2022-02-05\n0.765167\n0.928452\n0.901598\n0.561872\n0.427954\n1.571360\n1.317898\n1.327817\n1.327817\n-0.034034\n0.661099\n\n\n3\n1\n2022-02-06\n1.032644\n1.010103\n1.000895\n0.721533\n0.633509\n1.472482\n1.339687\n1.358186\n1.358186\n0.034521\n0.637762\n\n\n4\n1\n2022-02-07\n1.117871\n0.940542\n1.053691\n0.720811\n0.799598\n0.619373\n0.754560\n0.555699\n0.555699\n0.285614\n0.453590\n\n\n\n\n\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\ndf_test = df_plot.query(\"unique_id == 'final'\")[int(-horizon*1.5):]\ndf_test.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 84 entries, 280 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         84 non-null     datetime64[ns]\n 1   unique_id  84 non-null     object        \n 2   price      84 non-null     object        \ndtypes: datetime64[ns](1), object(2)\nmemory usage: 2.6+ KB\n\n\n\nY_hat_df_plot = long_form(Y_hat_df)\nY_hat_df_plot = pd.concat([Y_hat_df_plot, df_test])\nY_hat_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2022-02-03\nunique_id\n1.0\n\n\n1\n2022-02-04\nunique_id\n1.0\n\n\n2\n2022-02-05\nunique_id\n1.0\n\n\n3\n2022-02-06\nunique_id\n1.0\n\n\n4\n2022-02-07\nunique_id\n1.0\n\n\n...\n...\n...\n...\n\n\n359\n2022-03-26\nfinal\n-1.075326\n\n\n360\n2022-03-27\nfinal\n-1.071706\n\n\n361\n2022-03-28\nfinal\n-1.120472\n\n\n362\n2022-03-29\nfinal\n-0.144448\n\n\n363\n2022-03-30\nfinal\n-0.032383\n\n\n\n\n756 rows × 3 columns\n\n\n\n\naltair_plot(Y_hat_df_plot)"
  },
  {
    "objectID": "neural_models.html#cross_validate",
    "href": "neural_models.html#cross_validate",
    "title": "Neural Learning Models",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\nFEDformer\nTimesNet\ny\n\n\n\n\n0\n1\n2021-12-07\n2021-12-06\n-0.778345\n-0.969260\n-0.483473\n-0.407675\n-0.524945\n-0.242061\n-0.833775\n-0.968383\n-0.968383\n-0.462379\n-0.700719\n-0.267128\n\n\n1\n1\n2021-12-08\n2021-12-06\n-0.359986\n-0.682918\n-0.116346\n0.194638\n-0.258169\n0.580216\n-0.103585\n-0.040710\n-0.040710\n-0.272619\n-0.120367\n0.398125\n\n\n2\n1\n2021-12-09\n2021-12-06\n0.080185\n-0.489487\n0.189207\n0.893661\n0.122502\n0.703065\n0.251461\n0.202799\n0.202799\n-0.386625\n0.650040\n0.713358\n\n\n3\n1\n2021-12-10\n2021-12-06\n0.420153\n-0.168859\n0.588188\n1.055157\n0.567973\n0.423164\n0.301031\n-0.088764\n-0.088764\n-0.223006\n0.892449\n0.444308\n\n\n4\n1\n2021-12-11\n2021-12-06\n0.609588\n0.150088\n0.860370\n1.401579\n0.843187\n1.076406\n0.479369\n0.147314\n0.147314\n0.004845\n1.196441\n1.166163\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n163\n1\n2022-01-29\n2021-12-08\n-0.816204\n-0.884289\n-0.871417\n-0.255213\n-0.707452\n-0.492719\n-0.523672\n-0.679037\n-0.679037\n-0.484612\n-1.257144\n-0.454238\n\n\n164\n1\n2022-01-30\n2021-12-08\n-0.537164\n-0.846854\n-0.757211\n-0.179368\n-0.697815\n0.400379\n-0.333994\n-0.465267\n-0.465267\n-0.101902\n-0.588163\n0.476971\n\n\n165\n1\n2022-01-31\n2021-12-08\n-0.216456\n-0.549389\n-0.587268\n0.079020\n-0.561862\n-0.253625\n-0.154823\n-0.330897\n-0.330897\n-0.321723\n-0.174761\n-0.097651\n\n\n166\n1\n2022-02-01\n2021-12-08\n-0.128435\n-0.410667\n-0.445573\n0.186666\n-0.337242\n-0.443354\n-0.162674\n-0.416916\n-0.416916\n-0.184451\n-0.204958\n-0.622777\n\n\n167\n1\n2022-02-02\n2021-12-08\n0.210510\n-0.274854\n-0.250561\n0.431372\n-0.012956\n-0.316491\n-0.359991\n-0.399479\n-0.399479\n0.575822\n0.139392\n-0.501131\n\n\n\n\n168 rows × 15 columns\n\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_df = long_form(cv_df)\n\n\ncv_df\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2021-12-07\nRNN\n-0.778345\n\n\n1\n2021-12-08\nRNN\n-0.359986\n\n\n2\n2021-12-09\nRNN\n0.080185\n\n\n3\n2021-12-10\nRNN\n0.420153\n\n\n4\n2021-12-11\nRNN\n0.609588\n\n\n...\n...\n...\n...\n\n\n2011\n2022-01-29\ny\n-0.454238\n\n\n2012\n2022-01-30\ny\n0.476971\n\n\n2013\n2022-01-31\ny\n-0.097651\n\n\n2014\n2022-02-01\ny\n-0.622777\n\n\n2015\n2022-02-02\ny\n-0.501131\n\n\n\n\n2016 rows × 3 columns\n\n\n\n\naltair_plot(cv_df)"
  },
  {
    "objectID": "neural_models.html#performance",
    "href": "neural_models.html#performance",
    "title": "Neural Learning Models",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nY_hat_df.info(), Y_hat_df.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 56 entries, 0 to 55\nData columns (total 13 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   unique_id   56 non-null     int64         \n 1   ds          56 non-null     datetime64[ns]\n 2   RNN         56 non-null     float64       \n 3   GRU         56 non-null     float64       \n 4   LSTM        56 non-null     float64       \n 5   TCN         56 non-null     float64       \n 6   DilatedRNN  56 non-null     float64       \n 7   MLP         56 non-null     float64       \n 8   NHITS       56 non-null     float64       \n 9   NBEATS      56 non-null     float64       \n 10  NBEATSx     56 non-null     float64       \n 11  FEDformer   56 non-null     float64       \n 12  TimesNet    56 non-null     float64       \ndtypes: datetime64[ns](1), float64(11), int64(1)\nmemory usage: 5.8 KB\n\n\n(None,\n Index(['unique_id', 'ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP',\n        'NHITS', 'NBEATS', 'NBEATSx', 'FEDformer', 'TimesNet'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\ntrain.columns, test.columns, Y_hat_df.columns\n\n(Index(['y', 'unique_id', 'ds'], dtype='object'),\n Index(['y', 'unique_id', 'ds'], dtype='object'),\n Index(['unique_id', 'ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP',\n        'NHITS', 'NBEATS', 'NBEATSx', 'FEDformer', 'TimesNet'],\n       dtype='object'))\n\n\n\nevaluate_performace(train, test, Y_hat_df).style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['unique_id', 'ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP',\n       'NHITS', 'NBEATS', 'NBEATSx', 'FEDformer', 'TimesNet'],\n      dtype='object')\n\n\n\n\n\n\n\n \nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\nFEDformer\nTimesNet\n\n\n\n\nmase\n0.435034\n0.415272\n0.433738\n0.399972\n0.522614\n0.281610\n0.282663\n0.263021\n0.263021\n0.499493\n0.369554\n\n\nmae\n0.623615\n0.595287\n0.621757\n0.573353\n0.749160\n0.403684\n0.405194\n0.377037\n0.377037\n0.716016\n0.529751\n\n\nmape\n113.600705\n107.218164\n125.201220\n122.939644\n122.842224\n94.040212\n96.869127\n92.912373\n92.912373\n138.528635\n124.077713\n\n\nrmse\n0.748148\n0.719231\n0.734778\n0.697955\n0.884842\n0.509274\n0.512601\n0.496020\n0.496020\n0.869066\n0.658590\n\n\nsmape\n95.296856\n88.066236\n94.181277\n92.506326\n105.355498\n63.243037\n61.651949\n62.291407\n62.291407\n110.947770\n73.907614\n\n\n\n\n\n\nresults = evaluate_performace(train, test, Y_hat_df)\nresults.to_csv('Data/Neutral_results2.csv')\n\nIndex(['unique_id', 'ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP',\n       'NHITS', 'NBEATS', 'NBEATSx', 'FEDformer', 'TimesNet'],\n      dtype='object')"
  },
  {
    "objectID": "ml_models.html",
    "href": "ml_models.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "To use MLForecast for time series forecasting, we instantiate a new MLForecast object and provide it with various parameters to tailor the modeling process to our specific needs:\nAll these settings are passed to the MLForecast constructor. Once the MLForecast object is initialized with these settings, we call its fit method and pass the historical data frame as the argument. The fit method trains the models on the provided historical data, readying them for future forecasting tasks."
  },
  {
    "objectID": "ml_models.html#data-extraction",
    "href": "ml_models.html#data-extraction",
    "title": "Machine Learning Models",
    "section": "Data Extraction",
    "text": "Data Extraction\n\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\nimport matplotlib.pyplot as plt\n\n# Generate a sine wave for demonstration\n\n\nfrom tqdm.autonotebook import tqdm\nfrom statsforecast import StatsForecast\n\nfrom statsforecast.utils import AirPassengersDF, ConformalIntervals\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, cycles * np.pi, weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\nsine_wave = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': sine_wave})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf\n\n\n\n\n\n\n\n\ny\nunique_id\nds\n\n\n\n\n0\n0.882026\n1\n2021-04-01\n\n\n1\n1.184884\n1\n2021-04-02\n\n\n2\n1.680301\n1\n2021-04-03\n\n\n3\n2.234016\n1\n2021-04-04\n\n\n4\n1.950096\n1\n2021-04-05\n\n\n...\n...\n...\n...\n\n\n359\n-1.075326\n1\n2022-03-26\n\n\n360\n-1.071706\n1\n2022-03-27\n\n\n361\n-1.120472\n1\n2022-03-28\n\n\n362\n-0.144448\n1\n2022-03-29\n\n\n363\n-0.032383\n1\n2022-03-30\n\n\n\n\n364 rows × 3 columns\n\n\n\n\ndf_plot = df.copy()\ndf_plot.rename(columns={'y': 'final'}, inplace=True)\ndf_plot['unique_id'] = 'final'\ndf_plot['first'] = np.sin(t)\ndf_plot['second'] = 0.3*np.sin(5*t)\ndf_plot['noise'] = waveform\ndf_plot.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   final      364 non-null    float64       \n 1   unique_id  364 non-null    object        \n 2   ds         364 non-null    datetime64[ns]\n 3   first      364 non-null    float64       \n 4   second     364 non-null    float64       \n 5   noise      364 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1)\nmemory usage: 17.2+ KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\ndf_plot = long_form(df_plot)\naltair_plot(df_plot)\n\n\n\n\n\n\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=28)\na.plot();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")\n\n\n                                                \n\n\n\nhorizon = 28 * 2\n\ntrain_size = weeks * resolution - horizon\ntrain, test = df[:train_size], df[train_size:]\n\n\ndf.shape, train.shape, test.shape\n\n((364, 3), (308, 3), (56, 3))"
  },
  {
    "objectID": "ml_models.html#create-model",
    "href": "ml_models.html#create-model",
    "title": "Machine Learning Models",
    "section": "Create Model",
    "text": "Create Model\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import PredictionIntervals\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\n\nfrom utilsforecast.plotting import plot_series\n\n\n# Import the necessary models from various libraries\n\n# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\nfrom lightgbm import LGBMRegressor\n\n# XGBRegressor: A gradient boosting regressor model from the XGBoost library\nfrom xgboost import XGBRegressor\n\n# LinearRegression: A simple linear regression model from the scikit-learn library\nfrom sklearn.linear_model import LinearRegression\n\n\n# Instantiate the MLForecast object\nparams = {\n    \"verbosity\": -5\n}\n\nmlf = MLForecast(\n    models=[LGBMRegressor(**params), XGBRegressor(), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n    freq='D',  # Frequency of the data - 'D' for daily frequency\n    lags=list(range(1, 28)),  # Specific lags to use as regressors: 1 to 6 days\n    lag_transforms={\n        1: [expanding_mean],\n        3: [expanding_mean],\n        5: [expanding_mean],\n        14: [(rolling_mean, 28)],\n        7: [(rolling_mean, 14)],\n        3: [(rolling_mean, 7)],\n        \n    },\n\n    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week'],  # Date features to use as regressors\n    num_threads=10,\n)\n\n\nprep = mlf.preprocess(train, dropna=True)\nprep.loc[1:'y']\n\n\n\n\n\n\n\n\ny\nunique_id\nds\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\nlag7\n...\nrolling_mean_lag3_window_size7\nexpanding_mean_lag5\nrolling_mean_lag14_window_size28\nrolling_mean_lag7_window_size14\nyear\nmonth\nday\ndayofweek\nquarter\nweek\n\n\n\n\n41\n-1.163704\n1\n2021-05-12\n-0.912752\n-0.188616\n0.234450\n1.291528\n1.216687\n0.407324\n0.006757\n...\n0.354921\n0.380462\n0.431463\n-0.526721\n2021\n5\n12\n2\n2\n19\n\n\n42\n-0.227256\n1\n2021-05-13\n-1.163704\n-0.912752\n-0.188616\n0.234450\n1.291528\n1.216687\n0.407324\n...\n0.323695\n0.404438\n0.399534\n-0.408213\n2021\n5\n13\n3\n2\n19\n\n\n43\n1.526975\n1\n2021-05-14\n-0.227256\n-1.163704\n-0.912752\n-0.188616\n0.234450\n1.291528\n1.216687\n...\n0.293626\n0.400079\n0.367424\n-0.226265\n2021\n5\n14\n4\n2\n19\n\n\n44\n-0.071539\n1\n2021-05-15\n1.526975\n-0.227256\n-1.163704\n-0.912752\n-0.188616\n0.234450\n1.291528\n...\n0.126417\n0.385362\n0.312378\n0.006824\n2021\n5\n15\n5\n2\n19\n\n\n45\n-0.525568\n1\n2021-05-16\n-0.071539\n1.526975\n-0.227256\n-1.163704\n-0.912752\n-0.188616\n0.234450\n...\n0.035763\n0.353700\n0.254907\n0.027002\n2021\n5\n16\n6\n2\n19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303\n-0.454238\n1\n2022-01-29\n-0.585046\n0.049184\n-1.333844\n-0.160548\n-0.284042\n0.207878\n-0.171845\n...\n-0.182921\n0.041598\n-0.135169\n0.501497\n2022\n1\n29\n5\n1\n4\n\n\n304\n0.476971\n1\n2022-01-30\n-0.454238\n-0.585046\n0.049184\n-1.333844\n-0.160548\n-0.284042\n0.207878\n...\n-0.188547\n0.040925\n-0.187325\n0.464957\n2022\n1\n30\n6\n1\n4\n\n\n305\n-0.097651\n1\n2022-01-31\n0.476971\n-0.454238\n-0.585046\n0.049184\n-1.333844\n-0.160548\n-0.284042\n...\n-0.325466\n0.036357\n-0.233925\n0.419495\n2022\n1\n31\n0\n1\n5\n\n\n306\n-0.622777\n1\n2022-02-01\n-0.097651\n0.476971\n-0.454238\n-0.585046\n0.049184\n-1.333844\n-0.160548\n...\n-0.365808\n0.036400\n-0.146023\n0.343701\n2022\n2\n1\n1\n1\n5\n\n\n307\n-0.501131\n1\n2022-02-02\n-0.622777\n-0.097651\n0.476971\n-0.454238\n-0.585046\n0.049184\n-1.333844\n...\n-0.327366\n0.034349\n-0.153749\n0.228724\n2022\n2\n2\n2\n1\n5\n\n\n\n\n267 rows × 41 columns\n\n\n\n\nfrom time import time\n# Start the timer to calculate the time taken for fitting the models\ninit = time()\n\n# Fit the MLForecast models to the data, with prediction intervals set using a window size of 28 days\nmlf.fit(train, prediction_intervals=PredictionIntervals(h=horizon, n_windows=2))\n\n# Calculate the end time after fitting the models\nend = time()\n\n# Print the time taken to fit the MLForecast models, in minutes\nprint(f'MLForecast Minutes: {(end - init) / 60}')\n\nMLForecast Minutes: 0.06610998709996542\n\n\n\nfcst_mlf_df = mlf.predict(horizon,\n                          #level=[90]\n                         )\n\n\nfcst_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\n1\n2022-02-03\n0.165059\n0.862316\n-0.109627\n\n\n1\n1\n2022-02-04\n0.375490\n1.042413\n-0.017544\n\n\n2\n1\n2022-02-05\n0.352139\n1.294334\n0.300877\n\n\n3\n1\n2022-02-06\n1.063436\n1.249144\n0.506214\n\n\n4\n1\n2022-02-07\n0.508790\n1.422384\n0.301202\n\n\n\n\n\n\n\n\nfcst_mlf_df_plot = long_form(fcst_mlf_df)\nfcst_mlf_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2022-02-03\nunique_id\n1.000000\n\n\n1\n2022-02-04\nunique_id\n1.000000\n\n\n2\n2022-02-05\nunique_id\n1.000000\n\n\n3\n2022-02-06\nunique_id\n1.000000\n\n\n4\n2022-02-07\nunique_id\n1.000000\n\n\n...\n...\n...\n...\n\n\n219\n2022-03-26\nLinearRegression\n-0.143730\n\n\n220\n2022-03-27\nLinearRegression\n0.035111\n\n\n221\n2022-03-28\nLinearRegression\n-0.211611\n\n\n222\n2022-03-29\nLinearRegression\n-0.210740\n\n\n223\n2022-03-30\nLinearRegression\n-0.223617\n\n\n\n\n224 rows × 3 columns\n\n\n\n\ndf_test = df_plot.query(\"unique_id == 'final'\")[int(-horizon*1.5):]\ndf_test\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n280\n2022-01-06\nfinal\n-0.085467\n\n\n281\n2022-01-07\nfinal\n0.576483\n\n\n282\n2022-01-08\nfinal\n-0.385228\n\n\n283\n2022-01-09\nfinal\n0.719438\n\n\n284\n2022-01-10\nfinal\n0.352424\n\n\n...\n...\n...\n...\n\n\n359\n2022-03-26\nfinal\n-1.075326\n\n\n360\n2022-03-27\nfinal\n-1.071706\n\n\n361\n2022-03-28\nfinal\n-1.120472\n\n\n362\n2022-03-29\nfinal\n-0.144448\n\n\n363\n2022-03-30\nfinal\n-0.032383\n\n\n\n\n84 rows × 3 columns\n\n\n\n\nfcst_mlf_df_plot = pd.concat([fcst_mlf_df_plot,df_test])\n\n\nfcst_mlf_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2022-02-03\nunique_id\n1.0\n\n\n1\n2022-02-04\nunique_id\n1.0\n\n\n2\n2022-02-05\nunique_id\n1.0\n\n\n3\n2022-02-06\nunique_id\n1.0\n\n\n4\n2022-02-07\nunique_id\n1.0\n\n\n...\n...\n...\n...\n\n\n359\n2022-03-26\nfinal\n-1.075326\n\n\n360\n2022-03-27\nfinal\n-1.071706\n\n\n361\n2022-03-28\nfinal\n-1.120472\n\n\n362\n2022-03-29\nfinal\n-0.144448\n\n\n363\n2022-03-30\nfinal\n-0.032383\n\n\n\n\n308 rows × 3 columns\n\n\n\n\naltair_plot(fcst_mlf_df_plot)"
  },
  {
    "objectID": "ml_models.html#cross-validate",
    "href": "ml_models.html#cross-validate",
    "title": "Machine Learning Models",
    "section": "Cross Validate",
    "text": "Cross Validate\n\ninit = time()\ncv_mlf_df = mlf.cross_validation(\n    df=train, \n    h=horizon, \n    n_windows=4, \n    step_size=horizon, \n    level=[90],\n)\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\nCV Minutes: 0.09884265661239625\n\n\n\ncv_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\n1\n2021-06-24\n2021-06-23\n1.557940\n-0.703467\n0.046745\n13.555471\n\n\n1\n1\n2021-06-25\n2021-06-23\n1.977053\n-0.616080\n0.147216\n-14.964685\n\n\n2\n1\n2021-06-26\n2021-06-23\n1.558700\n-0.647443\n0.149506\n10.710260\n\n\n3\n1\n2021-06-27\n2021-06-23\n0.807387\n-0.139943\n0.204176\n-0.959143\n\n\n4\n1\n2021-06-28\n2021-06-23\n0.529640\n0.160516\n0.203331\n-5.428722\n\n\n\n\n\n\n\n\ncv_mlf_df.drop('cutoff', axis=1, inplace=True)\ncv_mlf_df.drop('LinearRegression', axis=1, inplace=True)\ncv_mlf_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_mlf_df = long_form(cv_mlf_df)\n\n\naltair_plot(cv_mlf_df)"
  },
  {
    "objectID": "ml_models.html#performance",
    "href": "ml_models.html#performance",
    "title": "Machine Learning Models",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nfcst_mlf_df.info(), fcst_mlf_df.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 56 entries, 0 to 55\nData columns (total 5 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   unique_id         56 non-null     int64         \n 1   ds                56 non-null     datetime64[ns]\n 2   LGBMRegressor     56 non-null     float64       \n 3   XGBRegressor      56 non-null     float32       \n 4   LinearRegression  56 non-null     float64       \ndtypes: datetime64[ns](1), float32(1), float64(2), int64(1)\nmemory usage: 2.1 KB\n\n\n(None,\n Index(['unique_id', 'ds', 'LGBMRegressor', 'XGBRegressor', 'LinearRegression'], dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\ntrain.columns, test.columns\n\n(Index(['y', 'unique_id', 'ds'], dtype='object'),\n Index(['y', 'unique_id', 'ds'], dtype='object'))\n\n\n\nevaluate_performace(train, test, fcst_mlf_df).style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['unique_id', 'ds', 'LGBMRegressor', 'XGBRegressor', 'LinearRegression'], dtype='object')\n\n\n\n\n\n\n\n \nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\nmase\n0.405929\n0.574238\n0.607204\n\n\nmae\n0.581893\n0.823161\n0.870418\n\n\nmape\n127.363073\n204.620876\n122.106361\n\n\nrmse\n0.724010\n0.963238\n1.026189\n\n\nsmape\n92.922506\n115.879716\n145.479426\n\n\n\n\n\n\nresults = evaluate_performace(train, test, fcst_mlf_df)\nresults.to_csv('Data/ML_results2.1.csv')\n\nIndex(['unique_id', 'ds', 'LGBMRegressor', 'XGBRegressor', 'LinearRegression'], dtype='object')"
  },
  {
    "objectID": "individual_model_aircon.html",
    "href": "individual_model_aircon.html",
    "title": "Aircon Model",
    "section": "",
    "text": "import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nimport pandas as pd\n\nimport warnings\nfrom influxdb_client.client.warnings import MissingPivotFunction\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nwarnings.simplefilter(\"ignore\", MissingPivotFunction)\n\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "individual_model_aircon.html#get-data",
    "href": "individual_model_aircon.html#get-data",
    "title": "Aircon Model",
    "section": "",
    "text": "import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nimport pandas as pd\n\nimport warnings\nfrom influxdb_client.client.warnings import MissingPivotFunction\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nwarnings.simplefilter(\"ignore\", MissingPivotFunction)\n\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "individual_model_aircon.html#energy-in-wide-form",
    "href": "individual_model_aircon.html#energy-in-wide-form",
    "title": "Aircon Model",
    "section": "Energy in Wide Form",
    "text": "Energy in Wide Form\n\nwide_df = result.pivot(index='_time', columns='sensor', values='_value')\n# Reset the index to make 'id' a regular column\nwide_df.reset_index(inplace=True)\n\nwide_df.columns\n\nIndex(['_time', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production', 'Unmonitored'],\n      dtype='object', name='sensor')\n\n\n\nwide_df['_time'] = pd.to_datetime(wide_df['_time']).dt.tz_localize(None)\nwide_df.drop(['Unmonitored', 'Consumption', 'ConsumptionNet'], axis = 1, inplace=True)\nwide_df.rename(columns={'_time':'ds'}, inplace=True)\n# Delete the last row\nwide_df = wide_df.drop(wide_df.index[-1])\n\n\nwide_df_new = pd.DataFrame()\nfor column in wide_df.columns:\n    wide_df_new[f'{column}'] = wide_df[f'{column}'].interpolate()\n\nwide_df = pd.DataFrame(wide_df_new)\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import',\n       'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production'],\n      dtype='object')\n\n\n\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n8.578005\n180.242247\n25.492343\n24.017087\n0.530865\n999.775552\n333.258545\n366.408449\n71.876417\n80.009421\n-108.365830\n66.086281\n176.392548\n491.748285\n\n\n241\n2023-10-29\n8.661682\n806.610093\n19.634587\n2.731102\n0.192283\n530.312474\n176.770805\n201.397413\n1.974071\n60.108551\n-804.636021\n1.253300\n103.451046\n1017.375874\n\n\n242\n2023-10-30\n8.603354\n757.601472\n27.081000\n0.983716\n0.183639\n788.486015\n262.828480\n290.616882\n3.378339\n66.827993\n-754.223132\n1.356785\n178.170521\n1052.272015\n\n\n243\n2023-10-31\n9.032917\n803.841970\n35.649472\n11.163953\n0.269313\n901.165305\n300.388379\n319.795146\n5.027126\n67.972347\n-798.814844\n30.217549\n166.649785\n1142.661342\n\n\n244\n2023-11-01\n8.743938\n898.594991\n30.389375\n3.655479\n0.189410\n623.050504\n207.683462\n245.486118\n2.309883\n50.624125\n-896.285107\n0.213375\n145.941312\n1134.978184\n\n\n\n\n\n\n\n\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n    return df\n\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='y')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='y:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nwide_df = data_scaler(wide_df)\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n0.000327\n0.087269\n0.054518\n0.475787\n0.000988\n0.441317\n0.270647\n0.263152\n0.098826\n0.397064\n0.702175\n0.217335\n0.451213\n0.164710\n\n\n241\n2023-10-29\n0.000459\n0.390543\n0.008991\n0.049113\n0.000167\n0.155476\n0.095349\n0.096613\n0.001842\n0.287229\n0.452268\n0.003559\n0.195780\n0.385757\n\n\n242\n2023-10-30\n0.000367\n0.366814\n0.066865\n0.014087\n0.000146\n0.312670\n0.191751\n0.186659\n0.003790\n0.324314\n0.470363\n0.003900\n0.457440\n0.400433\n\n\n243\n2023-10-31\n0.001045\n0.389203\n0.133461\n0.218148\n0.000354\n0.381277\n0.233826\n0.216107\n0.006078\n0.330630\n0.454358\n0.099064\n0.417095\n0.438445\n\n\n244\n2023-11-01\n0.000589\n0.435080\n0.092579\n0.067642\n0.000160\n0.211942\n0.129978\n0.141110\n0.002308\n0.234883\n0.419373\n0.000130\n0.344577\n0.435214\n\n\n\n\n\n\n\n\ndf = long_form(wide_df)\nwide_df.rename(columns={'Aircon': 'y'}, inplace=True)\nwide_df['unique_id'] = 'Aircon'\ndf.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-03-02\nAircon\n0.007134\n\n\n1\n2023-03-03\nAircon\n0.178748\n\n\n2\n2023-03-04\nAircon\n0.338990\n\n\n3\n2023-03-05\nAircon\n0.173844\n\n\n4\n2023-03-06\nAircon\n0.363831"
  },
  {
    "objectID": "individual_model_aircon.html#weather-data",
    "href": "individual_model_aircon.html#weather-data",
    "title": "Aircon Model",
    "section": "Weather data",
    "text": "Weather data\n\nimport pandas as pd\npath = 'Data'\nname = 'weather'\n\nweather_df = pd.read_csv(f'{path}/{name}.csv')\nweather_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01 00:00\n21.6\n0\n12.6\n126\n1013.0\n0.0\n84\n73\n21.6\n21.6\n24.4\n18.7\n0\n10.0\n19.8\n1.0\n\n\n1\n2023-01-01 01:00\n21.3\n0\n11.9\n133\n1013.0\n0.0\n85\n77\n21.3\n21.3\n24.3\n18.7\n0\n10.0\n18.7\n1.0\n\n\n2\n2023-01-01 02:00\n21.1\n0\n11.2\n140\n1012.0\n0.0\n86\n80\n21.1\n21.1\n24.2\n18.7\n0\n10.0\n17.6\n1.0\n\n\n3\n2023-01-01 03:00\n20.8\n0\n10.4\n147\n1012.0\n0.0\n88\n84\n20.8\n20.8\n20.8\n18.7\n0\n10.0\n16.6\n1.0\n\n\n4\n2023-01-01 04:00\n21.0\n0\n10.6\n144\n1012.0\n0.0\n86\n84\n21.0\n21.0\n21.0\n18.6\n0\n10.0\n16.3\n1.0\n\n\n\n\n\n\n\n\nweather_df = data_scaler(weather_df)\n\n\ndef data_day_avg(weather_df):\n    from tqdm.notebook import tqdm\n    avg_df = pd.DataFrame()\n    dates = pd.to_datetime(weather_df['ds']).dt.date.unique()\n    \n    for date in tqdm(dates):\n        filtered_df = weather_df[pd.to_datetime(weather_df['ds']).dt.date == date]\n        ds = filtered_df.pop('ds')\n\n        filtered_df = pd.DataFrame(filtered_df.mean()).T\n        filtered_df\n\n        filtered_df.insert(0, 'ds', date)\n\n        avg_df = pd.concat([avg_df, filtered_df], ignore_index=True)\n    \n    return avg_df\n\n\nweather_avg_df = data_day_avg(weather_df)\n\n\n\n\n\nweather_avg_df['ds'] = pd.to_datetime(weather_avg_df['ds'])\n\n\nweather_avg_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01\n0.566503\n0.583333\n0.459098\n0.345636\n0.428728\n0.000895\n0.670886\n0.583750\n0.516791\n0.595085\n0.512881\n0.778121\n0.0\n1.000000\n0.434678\n0.422619\n\n\n1\n2023-01-02\n0.554902\n0.583333\n0.451453\n0.396936\n0.359649\n0.001253\n0.602848\n0.409167\n0.499005\n0.584249\n0.486067\n0.725707\n0.0\n1.000000\n0.434844\n0.428571\n\n\n2\n2023-01-03\n0.582516\n0.583333\n0.263889\n0.415506\n0.258772\n0.000000\n0.584916\n0.287917\n0.525746\n0.610043\n0.502366\n0.738663\n0.0\n1.000000\n0.283533\n0.452381\n\n\n3\n2023-01-04\n0.650490\n0.583333\n0.313073\n0.382660\n0.163377\n0.000895\n0.687764\n0.196250\n0.608582\n0.673535\n0.609621\n0.866902\n0.0\n0.994792\n0.305777\n0.500000\n\n\n4\n2023-01-05\n0.654575\n0.583333\n0.257518\n0.506267\n0.166667\n0.026859\n0.735232\n0.700417\n0.629726\n0.677350\n0.608701\n0.911808\n0.0\n0.989583\n0.288098\n0.416667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\n2023-12-06\n0.580556\n0.583333\n0.254077\n0.264740\n0.558114\n0.000000\n0.608650\n0.174583\n0.519527\n0.608364\n0.515116\n0.745878\n0.0\n1.000000\n0.214558\n0.470238\n\n\n340\n2023-12-07\n0.605065\n0.583333\n0.266565\n0.202066\n0.565789\n0.000000\n0.582806\n0.050000\n0.543532\n0.631258\n0.533386\n0.748233\n0.0\n1.000000\n0.222693\n0.476190\n\n\n341\n2023-12-08\n0.618954\n0.583333\n0.349261\n0.251625\n0.575658\n0.000018\n0.626582\n0.342083\n0.561816\n0.644078\n0.555205\n0.795200\n0.0\n1.000000\n0.285940\n0.476190\n\n\n342\n2023-12-09\n0.612582\n0.583333\n0.362768\n0.190460\n0.544956\n0.000107\n0.577532\n0.497500\n0.564179\n0.638584\n0.539432\n0.757509\n0.0\n1.000000\n0.308267\n0.428571\n\n\n343\n2023-12-10\n0.630556\n0.583333\n0.513507\n0.233751\n0.530702\n0.000233\n0.515295\n0.537500\n0.571269\n0.655067\n0.546924\n0.730271\n0.0\n1.000000\n0.426627\n0.452381\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n(\n    wide_df[['y']].plot(title='Aircon')\n)\n\n&lt;Axes: title={'center': 'Aircon'}&gt;\n\n\n\n\n\n\nfrom statsforecast import StatsForecast\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nwide_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 245 entries, 0 to 244\nData columns (total 16 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   ds                  245 non-null    datetime64[ns]\n 1   y                   245 non-null    float64       \n 2   Export              245 non-null    float64       \n 3   Fridge              245 non-null    float64       \n 4   Garage              245 non-null    float64       \n 5   Hotwater            245 non-null    float64       \n 6   HouseBoardOutside   161 non-null    float64       \n 7   HouseBoardOutsideR  245 non-null    float64       \n 8   HouseBoardR         245 non-null    float64       \n 9   Import              245 non-null    float64       \n 10  Lights              245 non-null    float64       \n 11  Net                 245 non-null    float64       \n 12  OvenStove           245 non-null    float64       \n 13  Powerpoints         245 non-null    float64       \n 14  Production          245 non-null    float64       \n 15  unique_id           245 non-null    object        \ndtypes: datetime64[ns](1), float64(14), object(1)\nmemory usage: 30.8+ KB\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3430 entries, 0 to 3429\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         3430 non-null   datetime64[ns]\n 1   unique_id  3430 non-null   object        \n 2   y          3346 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 80.5+ KB\n\n\n\nweather_avg_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   ds              344 non-null    datetime64[ns]\n 1   temp_c          344 non-null    float64       \n 2   is_day          344 non-null    float64       \n 3   wind_kph        344 non-null    float64       \n 4   wind_degree     344 non-null    float64       \n 5   pressure_mb     344 non-null    float64       \n 6   precip_mm       344 non-null    float64       \n 7   humidity        344 non-null    float64       \n 8   cloud           344 non-null    float64       \n 9   feelslike_c     344 non-null    float64       \n 10  windchill_c     344 non-null    float64       \n 11  heatindex_c     344 non-null    float64       \n 12  dewpoint_c      344 non-null    float64       \n 13  chance_of_rain  344 non-null    float64       \n 14  vis_km          344 non-null    float64       \n 15  gust_kph        344 non-null    float64       \n 16  uv              344 non-null    float64       \ndtypes: datetime64[ns](1), float64(16)\nmemory usage: 45.8 KB\n\n\n\ndate_range_start, date_range_end  = wide_df['ds'].min(), wide_df['ds'].max()\n# Filter rows between the start and end dates\nweather_avg_filtered_df = weather_avg_df[(weather_avg_df['ds'] &gt;= date_range_start) & (weather_avg_df['ds'] &lt;= date_range_end)]\nweather_avg_filtered_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.0\n0.588811\n0.345238\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.0\n0.328934\n0.345238\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.0\n0.340139\n0.416667\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.0\n0.470286\n0.464286\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.0\n0.467795\n0.404762\n\n\n\n\n245 rows × 17 columns\n\n\n\n\naltair_plot(df)\n\n\n\n\n\n\n\n\nweather_avg_df_plot = long_form(weather_avg_filtered_df)\naltair_plot(weather_avg_df_plot)\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nwide_df[\"y\"]\n\n0      0.007134\n1      0.178748\n2      0.338990\n3      0.173844\n4      0.363831\n         ...   \n240    0.000327\n241    0.000459\n242    0.000367\n243    0.001045\n244    0.000589\nName: y, Length: 245, dtype: float64\n\n\n\ndef augmented_dickey_fuller_test(series , column_name):\n    from statsmodels.tsa.stattools import adfuller\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\naugmented_dickey_fuller_test(wide_df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic                  -3.009405\np-value                          0.034025\nNo Lags Used                    12.000000\nNumber of observations used    232.000000\nCritical Value (1%)             -3.458855\nCritical Value (5%)             -2.874080\nCritical Value (10%)            -2.573453\ndtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=28, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=28, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=14, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=14, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(wide_df[\"y\"], model = \"additive\", period=28)\na.plot();"
  },
  {
    "objectID": "individual_model_aircon.html#designing-models",
    "href": "individual_model_aircon.html#designing-models",
    "title": "Aircon Model",
    "section": "Designing Models",
    "text": "Designing Models\n\nweather_avg_filtered_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n\n\n\n\nwide_df = pd.merge(wide_df, weather_avg_filtered_df, on='ds', how='outer')\n\n\nwide_df.head()\n\n\n\n\n\n\n\n\nds\ny\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\n...\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-03-02\n0.007134\n0.016595\n0.814609\n0.003794\n0.000287\nNaN\n0.356702\n0.364385\n0.085067\n...\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n1\n2023-03-03\n0.178748\n0.165011\n0.669709\n0.015031\n0.000143\nNaN\n0.292065\n0.295309\n0.296304\n...\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n2\n2023-03-04\n0.338990\n0.294620\n0.686509\n0.059559\n0.000460\nNaN\n0.496691\n0.501420\n0.551547\n...\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n3\n2023-03-05\n0.173844\n0.302607\n0.701288\n0.009916\n0.000256\nNaN\n0.419958\n0.422212\n0.467784\n...\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n4\n2023-03-06\n0.363831\n0.381220\n0.773714\n0.008798\n0.002112\nNaN\n0.562640\n0.561444\n0.600856\n...\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n5 rows × 32 columns\n\n\n\n\nhorizon = 10\n\ntrain_size = len(wide_df) - horizon\ntrain, test = wide_df[:train_size], wide_df[train_size:]\n\n\nwide_df.shape, train.shape, test.shape\n\n((245, 32), (235, 32), (10, 32))\n\n\n\nfutr_df=weather_avg_filtered_df[train_size:]\nfutr_df['unique_id'] = 'Aircon'\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nAircon\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nAircon\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nAircon\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nAircon\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nAircon\n\n\n\n\n\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n #FEDformer, #taken too long\n #StemGNN, #need n_series\n #HINT, #need n_series\n #TimesNet # takes too long\n             ]\n\n\nwide_df.columns\n\nIndex(['ds', 'y', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import',\n       'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production', 'unique_id',\n       'temp_c', 'is_day', 'wind_kph', 'wind_degree', 'pressure_mb',\n       'precip_mm', 'humidity', 'cloud', 'feelslike_c', 'windchill_c',\n       'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph',\n       'uv'],\n      dtype='object')\n\n\n\nmodels = [model(input_size=5 * horizon,\n                h=horizon,\n                max_steps=20,\n                hist_exog_list =['Export',\n                                ],\n                futr_exog_list = ['temp_c', 'is_day', 'wind_kph', 'wind_degree',\n                    'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n                    'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain',\n                    'vis_km', 'gust_kph', 'uv']\n               ) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN, GRU, LSTM, TCN, DilatedRNN, MLP, NHITS, NBEATS, NBEATSx]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\ntrain.columns\n\nIndex(['ds', 'y', 'Export', 'Fridge', 'Garage', 'Hotwater',\n       'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR', 'Import',\n       'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production', 'unique_id',\n       'temp_c', 'is_day', 'wind_kph', 'wind_degree', 'pressure_mb',\n       'precip_mm', 'humidity', 'cloud', 'feelslike_c', 'windchill_c',\n       'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph',\n       'uv'],\n      dtype='object')\n\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n295\n2023-10-23\n0.560784\n0.500000\n0.579256\n0.550836\n0.439693\n0.000000\n0.614979\n0.075000\n0.509328\n0.589896\n0.485279\n0.734246\n0.0\n1.000000\n0.512699\n0.392857\nAircon\n\n\n296\n2023-10-24\n0.550490\n0.541667\n0.571738\n0.437210\n0.500000\n0.000430\n0.684599\n0.150833\n0.503483\n0.580128\n0.486593\n0.771349\n0.0\n1.000000\n0.495933\n0.410714\nAircon\n\n\n297\n2023-10-25\n0.558660\n0.541667\n0.585117\n0.546657\n0.482456\n0.000000\n0.656646\n0.073750\n0.511070\n0.588217\n0.487119\n0.759570\n0.0\n1.000000\n0.513197\n0.428571\nAircon\n\n\n298\n2023-10-26\n0.495261\n0.541667\n0.518349\n0.551996\n0.493421\n0.005766\n0.684072\n0.593333\n0.449627\n0.528541\n0.422581\n0.725265\n0.0\n0.953125\n0.448705\n0.363095\nAircon\n\n\n299\n2023-10-27\n0.309314\n0.541667\n0.621814\n0.516481\n0.633772\n0.008792\n0.767932\n0.862500\n0.289552\n0.355311\n0.249211\n0.616166\n0.0\n0.994792\n0.573373\n0.232143\nAircon\n\n\n\n\n\n\n\n\nconsumption_predict = nf.predict(futr_df=futr_df).reset_index()\nconsumption_predict.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\nAircon\n2023-10-23\n-0.003765\n0.001875\n0.000757\n0.001369\n0.005308\n0.003426\n-0.000102\n-0.007408\n0.029458\n\n\n1\nAircon\n2023-10-24\n-0.002244\n0.003480\n-0.000198\n0.002409\n0.005081\n-0.001419\n-0.002336\n0.002019\n0.004433\n\n\n2\nAircon\n2023-10-25\n-0.000652\n0.001929\n-0.000283\n0.001561\n0.004017\n0.003254\n0.001504\n-0.007340\n-0.017543\n\n\n3\nAircon\n2023-10-26\n-0.000010\n0.002397\n0.001231\n0.002679\n0.005051\n0.006323\n0.003396\n0.005736\n-0.002234\n\n\n4\nAircon\n2023-10-27\n0.004546\n0.006976\n0.004145\n0.006659\n0.009386\n-0.004586\n0.003071\n0.003898\n-0.008494\n\n\n\n\n\n\n\n\nconsumption_predict.drop('unique_id', axis=1, inplace=True)\n\n\njust_production = df[df['unique_id'] == 'Aircon']\n\n\nconsumption_predict_plot = long_form(consumption_predict)\nconsumption_predict_plot = pd.concat([consumption_predict_plot, just_production.tail(50)]\n                                     , ignore_index=True)\nconsumption_predict_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-23\nRNN\n-0.003765\n\n\n1\n2023-10-24\nRNN\n-0.002244\n\n\n2\n2023-10-25\nRNN\n-0.000652\n\n\n3\n2023-10-26\nRNN\n-0.000010\n\n\n4\n2023-10-27\nRNN\n0.004546\n\n\n...\n...\n...\n...\n\n\n135\n2023-10-28\nAircon\n0.000327\n\n\n136\n2023-10-29\nAircon\n0.000459\n\n\n137\n2023-10-30\nAircon\n0.000367\n\n\n138\n2023-10-31\nAircon\n0.001045\n\n\n139\n2023-11-01\nAircon\n0.000589\n\n\n\n\n140 rows × 3 columns\n\n\n\n\naltair_plot(consumption_predict_plot)"
  },
  {
    "objectID": "individual_model_aircon.html#performance",
    "href": "individual_model_aircon.html#performance",
    "title": "Aircon Model",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nconsumption_predict.info(), consumption_predict.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   ds          10 non-null     datetime64[ns]\n 1   RNN         10 non-null     float64       \n 2   GRU         10 non-null     float64       \n 3   LSTM        10 non-null     float64       \n 4   TCN         10 non-null     float64       \n 5   DilatedRNN  10 non-null     float64       \n 6   MLP         10 non-null     float64       \n 7   NHITS       10 non-null     float64       \n 8   NBEATS      10 non-null     float64       \n 9   NBEATSx     10 non-null     float64       \ndtypes: datetime64[ns](1), float64(9)\nmemory usage: 932.0 bytes\n\n\n(None,\n Index(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n        'NBEATS', 'NBEATSx'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nscore = evaluate_performace(train, test, consumption_predict)\nscore.style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\n\n\n\n\n \nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\nmase\n0.371110\n0.373379\n0.376333\n0.386538\n0.426027\n0.392198\n0.419957\n0.429512\n\n\nmae\n0.048412\n0.048708\n0.049094\n0.050425\n0.055577\n0.051163\n0.054785\n0.056031\n\n\nmape\n259.057162\n160.017654\n375.530638\n807.648400\n1278.895162\n616.177544\n1099.112504\n2281.618162\n\n\nrmse\n0.086880\n0.088143\n0.087256\n0.085695\n0.088678\n0.088601\n0.090735\n0.086129\n\n\nsmape\n138.050393\n137.216017\n143.938382\n171.561234\n187.530805\n164.534808\n173.732277\n187.285272\n\n\n\n\n\n\nscore = data_scaler(score.T)\nscore.plot(kind='bar', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nresults = evaluate_performace(train, test, consumption_predict)\nresults.to_csv('Data/Neutral_results6.csv')\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')"
  },
  {
    "objectID": "individual_model_aircon.html#cross_validate",
    "href": "individual_model_aircon.html#cross_validate",
    "title": "Aircon Model",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df.columns\n\nIndex(['unique_id', 'ds', 'cutoff', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN',\n       'MLP', 'NHITS', 'NBEATS', 'NBEATSx', 'y', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'temp_c', 'is_day', 'wind_kph', 'wind_degree', 'pressure_mb',\n       'precip_mm', 'humidity', 'cloud', 'feelslike_c', 'windchill_c',\n       'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km', 'gust_kph',\n       'uv'],\n      dtype='object')\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_df['RNN']\n\n0     0.003673\n1     0.003411\n2     0.005272\n3     0.004834\n4     0.003371\n5     0.004693\n6     0.002750\n7     0.003084\n8     0.003110\n9     0.003422\n10    0.003683\n11    0.004200\n12    0.004993\n13    0.003823\n14    0.003562\n15    0.002538\n16    0.003389\n17    0.001889\n18    0.003593\n19    0.002887\n20    0.003859\n21    0.004063\n22    0.004764\n23    0.004584\n24    0.002643\n25    0.002991\n26    0.002939\n27    0.002597\n28    0.003465\n29    0.003382\nName: RNN, dtype: float32\n\n\n\nconsumption_predict.columns\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\ncv_df_filter = cv_df[consumption_predict.columns]\ncv_df_filter\n\n\n\n\n\n\n\n\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\n2023-10-11\n0.003673\n0.005021\n0.004987\n0.005792\n0.005489\n-0.009889\n-0.003473\n-0.001256\n-0.030136\n\n\n1\n2023-10-12\n0.003411\n0.005343\n0.005253\n0.005539\n0.006155\n-0.005851\n-0.002714\n0.004471\n-0.001906\n\n\n2\n2023-10-13\n0.005272\n0.004845\n0.005999\n0.007143\n0.004875\n-0.002195\n-0.004884\n-0.004693\n-0.008124\n\n\n3\n2023-10-14\n0.004834\n0.004222\n0.005454\n0.007220\n0.005831\n-0.001914\n-0.004632\n0.003145\n0.017054\n\n\n4\n2023-10-15\n0.003371\n0.004710\n0.005032\n0.006623\n0.005268\n-0.005891\n-0.005599\n0.005866\n-0.004327\n\n\n5\n2023-10-16\n0.004693\n0.004753\n0.004767\n0.006278\n0.005445\n-0.007138\n0.014494\n-0.010272\n-0.012851\n\n\n6\n2023-10-17\n0.002750\n0.002801\n0.001887\n0.003215\n0.004832\n-0.002510\n0.015675\n0.006893\n-0.014723\n\n\n7\n2023-10-18\n0.003084\n0.003364\n0.003110\n0.005126\n0.006354\n-0.000340\n-0.000863\n0.007038\n0.006363\n\n\n8\n2023-10-19\n0.003110\n0.002219\n0.003411\n0.004707\n0.005641\n0.004607\n-0.002232\n-0.005905\n-0.017330\n\n\n9\n2023-10-20\n0.003422\n0.002788\n0.003836\n0.005737\n0.005702\n-0.001130\n-0.005217\n-0.000549\n-0.017569\n\n\n10\n2023-10-12\n0.003683\n0.005175\n0.004413\n0.005611\n0.005654\n-0.007089\n-0.003497\n-0.001838\n-0.029171\n\n\n11\n2023-10-13\n0.004200\n0.006124\n0.006182\n0.007197\n0.004898\n-0.004902\n-0.003242\n0.003925\n-0.004157\n\n\n12\n2023-10-14\n0.004993\n0.005107\n0.005692\n0.007183\n0.005525\n-0.001707\n-0.005932\n-0.005245\n-0.006840\n\n\n13\n2023-10-15\n0.003823\n0.003455\n0.004988\n0.006447\n0.005087\n-0.000212\n-0.004289\n0.002667\n0.018079\n\n\n14\n2023-10-16\n0.003562\n0.004341\n0.005024\n0.006695\n0.004611\n-0.007060\n-0.006028\n0.005395\n-0.003859\n\n\n15\n2023-10-17\n0.002538\n0.002963\n0.001651\n0.003516\n0.004259\n-0.005497\n0.013891\n-0.010744\n-0.013867\n\n\n16\n2023-10-18\n0.003389\n0.003141\n0.002055\n0.004616\n0.005372\n0.000017\n0.016669\n0.006431\n-0.017220\n\n\n17\n2023-10-19\n0.001889\n0.002799\n0.002895\n0.004982\n0.006241\n0.001313\n0.000602\n0.006591\n0.009745\n\n\n18\n2023-10-20\n0.003593\n0.002599\n0.004550\n0.005695\n0.005590\n0.005149\n-0.001900\n-0.006328\n-0.020834\n\n\n19\n2023-10-21\n0.002887\n0.003031\n0.003053\n0.004711\n0.005212\n0.000061\n-0.004911\n-0.000964\n-0.015837\n\n\n20\n2023-10-13\n0.003859\n0.005407\n0.005920\n0.006619\n0.006034\n-0.007852\n-0.003390\n-0.002004\n-0.033442\n\n\n21\n2023-10-14\n0.004063\n0.006056\n0.005797\n0.007344\n0.005953\n-0.004207\n-0.003498\n0.003762\n-0.002752\n\n\n22\n2023-10-15\n0.004764\n0.004569\n0.005187\n0.006584\n0.004831\n-0.000522\n-0.005164\n-0.005370\n-0.004247\n\n\n23\n2023-10-16\n0.004584\n0.003823\n0.004438\n0.006753\n0.004477\n-0.002999\n-0.003351\n0.002537\n0.017125\n\n\n24\n2023-10-17\n0.002643\n0.002456\n0.001815\n0.003857\n0.005319\n-0.006987\n-0.004216\n0.005273\n-0.005012\n\n\n25\n2023-10-18\n0.002991\n0.003463\n0.002342\n0.004471\n0.005252\n-0.007555\n0.015235\n-0.010860\n-0.016209\n\n\n26\n2023-10-19\n0.002939\n0.002420\n0.002808\n0.004705\n0.005759\n-0.003243\n0.017825\n0.006352\n-0.015042\n\n\n27\n2023-10-20\n0.002597\n0.003378\n0.003937\n0.006227\n0.005866\n0.000581\n0.001925\n0.006536\n0.007198\n\n\n28\n2023-10-21\n0.003465\n0.003007\n0.003841\n0.005584\n0.005469\n0.004090\n-0.001287\n-0.006406\n-0.018812\n\n\n29\n2023-10-22\n0.003382\n0.002835\n0.003759\n0.005523\n0.006076\n0.001502\n-0.003799\n-0.001092\n-0.019789\n\n\n\n\n\n\n\n\ncv_df_plot = long_form(cv_df_filter)\n\n\ncv_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-10-11\nRNN\n0.003673\n\n\n1\n2023-10-12\nRNN\n0.003411\n\n\n2\n2023-10-13\nRNN\n0.005272\n\n\n3\n2023-10-14\nRNN\n0.004834\n\n\n4\n2023-10-15\nRNN\n0.003371\n\n\n...\n...\n...\n...\n\n\n265\n2023-10-18\nNBEATSx\n-0.016209\n\n\n266\n2023-10-19\nNBEATSx\n-0.015042\n\n\n267\n2023-10-20\nNBEATSx\n0.007198\n\n\n268\n2023-10-21\nNBEATSx\n-0.018812\n\n\n269\n2023-10-22\nNBEATSx\n-0.019789\n\n\n\n\n270 rows × 3 columns\n\n\n\n\naltair_plot(cv_df_plot)"
  },
  {
    "objectID": "weather_api.html",
    "href": "weather_api.html",
    "title": "Weather API",
    "section": "",
    "text": "def long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\nurl1 = 'http://api.weatherapi.com/v1/current.json?key=e7e650398d6447f7a35121707230412&q=Brisbane&aqi=yes'\nurl2 = 'http://api.weatherapi.com/v1/forecast.json?key=e7e650398d6447f7a35121707230412&q=Brisbane&days=10&aqi=no&alerts=no'\nurl3 = 'http://api.weatherapi.com/v1/history.json?key=e7e650398d6447f7a35121707230412&q=Brisbane&dt=2023-01-01'\nclass Weather_api_url:\n    def __init__(self, token:str= None, dataType:str = 'json', location:str = 'Brisbane', api:str = 'no'):\n        self.url = 'http://api.weatherapi.com/v1/'\n        self.token = token\n        self.dataType = dataType\n        self.location = location\n        self.aqi = api\n        \n    def __str__(self):\n        return self.url + '&lt;detail_type_ex.current&gt;&lt;json,xml&gt;?key=&lt;token&gt;&q=&lt;location&gt;&aqi=&lt;yes,no&gt;'\n        \n    def current(self):\n        url = self.url + 'current.' + self.dataType+'?key='+ self.token + '&q=' + self.location + '&aqi=' + self.aqi\n        return url\n\n    \n    def forecast(self, days:int = 10):\n        url = self.url + 'forecast.' + self.dataType+'?key='+ self.token + '&q=' + self.location + '&days=' + str(days) + '&aqi=' + self.aqi + '&alerts=no'\n        return url\n    \n    def history(self, date:str = '2023-01-01'):\n        url = self.url + 'history.' + self.dataType+'?key='+ self.token + '&q=' + self.location + '&dt=' + date\n        return url\nfrom io import StringIO\n\ndef weather_request(url):\n    import requests\n    import json\n    # Make a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful (status code 200)\n    if response.status_code == 200:\n        data = response.json()\n        # Print or work with each array\n        return data\n    else:\n        # Print an error message if the request was not successful\n        print(f\"Error: {response.status_code}\")\n\n        \ndef data_filter(data, filter_sub: str = 'forecast'):\n    import json\n    import pandas as pd\n    \n    df = pd.DataFrame()\n    location = data.get(\"location\")\n    current = data.get(\"current\")\n    forecast = data.get(\"forecast\")\n    \n    if filter_sub == 'forecast':\n        if forecast is None:\n            print(\"The variable is None\")\n            return\n        else:\n            forecastday = forecast.get(\"forecastday\")\n            \n            for days in forecastday:\n                a = days.get(\"hour\", [])\n                for rows in a:\n                    new = pd.DataFrame(rows)\n                    new.drop(['time_epoch', 'condition', 'wind_dir', 'pressure_in', 'wind_mph', 'precip_in', \n                            'temp_f', 'heatindex_f', 'feelslike_f', 'windchill_f', 'dewpoint_f', 'will_it_snow',\n                            'will_it_rain', 'chance_of_snow', 'vis_miles','gust_mph' ], axis=1, inplace=True)\n                    df = pd.concat([df, new[0:1]], ignore_index=True)\n                    \n            \n            df.rename(columns={'time': 'ds'}, inplace=True)\n                    \n    elif filter_sub == 'current':\n        if current is None:\n            print(\"The variable is None\")\n            return\n        else:\n            df = pd.DataFrame(current)\n            df.drop(['last_updated_epoch', 'condition', 'wind_dir', 'pressure_in', 'wind_mph', 'precip_in', \n                             'temp_f', 'feelslike_f', 'vis_miles','gust_mph' ], axis=1, inplace=True)\n            df.rename(columns={'last_updated': 'ds'}, inplace=True)\n        \n    else:\n        if location is None:\n            print(\"The variable is None\")\n            return\n        else:\n            df = pd.DataFrame(location, index=[0]) \n\n    return df\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n\n    return df\ndef his_data(token, date):\n    url = Weather_api_url(token).history(date) #creates the link to retrieve json data\n    data_json = weather_request(url) #returns the json data\n    \n    df = data_filter(data_json, 'forecast')  #turns json into pd and filters the data\n    \n    return df    #return pd dateframe\ndef historical_data(token, start_date = '2023-01-01', end_date = '2023-01-10', freq = 'D', path = None, name = 'weather'):\n    from tqdm.notebook import trange, tqdm\n    import pandas as pd\n    \n    rng = pd.date_range(start_date, end_date, freq=freq) #create date list between start and end date\n\n    dates =  rng.strftime('%Y-%m-%d')  #change date format from timestamp to date\n\n    df = pd.DataFrame()\n    for date in tqdm(dates, desc=\"Downloading Days\", unit=\"days\"):    #loop through date in date list\n        new = his_data(token, date)      #download pd dataframe for the day\n        df = pd.concat([df, new], ignore_index=True)  #add the pd dataframe to main pd dataframe \n    \n\n    if path is None:\n        pass\n    else:\n        df.to_csv(f'{path}/{name}.csv', index=False) #save the data as a csv\n    \n    return pd\nstartDate= '2023-01-01'\nendDate = '2023-12-10'\nfreq = 'D'\npath = 'Data'\nname = 'weather'\nimport pandas as pd\ndf = pd.read_csv(f'{path}/{name}.csv')\ndf.shape\n\n(8256, 17)\ndf[-24:]\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n8232\n2023-12-10 00:00\n22.3\n0\n9.0\n59\n1017.0\n0.01\n77\n88\n24.7\n22.3\n24.7\n18.1\n0\n10.0\n14.9\n1.0\n\n\n8233\n2023-12-10 01:00\n22.9\n0\n9.7\n67\n1016.0\n0.00\n72\n86\n25.0\n22.9\n25.0\n17.7\n0\n10.0\n15.2\n1.0\n\n\n8234\n2023-12-10 02:00\n23.8\n0\n11.9\n72\n1016.0\n0.00\n66\n68\n25.5\n23.8\n25.5\n17.0\n0\n10.0\n17.0\n1.0\n\n\n8235\n2023-12-10 03:00\n24.0\n0\n13.3\n74\n1015.0\n0.00\n64\n64\n25.5\n24.0\n25.5\n16.7\n0\n10.0\n18.8\n1.0\n\n\n8236\n2023-12-10 04:00\n24.0\n0\n14.0\n75\n1015.0\n0.00\n64\n68\n25.5\n24.0\n25.5\n16.6\n0\n10.0\n20.0\n1.0\n\n\n8237\n2023-12-10 05:00\n24.1\n1\n15.5\n76\n1016.0\n0.00\n63\n59\n25.6\n24.1\n25.6\n16.6\n0\n10.0\n21.7\n6.0\n\n\n8238\n2023-12-10 06:00\n24.4\n1\n16.2\n83\n1016.0\n0.00\n62\n61\n25.8\n24.4\n25.8\n16.5\n0\n10.0\n21.8\n6.0\n\n\n8239\n2023-12-10 07:00\n24.7\n1\n16.6\n84\n1017.0\n0.01\n61\n69\n26.0\n24.7\n26.0\n16.7\n0\n10.0\n21.1\n5.0\n\n\n8240\n2023-12-10 08:00\n24.9\n1\n16.6\n83\n1017.0\n0.00\n60\n41\n26.2\n24.9\n26.2\n16.5\n0\n10.0\n21.2\n6.0\n\n\n8241\n2023-12-10 09:00\n25.4\n1\n17.3\n88\n1017.0\n0.01\n59\n53\n26.5\n25.4\n26.5\n16.7\n0\n10.0\n20.4\n6.0\n\n\n8242\n2023-12-10 10:00\n26.3\n1\n16.6\n90\n1017.0\n0.01\n57\n75\n27.3\n26.3\n27.3\n17.0\n0\n10.0\n19.0\n6.0\n\n\n8243\n2023-12-10 11:00\n27.3\n1\n19.4\n84\n1017.0\n0.00\n52\n2\n28.1\n27.3\n28.1\n16.6\n0\n10.0\n22.4\n7.0\n\n\n8244\n2023-12-10 12:00\n27.8\n1\n19.4\n83\n1016.0\n0.01\n51\n72\n28.6\n27.8\n28.6\n16.8\n0\n10.0\n22.4\n6.0\n\n\n8245\n2023-12-10 13:00\n27.8\n1\n20.5\n84\n1016.0\n0.00\n51\n0\n28.6\n27.8\n28.6\n16.9\n0\n10.0\n23.6\n7.0\n\n\n8246\n2023-12-10 14:00\n27.6\n1\n20.9\n88\n1015.0\n0.00\n52\n0\n28.3\n27.6\n28.3\n16.7\n0\n10.0\n24.0\n7.0\n\n\n8247\n2023-12-10 15:00\n27.1\n1\n20.5\n87\n1015.0\n0.00\n53\n1\n27.9\n27.1\n27.9\n16.7\n0\n10.0\n23.6\n7.0\n\n\n8248\n2023-12-10 16:00\n26.5\n1\n20.2\n88\n1015.0\n0.00\n55\n9\n27.4\n26.5\n27.4\n16.7\n0\n10.0\n23.2\n7.0\n\n\n8249\n2023-12-10 17:00\n25.8\n1\n19.8\n91\n1015.0\n0.00\n57\n15\n26.8\n25.8\n26.8\n16.6\n0\n10.0\n23.4\n7.0\n\n\n8250\n2023-12-10 18:00\n25.1\n1\n19.8\n93\n1016.0\n0.00\n59\n35\n26.2\n25.1\n26.2\n16.4\n0\n10.0\n25.8\n7.0\n\n\n8251\n2023-12-10 19:00\n24.8\n0\n20.2\n93\n1016.0\n0.00\n59\n78\n26.0\n24.8\n26.0\n16.3\n0\n10.0\n27.1\n1.0\n\n\n8252\n2023-12-10 20:00\n24.7\n0\n19.8\n91\n1017.0\n0.01\n59\n90\n25.9\n24.7\n25.9\n16.2\n0\n10.0\n26.6\n1.0\n\n\n8253\n2023-12-10 21:00\n24.6\n0\n19.1\n92\n1017.0\n0.01\n58\n89\n25.9\n24.7\n25.9\n15.9\n0\n10.0\n26.3\n1.0\n\n\n8254\n2023-12-10 22:00\n24.6\n0\n19.4\n91\n1017.0\n0.02\n57\n93\n25.8\n24.6\n25.8\n15.4\n0\n10.0\n26.3\n1.0\n\n\n8255\n2023-12-10 23:00\n23.8\n0\n16.9\n98\n1017.0\n0.04\n65\n74\n25.4\n23.8\n25.4\n16.7\n0\n10.0\n22.6\n1.0\ndf = data_scaler(df)  #normalise the data\ndf_plot = long_form(df[0:200])\naltair_plot(df_plot)"
  },
  {
    "objectID": "weather_api.html#forecast",
    "href": "weather_api.html#forecast",
    "title": "Weather API",
    "section": "Forecast",
    "text": "Forecast\n\nurl = Weather_api_url(token).forecast(10)\ndata_json = weather_request(url)\ndf = data_filter(data_json, 'forecast')\ndf = data_scaler(df)\n\n\nimport pandas as pd\ndf.shape\n\n\ndf_plot = long_form(df)\naltair_plot(df_plot)"
  },
  {
    "objectID": "xplainable forecasting.html",
    "href": "xplainable forecasting.html",
    "title": "Example – Using xplainable for ML Forecasting",
    "section": "",
    "text": "the default version of altair in colab doesn’t work with xplainable, so run this script to upgrade it\npip install --upgrade xplainable altair\npip install statsmodels"
  },
  {
    "objectID": "xplainable forecasting.html#define-the-forecasting-class",
    "href": "xplainable forecasting.html#define-the-forecasting-class",
    "title": "Example – Using xplainable for ML Forecasting",
    "section": "Define the Forecasting Class",
    "text": "Define the Forecasting Class\nWe will add an enhanced version of this to the xplainable package in the future. This is a fast and dirty approach.\n\nclass XForecast:\n    \"\"\"ML Forecasting with xplainable.\n\n      This is whipped together for testing – feel free to update and adjust.\n\n    \"\"\"\n    def __init__(self, model, pipeline, trend_decompose=True):\n        self.model = model\n        self.pipeline = pipeline\n        self.trend_decompose = trend_decompose\n        self.target_column = None\n        self.slope = None\n        self.intercept = None\n        self.observations = None\n\n    def _apply_transformations(self, data, is_train=True):\n        \"\"\"Fast and dirty feature preprocessor\"\"\"\n        transformed_data = data.copy()\n\n        transformed_data['month'] = transformed_data.index.month\n\n        for column, operations in self.pipeline.items():\n            if column not in transformed_data.columns:\n                continue\n\n            if \"lag\" in operations:\n                for lag in operations[\"lag\"]:\n                    transformed_data[f\"{column}_lag{lag}\"] = \\\n                    transformed_data[column].shift(lag)\n\n            if \"rolling\" in operations:\n                for window in operations[\"rolling\"]:\n                    transformed_data[f\"{column}_rolling{window}\"] = \\\n                    transformed_data[column].rolling(window).mean()\n\n            if \"gradient\" in operations:\n                for gradient in operations[\"gradient\"]:\n                    transformed_data[f\"{column}_gradient{gradient}\"] = (\n                        transformed_data[column].shift(1) - \\\n                        transformed_data[column].shift(1+gradient)) / \\\n                        transformed_data[column].shift(1+gradient)\n\n            if \"change\" in operations:\n                for n_steps in operations[\"change\"]:\n                    transformed_data[f\"{column}_change{n_steps}\"] = \\\n                    transformed_data[column] - transformed_data[column].shift(\n                        n_steps)\n\n            if \"impute\" in operations:\n                method = operations[\"impute\"]\n                transformed_data[column].fillna(method=method, inplace=True)\n\n        if is_train:\n            transformed_data.dropna(inplace=True)\n\n        return transformed_data\n\n    def _get_trend_coefficients(self, trend_data):\n        \"\"\"Essentially a linear regression on the trend values\"\"\"\n\n        # Calculate linear model on trend\n        y_values = trend_data.values[~np.isnan(trend_data.values)]\n        self.observations = len(y_values)\n        x_values = np.arange(len(y_values))\n\n        sum_x = sum(x_values)\n        sum_y = sum(y_values)\n        sum_x_squared = sum(x**2 for x in x_values)\n        sum_xy = sum(x*y for x, y in zip(x_values, y_values))\n        n = len(x_values)\n\n        # Calculate slope m and y-intercept c\n        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x**2)\n        intercept = (sum_y - slope * sum_x) / n\n\n        return slope, intercept\n\n    def fit(self, data, target_column, opt=False):\n        \"\"\"Fits the data to the model. `opt` will optimise the xplainable model\n        \"\"\"\n\n        data = data.copy()\n        self.target_column = target_column\n\n        if self.trend_decompose:\n            # Perform STL decomposition\n            result = seasonal_decompose(data[target_column], model='additive')\n\n            # Remove the trend\n            detrended = data[target_column] - result.trend\n\n            # Calculate the LR coefficient and intercept\n            self.slope, self.intercept = self._get_trend_coefficients(\n                result.trend)\n\n            data[target_column] = detrended.values\n            # Will remove the first and last 3 rows because of rolling window\n            data = data.dropna()\n\n        # Run pipeline\n        train_data = self._apply_transformations(data)\n\n        X_train = train_data.drop(target_column, axis=1)\n        y_train = train_data[target_column]\n\n        # Fit the model\n        self.model.fit(X_train, y_train)\n\n        # Only run this line if using xplainable\n        if self.model.__class__.__name__ == 'XRegressor':\n            self.model.optimise_tail_sensitivity(X_train, y_train)\n\n        # This can only be done on xplainable models\n        if opt:\n            # Create the network\n            network = XEvolutionaryNetwork(self.model)\n\n            # Add the layers\n            # Add an Evolve layer with a high severity\n            network.add_layer(\n                Evolve(\n                    mutations=100,\n                    generations=50,\n                    max_severity=0.5,\n                    max_leaves=20,\n                    early_stopping=20\n                    )\n                )\n\n            # Add another Evolve layer with a lower severity and reach\n            network.add_layer(\n                Evolve(\n                    mutations=100,\n                    generations=50,\n                    max_severity=0.3,\n                    max_leaves=15,\n                    early_stopping=20\n                    )\n                )\n\n            # Fit the network (before or after adding layers)\n            network.fit(X_train, y_train)\n\n            # Run the network\n            network.optimise()\n\n    def predict(self, data, nsteps=1):\n        data = data.copy()\n\n        offset = 0\n        if self.trend_decompose:\n            # Offset is due to the dropped nan values when decomposing in fit()\n            offset = -6\n\n        forecast_dates = pd.date_range(\n            start=data.index[-1] + pd.DateOffset(months=offset),\n            periods=nsteps, freq='MS')\n\n        if self.trend_decompose:\n            result = seasonal_decompose(\n                data[self.target_column], model='additive')\n\n            # Removing the seasonality\n            detrended = data[self.target_column] - result.trend\n            data[self.target_column] = detrended.values\n            data = data.dropna()\n            trend_idxs = np.arange(len(data), len(data) + nsteps)\n            predicted_trend = (trend_idxs * self.slope) + self.intercept\n\n        predictions = []\n        for _ in range(nsteps):\n            transformed_data = self._apply_transformations(data, is_train=False)\n\n            X_pred = transformed_data.iloc[-1].drop(\n                labels=[self.target_column], errors='ignore')\n\n            X_pred = pd.DataFrame(X_pred).T\n            y_pred = self.model.predict(X_pred)[0]\n            predictions.append(y_pred)\n\n            new_index = data.index[-1] + pd.DateOffset(months=1)\n            new_data_row = pd.DataFrame(\n                {self.target_column: [y_pred]}, index=[new_index])\n\n            data = pd.concat([data, new_data_row])\n\n        if self.trend_decompose:\n          predictions = predictions + predicted_trend\n\n        output = pd.DataFrame({\n            'date': forecast_dates,\n            'forecast': predictions\n        })\n\n        return output\n\n    def plot_predictions(self, X, prediction_df):\n\n        # Plot actual data\n        plt.figure(figsize=(14, 7))\n        plt.plot(X.index, train[\"Passengers\"], label=\"Actual\", color=\"blue\")\n\n        # Plot Prediction\n        plt.plot(\n            prediction_df.date,\n            prediction_df.forecast,\n            label=\"Forecast\",\n            linestyle=\"--\",\n            color=\"red\"\n            )\n\n        plt.title(\"Monthly Air Passengers: Actual vs Forecast\")\n        plt.xlabel(\"Date\")\n        plt.ylabel(\"Number of Passengers\")\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()"
  },
  {
    "objectID": "xplainable forecasting.html#load-example-data",
    "href": "xplainable forecasting.html#load-example-data",
    "title": "Example – Using xplainable for ML Forecasting",
    "section": "Load Example Data",
    "text": "Load Example Data\n\n# Load the dataset\nflights = sns.load_dataset(\"flights\")\n\n# Convert month names to datetime format with the given year\nflights['date'] = flights.apply(lambda row: pd.to_datetime(f\"{row['year']}-{row['month']}-01\"), axis=1)\nflights.set_index('date', inplace=True)\n\n# Drop redundant columns and rename the passengers column for clarity\nflights.drop(['year', 'month'], axis=1, inplace=True)\nflights.rename(columns={'passengers': 'Passengers'}, inplace=True)\n\n# Plot the data\nflights.plot(title=\"Monthly Air Passengers\", figsize=(12,6))\nplt.show()\n\n\n\n\n\ntrain = flights.copy()\n\n# Filter on later dates gives better results in this example\ntrain = train[train.index &gt;= '1956-07-01']\n\n\npipeline = {\n    \"Passengers\": {\n        \"lag\": [12], # Select\n        \"impute\": \"ffill\"\n    }\n}\n\nmodel = XForecast(XRegressor(max_depth=7), pipeline, trend_decompose=True)\nmodel.fit(train, \"Passengers\", opt=False) # Setting opt=True can be a little slow. Set to False for quick testing\n\n\nnsteps = 30  # You can change this to the desired forecast length\n\npredictions = model.predict(train, nsteps=nsteps)\nmodel.plot_predictions(train, predictions)"
  },
  {
    "objectID": "xplainable forecasting.html#explainers",
    "href": "xplainable forecasting.html#explainers",
    "title": "Example – Using xplainable for ML Forecasting",
    "section": "Explainers",
    "text": "Explainers\n\nxplainable model\nNote that this explainer ignores trend when decomposition is applied. See below to understand how the trend line comes into play.\n\nmodel.model.explain()\n\n\n    \n    \n\n\n\n\n\n\n\n\n\n\nTrend\n\nprint(f'The trend starts with an intercept of {round(model.intercept, 2)} and a slope coefficient of {round(model.slope, 2)}\\n')\n\nax, fig = plt.subplots(figsize=(16, 6))\n_len = len(train)+nsteps\nplt.plot(np.arange(0, _len) * model.slope + model.intercept)\n\n# Calculate y0 and y1\ny0 = model.intercept\ny1 = 10 * model.slope + model.intercept\n\n# Assume we want to illustrate the slope from the midpoint\nmidpoint = _len // 2\ny_at_midpoint = midpoint * model.slope + model.intercept\n\n# Add a horizontal line from the middle of the line going across ten units\nplt.plot([midpoint, midpoint+10], [y_at_midpoint, y_at_midpoint], 'r--')\n\n# Add a vertical line going from the end of the horizontal line back to the slope\nplt.plot([midpoint+10, midpoint+10], [y_at_midpoint, y_at_midpoint + 10 * model.slope], 'r--')\n\n# # Label for intercept and slope\nax.text(0.04, 0.1, f'Intercept: {round(y0, 2)}', verticalalignment='bottom', color='blue')\nax.text(0.59, 0.54, f'Slope: {round(model.slope, 2)}', verticalalignment='center', color='red')\nfig.set_xlim(1, None)\n\nplt.title(\"Understanding the Trend Line\")\nplt.grid(True)\nplt.show()\n\nThe trend starts with an intercept of 342.33 and a slope coefficient of 2.95"
  },
  {
    "objectID": "statistical_models.html",
    "href": "statistical_models.html",
    "title": "Statistical Models",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\nimport matplotlib.pyplot as plt\n\n# Generate a sine wave for demonstration\n\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, cycles * np.pi, weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\nsine_wave = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\nplt.plot( np.sin(t))\nplt.plot(0.3*np.sin(5*t))\nplt.plot(np.random.normal(scale=0.2, size=len(t)))\nplt.plot(waveform)\nplt.plot(sine_wave)\n\n\n\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': sine_wave})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   y          364 non-null    float64       \n 1   unique_id  364 non-null    int64         \n 2   ds         364 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 8.7 KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nfrom tqdm.autonotebook import tqdm\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive,\n    SeasonalExponentialSmoothing,\n)\n\nfrom statsforecast.utils import AirPassengersDF, ConformalIntervals\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=28)\na.plot();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "statistical_models.html#data-extraction",
    "href": "statistical_models.html#data-extraction",
    "title": "Statistical Models",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\nimport matplotlib.pyplot as plt\n\n# Generate a sine wave for demonstration\n\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, cycles * np.pi, weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\nsine_wave = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\nplt.plot( np.sin(t))\nplt.plot(0.3*np.sin(5*t))\nplt.plot(np.random.normal(scale=0.2, size=len(t)))\nplt.plot(waveform)\nplt.plot(sine_wave)\n\n\n\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': sine_wave})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   y          364 non-null    float64       \n 1   unique_id  364 non-null    int64         \n 2   ds         364 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 8.7 KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nfrom tqdm.autonotebook import tqdm\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive,\n    SeasonalExponentialSmoothing,\n)\n\nfrom statsforecast.utils import AirPassengersDF, ConformalIntervals\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=28)\na.plot();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "statistical_models.html#create-model",
    "href": "statistical_models.html#create-model",
    "title": "Statistical Models",
    "section": "Create Model",
    "text": "Create Model\n\nfrom statsforecast import models\nmodels.__all__\n\n['AutoARIMA',\n 'AutoETS',\n 'ETS',\n 'AutoCES',\n 'AutoTheta',\n 'ARIMA',\n 'AutoRegressive',\n 'SimpleExponentialSmoothing',\n 'SimpleExponentialSmoothingOptimized',\n 'SeasonalExponentialSmoothing',\n 'SeasonalExponentialSmoothingOptimized',\n 'Holt',\n 'HoltWinters',\n 'HistoricAverage',\n 'Naive',\n 'RandomWalkWithDrift',\n 'SeasonalNaive',\n 'WindowAverage',\n 'SeasonalWindowAverage',\n 'ADIDA',\n 'CrostonClassic',\n 'CrostonOptimized',\n 'CrostonSBA',\n 'IMAPA',\n 'TSB',\n 'MSTL',\n 'Theta',\n 'OptimizedTheta',\n 'DynamicTheta',\n 'DynamicOptimizedTheta',\n 'GARCH',\n 'ARCH',\n 'ConstantModel',\n 'ZeroModel',\n 'NaNModel']\n\n\n\nhorizon = 28 * 2\n\ntrain_size = weeks * resolution - horizon\ntrain, test = df[:train_size], df[train_size:]\n\n\ndf.shape, train.shape, test.shape\n\n((364, 3), (308, 3), (56, 3))\n\n\n\nlen(test)\n\nNameError: name 'futr_df' is not defined\n\n\n\nintervals = ConformalIntervals(h=horizon, n_windows=2)\nseason_length = 28\n\nmodels = [\n    AutoARIMA(season_length=season_length, prediction_intervals=intervals),\n    HoltWinters(season_length=season_length, error_type=\"A\"),\n    #Croston(),\n    SeasonalNaive(season_length=season_length),\n    #HistoricAverage(),\n    DOT(season_length=season_length),\n    SeasonalExponentialSmoothing(alpha=0.8, season_length=season_length),\n]\n\n\nsf = StatsForecast(\n    df = train,\n    models = models,\n    freq = 'D',\n    n_jobs=-1,\n    verbose = True\n)"
  },
  {
    "objectID": "statistical_models.html#forecast",
    "href": "statistical_models.html#forecast",
    "title": "Statistical Models",
    "section": "Forecast",
    "text": "Forecast\n\nfore = sf.forecast(h=horizon,\n                   #prediction_intervals=intervals,\n                   \n                 # level=[90]\n                 )\n\n\n\n\n\nfore_plot = long_form(fore)\nfore_plot\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2022-02-03\nunique_id\n1.000000\n\n\n1\n2022-02-04\nunique_id\n1.000000\n\n\n2\n2022-02-05\nunique_id\n1.000000\n\n\n3\n2022-02-06\nunique_id\n1.000000\n\n\n4\n2022-02-07\nunique_id\n1.000000\n\n\n...\n...\n...\n...\n\n\n331\n2022-03-26\nSeasonalES\n-0.611406\n\n\n332\n2022-03-27\nSeasonalES\n0.154510\n\n\n333\n2022-03-28\nSeasonalES\n-0.348743\n\n\n334\n2022-03-29\nSeasonalES\n-0.824028\n\n\n335\n2022-03-30\nSeasonalES\n-0.291591\n\n\n\n\n336 rows × 3 columns\n\n\n\n\ny_plot = df[int(-horizon*2):]\ny_plot.columns\n\nIndex(['y', 'unique_id', 'ds'], dtype='object')\n\n\n\ny_plot.rename(columns={'y': 'price'}, inplace= True)\n\n\nfore_plot = pd.concat([fore_plot,y_plot])\nfore_plot\n\n\n\n\n\n\n\n\nds\nunique_id\nprice\n\n\n\n\n0\n2022-02-03\nunique_id\n1.000000\n\n\n1\n2022-02-04\nunique_id\n1.000000\n\n\n2\n2022-02-05\nunique_id\n1.000000\n\n\n3\n2022-02-06\nunique_id\n1.000000\n\n\n4\n2022-02-07\nunique_id\n1.000000\n\n\n...\n...\n...\n...\n\n\n359\n2022-03-26\n1\n-1.075326\n\n\n360\n2022-03-27\n1\n-1.071706\n\n\n361\n2022-03-28\n1\n-1.120472\n\n\n362\n2022-03-29\n1\n-0.144448\n\n\n363\n2022-03-30\n1\n-0.032383\n\n\n\n\n448 rows × 3 columns\n\n\n\n\naltair_plot(fore_plot)\n\n\n\n\n\n\n\n\nfore.iloc[:,2:].head()\n\n\n\n\n\n\n\n\nAutoARIMA\nHoltWinters\nSeasonalNaive\nDynamicOptimizedTheta\nSeasonalES\n\n\n\n\n0\n0.229876\n0.258348\n-0.085467\n0.578754\n0.034635\n\n\n1\n0.685502\n0.835712\n0.576483\n1.112219\n0.562846\n\n\n2\n0.532418\n0.842493\n-0.385228\n1.109493\n-0.068535\n\n\n3\n1.296907\n1.168408\n0.719438\n1.385704\n0.892200\n\n\n4\n0.933898\n1.007089\n0.352424\n1.289210\n0.525480"
  },
  {
    "objectID": "statistical_models.html#cross-validation",
    "href": "statistical_models.html#cross-validation",
    "title": "Statistical Models",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nn_windows = 5\ncrossvalidation_df = sf.cross_validation(\n    h = horizon,\n    step_size = horizon,\n    n_windows = n_windows\n  )\n\n\n\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nSeasonalNaive\nDynamicOptimizedTheta\nSeasonalES\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2021-04-29\n2021-04-28\n-0.011988\n-0.803447\n-1.114567\n0.882026\n-0.910163\n0.882026\n\n\n1\n2021-04-30\n2021-04-28\n0.285825\n-0.849951\n-1.202070\n1.184884\n-0.910163\n1.184884\n\n\n1\n2021-05-01\n2021-04-28\n0.139007\n-0.827595\n-1.289573\n1.680301\n-0.910163\n1.680301\n\n\n1\n2021-05-02\n2021-04-28\n0.624833\n-0.838343\n-1.377077\n2.234016\n-0.910163\n2.234016\n\n\n1\n2021-05-03\n2021-04-28\n0.029964\n-0.833176\n-1.464580\n1.950096\n-0.910163\n1.950096\n\n\n\n\n\n\n\n\ncrossvalidation_df.drop('cutoff', axis=1, inplace=True)\n\n\ncrossvalidation_df_plot = long_form(crossvalidation_df)\n\n\naltair_plot(crossvalidation_df_plot)"
  },
  {
    "objectID": "statistical_models.html#performance",
    "href": "statistical_models.html#performance",
    "title": "Statistical Models",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nfore.info(), fore.columns, fore['AutoARIMA'].values\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 56 entries, 0 to 55\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype         \n---  ------                 --------------  -----         \n 0   unique_id              56 non-null     int64         \n 1   ds                     56 non-null     datetime64[ns]\n 2   AutoARIMA              56 non-null     float32       \n 3   HoltWinters            56 non-null     float32       \n 4   SeasonalNaive          56 non-null     float32       \n 5   DynamicOptimizedTheta  56 non-null     float32       \n 6   SeasonalES             56 non-null     float32       \ndtypes: datetime64[ns](1), float32(5), int64(1)\nmemory usage: 2.1 KB\n\n\n(None,\n Index(['unique_id', 'ds', 'AutoARIMA', 'HoltWinters', 'SeasonalNaive',\n        'DynamicOptimizedTheta', 'SeasonalES'],\n       dtype='object'),\n array([ 0.22987553,  0.6855018 ,  0.5324179 ,  1.296907  ,  0.93389785,\n         1.0671949 ,  0.5716412 ,  1.4716152 ,  0.6917453 ,  0.95640403,\n         0.4769931 ,  0.58612704,  1.0626335 ,  0.30788222,  0.2735823 ,\n         0.34365815, -0.02484687,  0.14583941, -0.24158752, -1.596629  ,\n        -1.0124251 , -0.54332614, -0.99400973, -0.8594592 , -0.24258256,\n        -0.69162637, -1.2751557 ,  0.04646311,  0.08479773,  0.6407017 ,\n         0.08987495,  1.0205237 ,  0.6548361 ,  0.9910863 ,  0.4315685 ,\n         1.3673506 ,  0.661564  ,  0.8148673 ,  0.09596778,  0.24554151,\n         1.631837  ,  0.2345393 ,  0.18494536,  0.36019734, -0.09524097,\n         0.1778594 , -0.26101044, -0.8905563 , -1.1690307 , -0.25150737,\n        -0.7923761 , -0.65975606,  0.11130547, -0.3994452 , -0.95437896,\n        -0.22174656], dtype=float32))\n\n\n\nrmse_score = rmse(test['y'].values, fore['HoltWinters'].values)\nprint(\"RMSE between forecast and real: \", rmse_score)\n\nRMSE between forecast and real:  0.6662927609114181\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 2: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nevaluate_performace(train, test, fore).style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['unique_id', 'ds', 'AutoARIMA', 'HoltWinters', 'SeasonalNaive',\n       'DynamicOptimizedTheta', 'SeasonalES'],\n      dtype='object')\n\n\n\n\n\n\n\n \nAutoARIMA\nHoltWinters\nSeasonalNaive\nDynamicOptimizedTheta\nSeasonalES\n\n\n\n\nmase\n0.552126\n0.376421\n0.631609\n0.442360\n0.564715\n\n\nmae\n0.791465\n0.539595\n0.905403\n0.634117\n0.809510\n\n\nmape\n165.287808\n113.312947\n141.615219\n147.870999\n135.201094\n\n\nrmse\n0.904719\n0.666293\n1.083679\n0.769611\n0.951087\n\n\nsmape\n116.076059\n86.779071\n130.180295\n89.625308\n123.576836\n\n\n\n\n\n\nresults = evaluate_performace(train, test, fore)\nresults.to_csv('Data/Stat_results2.csv')\n\nIndex(['unique_id', 'ds', 'AutoARIMA', 'HoltWinters', 'SeasonalNaive',\n       'DynamicOptimizedTheta', 'SeasonalES'],\n      dtype='object')\n\n\n\ncutoff = crossvalidation_df['cutoff'].unique()\nn_models = len(crossvalidation_df.columns) - 3\nrmse_score = np.zeros((n_windows,n_models), dtype=np.float32)\n\nfor i, model in enumerate(crossvalidation_df.columns):\n    if i &lt; 3:\n        continue    \n    for k in range(len(cutoff)): \n        cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    \n        rmse_score[k,i-3] = rmse(cv[f'{model}'], cv.y)\n    plt.plot(rmse_score[:,i-3], label = f'{model}')\n        \n\nplt.legend()\nplt.show()\n\n\n\n\n\nrmse_score\n\narray([[1.0727004 , 3.50553   , 1.2164838 , 1.1151466 , 1.2164838 ],\n       [3.1713963 , 1.5862346 , 1.1854093 , 1.8507779 , 1.05398   ],\n       [1.1339344 , 0.7946055 , 1.142026  , 1.2360449 , 1.0382159 ],\n       [0.7463759 , 0.76247925, 1.0607373 , 0.8071447 , 0.97478694],\n       [0.908804  , 0.74267846, 1.0655735 , 0.7429336 , 0.94502705]],\n      dtype=float32)"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Phisaver Project",
    "section": "",
    "text": "Develop ML model to\n\nPredict consumption\nDetect anomalies\ndetect devices from using only the total power usage\n\nsolar = func(future clouds, future sunlight, past three days)\nconsumption = func(individual components, future weather, future humidity, past three days, past week, past month)\naircon = func(future weather, future humidity, past three days)"
  },
  {
    "objectID": "core.html#project-specifications",
    "href": "core.html#project-specifications",
    "title": "Phisaver Project",
    "section": "",
    "text": "Develop ML model to\n\nPredict consumption\nDetect anomalies\ndetect devices from using only the total power usage\n\nsolar = func(future clouds, future sunlight, past three days)\nconsumption = func(individual components, future weather, future humidity, past three days, past week, past month)\naircon = func(future weather, future humidity, past three days)"
  },
  {
    "objectID": "core.html#data-strucuture",
    "href": "core.html#data-strucuture",
    "title": "Phisaver Project",
    "section": "Data Strucuture",
    "text": "Data Strucuture\n\nEnergy\n\n1 hour integral of power in kWh\n2021/04/01 - 2023/11/09\n\nPower\n\n5 mins average of Iotawatt\n2022/07/21 - 2023/11/09\n\nIotawatt\n\n1 min average of sensor readings"
  },
  {
    "objectID": "core.html#frameworks",
    "href": "core.html#frameworks",
    "title": "Phisaver Project",
    "section": "Frameworks",
    "text": "Frameworks\n\nDarts models : https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html\nNixtla models : https://nixtla.github.io/statsforecast/src/core/models.html\nsktime"
  },
  {
    "objectID": "core.html#models",
    "href": "core.html#models",
    "title": "Phisaver Project",
    "section": "Models",
    "text": "Models\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nStatical\n\nAutoARIMA:\n\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion.\n\nHoltWinters:\n\nTriple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality.\n\nCrostonClassic as Croston:\nHistoricAverage: arthimetic mean\nDynamicOptimizedTheta as DOT:\n\nThe theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series.\n\nSeasonalNaive:\n\nMemory Efficient Seasonal Naive predictions\n\nMSTL\nFFT\n\n\n\nML\n\nXGBRegressor\nLGBMRegressor\nLinearRegression\n\ntarget_transforms: These are transformations applied to the target variable before model training and after model prediction. This can be useful when working with data that may benefit from transformations, such as log-transforms for highly skewed data.\nlags: This parameter accepts specific lag values to be used as regressors. Lags represent how many steps back in time you want to look when creating features for your model. For example, if you want to use the previous day’s data as a feature for predicting today’s value, you would specify a lag of 1.\nlags_transforms: These are specific transformations for each lag. This allows you to apply transformations to your lagged features.\n\nexpanding_mean\nrolling_mean\n\ndate_features: This parameter specifies date-related features to be used as regressors. For instance, you might want to include the day of the week or the month as a feature in your model.\nnum_threads: This parameter controls the number of threads to use for parallelizing feature creation, helping to speed up this process when working with large datasets."
  },
  {
    "objectID": "core.html#neural",
    "href": "core.html#neural",
    "title": "Phisaver Project",
    "section": "Neural",
    "text": "Neural\n\nNBEATS\nNHITS\nMLP"
  },
  {
    "objectID": "index.html#future-prediction-using-nixtla-library",
    "href": "index.html#future-prediction-using-nixtla-library",
    "title": "Time Series Analysis",
    "section": "Future Prediction using Nixtla Library",
    "text": "Future Prediction using Nixtla Library"
  },
  {
    "objectID": "consumption_model.html",
    "href": "consumption_model.html",
    "title": "Consumption Model for a Month",
    "section": "",
    "text": "Energy\n\n1 hour integral of power in kWh\n2021/04/01 - 2023/11/09\n\nPower\n\n5 mins average of Iotawatt\n2022/07/21 - 2023/11/09\n\ncreate list of other variable to tune"
  },
  {
    "objectID": "consumption_model.html#get-data",
    "href": "consumption_model.html#get-data",
    "title": "Consumption Model for a Month",
    "section": "Get Data",
    "text": "Get Data\n\nresult = query_api.query_data_frame(org=org, query=query)\nresult.columns\n\nIndex(['result', 'table', '_start', '_stop', '_time', '_value', '_field',\n       '_measurement', 'device', 'sensor', 'units'],\n      dtype='object')\n\n\n\nresult.shape\n\n(4182, 11)\n\n\nresult.to_csv(f’./Data/energy_info.csv’, index=False) #save the data as a csv\n\nresult.drop(['result', 'table', '_field', '_start', '_stop', '_measurement', 'units', 'device'], axis=1, inplace=True)\n\n\nresult.head()\n\n\n\n\n\n\n\n\n_time\n_value\nsensor\n\n\n\n\n0\n2023-03-02 00:00:00+00:00\n12.888000\nAircon\n\n\n1\n2023-03-03 00:00:00+00:00\n121.541007\nAircon\n\n\n2\n2023-03-04 00:00:00+00:00\n222.993806\nAircon\n\n\n3\n2023-03-05 00:00:00+00:00\n118.435757\nAircon\n\n\n4\n2023-03-06 00:00:00+00:00\n238.721389\nAircon\n\n\n\n\n\n\n\n\nresult.columns\n\nIndex(['_time', '_value', 'sensor'], dtype='object')"
  },
  {
    "objectID": "consumption_model.html#energy-in-wide-form",
    "href": "consumption_model.html#energy-in-wide-form",
    "title": "Consumption Model for a Month",
    "section": "Energy in Wide Form",
    "text": "Energy in Wide Form\n\nwide_df = result.pivot(index='_time', columns='sensor', values='_value')\n# Reset the index to make 'id' a regular column\nwide_df.reset_index(inplace=True)\n\nwide_df.columns\n\nIndex(['_time', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production', 'Unmonitored'],\n      dtype='object', name='sensor')\n\n\n\nwide_df['_time'] = pd.to_datetime(wide_df['_time']).dt.tz_localize(None)\nwide_df.drop(['Unmonitored'], axis = 1, inplace=True)\nwide_df.rename(columns={'_time':'ds'}, inplace=True)\n# Delete the last row\nwide_df = wide_df.drop(wide_df.index[-1])\n\n\nwide_df_new = pd.DataFrame()\nfor column in wide_df.columns:\n    wide_df_new[f'{column}'] = wide_df[f'{column}'].interpolate()\n\nwide_df = pd.DataFrame(wide_df_new)\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'Consumption', 'ConsumptionNet', 'Export', 'Fridge',\n       'Garage', 'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR',\n       'HouseBoardR', 'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints',\n       'Production'],\n      dtype='object')\n\n\n\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nConsumption\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n8.578005\n357.275503\n866.654129\n180.242247\n25.492343\n24.017087\n0.530865\n999.775552\n333.258545\n366.408449\n71.876417\n80.009421\n-108.365830\n66.086281\n176.392548\n491.748285\n\n\n241\n2023-10-29\n8.661682\n179.501779\n866.654129\n806.610093\n19.634587\n2.731102\n0.192283\n530.312474\n176.770805\n201.397413\n1.974071\n60.108551\n-804.636021\n1.253300\n103.451046\n1017.375874\n\n\n242\n2023-10-30\n8.603354\n263.812291\n866.654129\n757.601472\n27.081000\n0.983716\n0.183639\n788.486015\n262.828480\n290.616882\n3.378339\n66.827993\n-754.223132\n1.356785\n178.170521\n1052.272015\n\n\n243\n2023-10-31\n9.032917\n311.552394\n866.654129\n803.841970\n35.649472\n11.163953\n0.269313\n901.165305\n300.388379\n319.795146\n5.027126\n67.972347\n-798.814844\n30.217549\n166.649785\n1142.661342\n\n\n244\n2023-11-01\n8.743938\n211.339011\n866.654129\n898.594991\n30.389375\n3.655479\n0.189410\n623.050504\n207.683462\n245.486118\n2.309883\n50.624125\n-896.285107\n0.213375\n145.941312\n1134.978184\n\n\n\n\n\n\n\n\ndef data_scaler(df):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n\n    numeric_columns = df.select_dtypes(include=['number']).columns\n        \n    # Scale numeric columns\n    df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n\n    return df\n\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='y')\n\ndef altair_plot(df_plot): \n    import altair as alt\n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], bind='legend', nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='y:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\n\nwide_df = data_scaler(wide_df)\nwide_df.tail()\n\n\n\n\n\n\n\n\nds\nAircon\nConsumption\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\nHouseBoardR\nImport\nLights\nNet\nOvenStove\nPowerpoints\nProduction\n\n\n\n\n240\n2023-10-28\n0.000327\n0.295413\n0.639383\n0.087269\n0.054518\n0.475787\n0.000988\n0.441317\n0.270647\n0.263152\n0.098826\n0.397064\n0.702175\n0.217335\n0.451213\n0.164710\n\n\n241\n2023-10-29\n0.000459\n0.096052\n0.639383\n0.390543\n0.008991\n0.049113\n0.000167\n0.155476\n0.095349\n0.096613\n0.001842\n0.287229\n0.452268\n0.003559\n0.195780\n0.385757\n\n\n242\n2023-10-30\n0.000367\n0.190600\n0.639383\n0.366814\n0.066865\n0.014087\n0.000146\n0.312670\n0.191751\n0.186659\n0.003790\n0.324314\n0.470363\n0.003900\n0.457440\n0.400433\n\n\n243\n2023-10-31\n0.001045\n0.244137\n0.639383\n0.389203\n0.133461\n0.218148\n0.000354\n0.381277\n0.233826\n0.216107\n0.006078\n0.330630\n0.454358\n0.099064\n0.417095\n0.438445\n\n\n244\n2023-11-01\n0.000589\n0.131755\n0.639383\n0.435080\n0.092579\n0.067642\n0.000160\n0.211942\n0.129978\n0.141110\n0.002308\n0.234883\n0.419373\n0.000130\n0.344577\n0.435214\n\n\n\n\n\n\n\n\ndf = long_form(wide_df)\nwide_df.rename(columns={'Consumption': 'y'}, inplace=True)\nwide_df['unique_id'] = 'Consumption'\ndf.head()\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-03-02\nAircon\n0.007134\n\n\n1\n2023-03-03\nAircon\n0.178748\n\n\n2\n2023-03-04\nAircon\n0.338990\n\n\n3\n2023-03-05\nAircon\n0.173844\n\n\n4\n2023-03-06\nAircon\n0.363831"
  },
  {
    "objectID": "consumption_model.html#weather-data",
    "href": "consumption_model.html#weather-data",
    "title": "Consumption Model for a Month",
    "section": "Weather data",
    "text": "Weather data\n\nimport pandas as pd\npath = 'Data'\nname = 'weather'\n\nweather_df = pd.read_csv(f'{path}/{name}.csv')\nweather_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01 00:00\n21.6\n0\n12.6\n126\n1013.0\n0.0\n84\n73\n21.6\n21.6\n24.4\n18.7\n0\n10.0\n19.8\n1.0\n\n\n1\n2023-01-01 01:00\n21.3\n0\n11.9\n133\n1013.0\n0.0\n85\n77\n21.3\n21.3\n24.3\n18.7\n0\n10.0\n18.7\n1.0\n\n\n2\n2023-01-01 02:00\n21.1\n0\n11.2\n140\n1012.0\n0.0\n86\n80\n21.1\n21.1\n24.2\n18.7\n0\n10.0\n17.6\n1.0\n\n\n3\n2023-01-01 03:00\n20.8\n0\n10.4\n147\n1012.0\n0.0\n88\n84\n20.8\n20.8\n20.8\n18.7\n0\n10.0\n16.6\n1.0\n\n\n4\n2023-01-01 04:00\n21.0\n0\n10.6\n144\n1012.0\n0.0\n86\n84\n21.0\n21.0\n21.0\n18.6\n0\n10.0\n16.3\n1.0\n\n\n\n\n\n\n\n\nweather_df = data_scaler(weather_df)\n\n\ndef data_day_avg(weather_df):\n    from tqdm.notebook import tqdm\n    avg_df = pd.DataFrame()\n    dates = pd.to_datetime(weather_df['ds']).dt.date.unique()\n    \n    for date in tqdm(dates):\n        filtered_df = weather_df[pd.to_datetime(weather_df['ds']).dt.date == date]\n        ds = filtered_df.pop('ds')\n\n        filtered_df = pd.DataFrame(filtered_df.mean()).T\n        filtered_df\n\n        filtered_df.insert(0, 'ds', date)\n\n        avg_df = pd.concat([avg_df, filtered_df], ignore_index=True)\n    \n    return avg_df\n\n\nweather_avg_df = data_day_avg(weather_df)\n\n\n\n\n\nweather_avg_df['ds'] = pd.to_datetime(weather_avg_df['ds'])\n\n\nweather_avg_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-01-01\n0.566503\n0.583333\n0.459098\n0.345636\n0.428728\n0.000895\n0.670886\n0.583750\n0.516791\n0.595085\n0.512881\n0.778121\n0.0\n1.000000\n0.434678\n0.422619\n\n\n1\n2023-01-02\n0.554902\n0.583333\n0.451453\n0.396936\n0.359649\n0.001253\n0.602848\n0.409167\n0.499005\n0.584249\n0.486067\n0.725707\n0.0\n1.000000\n0.434844\n0.428571\n\n\n2\n2023-01-03\n0.582516\n0.583333\n0.263889\n0.415506\n0.258772\n0.000000\n0.584916\n0.287917\n0.525746\n0.610043\n0.502366\n0.738663\n0.0\n1.000000\n0.283533\n0.452381\n\n\n3\n2023-01-04\n0.650490\n0.583333\n0.313073\n0.382660\n0.163377\n0.000895\n0.687764\n0.196250\n0.608582\n0.673535\n0.609621\n0.866902\n0.0\n0.994792\n0.305777\n0.500000\n\n\n4\n2023-01-05\n0.654575\n0.583333\n0.257518\n0.506267\n0.166667\n0.026859\n0.735232\n0.700417\n0.629726\n0.677350\n0.608701\n0.911808\n0.0\n0.989583\n0.288098\n0.416667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\n2023-12-06\n0.580556\n0.583333\n0.254077\n0.264740\n0.558114\n0.000000\n0.608650\n0.174583\n0.519527\n0.608364\n0.515116\n0.745878\n0.0\n1.000000\n0.214558\n0.470238\n\n\n340\n2023-12-07\n0.605065\n0.583333\n0.266565\n0.202066\n0.565789\n0.000000\n0.582806\n0.050000\n0.543532\n0.631258\n0.533386\n0.748233\n0.0\n1.000000\n0.222693\n0.476190\n\n\n341\n2023-12-08\n0.618954\n0.583333\n0.349261\n0.251625\n0.575658\n0.000018\n0.626582\n0.342083\n0.561816\n0.644078\n0.555205\n0.795200\n0.0\n1.000000\n0.285940\n0.476190\n\n\n342\n2023-12-09\n0.612582\n0.583333\n0.362768\n0.190460\n0.544956\n0.000107\n0.577532\n0.497500\n0.564179\n0.638584\n0.539432\n0.757509\n0.0\n1.000000\n0.308267\n0.428571\n\n\n343\n2023-12-10\n0.630556\n0.583333\n0.513507\n0.233751\n0.530702\n0.000233\n0.515295\n0.537500\n0.571269\n0.655067\n0.546924\n0.730271\n0.0\n1.000000\n0.426627\n0.452381\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n(\n    wide_df[['y']].plot(title='Consumption')\n)\n\n&lt;Axes: title={'center': 'Consumption'}&gt;\n\n\n\n\n\n\nfrom statsforecast import StatsForecast\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/core.py:25: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (15,4)\nStatsForecast.plot(df, engine='plotly')\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/_plotly_utils/basevalidators.py:105: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\n\nwide_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 245 entries, 0 to 244\nData columns (total 18 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   ds                  245 non-null    datetime64[ns]\n 1   Aircon              245 non-null    float64       \n 2   y                   245 non-null    float64       \n 3   ConsumptionNet      198 non-null    float64       \n 4   Export              245 non-null    float64       \n 5   Fridge              245 non-null    float64       \n 6   Garage              245 non-null    float64       \n 7   Hotwater            245 non-null    float64       \n 8   HouseBoardOutside   161 non-null    float64       \n 9   HouseBoardOutsideR  245 non-null    float64       \n 10  HouseBoardR         245 non-null    float64       \n 11  Import              245 non-null    float64       \n 12  Lights              245 non-null    float64       \n 13  Net                 245 non-null    float64       \n 14  OvenStove           245 non-null    float64       \n 15  Powerpoints         245 non-null    float64       \n 16  Production          245 non-null    float64       \n 17  unique_id           245 non-null    object        \ndtypes: datetime64[ns](1), float64(16), object(1)\nmemory usage: 34.6+ KB\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3920 entries, 0 to 3919\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   ds         3920 non-null   datetime64[ns]\n 1   unique_id  3920 non-null   object        \n 2   y          3789 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 92.0+ KB\n\n\n\nweather_avg_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   ds              344 non-null    datetime64[ns]\n 1   temp_c          344 non-null    float64       \n 2   is_day          344 non-null    float64       \n 3   wind_kph        344 non-null    float64       \n 4   wind_degree     344 non-null    float64       \n 5   pressure_mb     344 non-null    float64       \n 6   precip_mm       344 non-null    float64       \n 7   humidity        344 non-null    float64       \n 8   cloud           344 non-null    float64       \n 9   feelslike_c     344 non-null    float64       \n 10  windchill_c     344 non-null    float64       \n 11  heatindex_c     344 non-null    float64       \n 12  dewpoint_c      344 non-null    float64       \n 13  chance_of_rain  344 non-null    float64       \n 14  vis_km          344 non-null    float64       \n 15  gust_kph        344 non-null    float64       \n 16  uv              344 non-null    float64       \ndtypes: datetime64[ns](1), float64(16)\nmemory usage: 45.8 KB\n\n\n\ndate_range_start, date_range_end  = wide_df['ds'].min(), wide_df['ds'].max()\n# Filter rows between the start and end dates\nweather_avg_filtered_df = weather_avg_df[(weather_avg_df['ds'] &gt;= date_range_start) & (weather_avg_df['ds'] &lt;= date_range_end)]\nweather_avg_filtered_df\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n300\n2023-10-28\n0.379575\n0.541667\n0.700051\n0.446611\n0.708333\n0.000233\n0.500000\n0.389167\n0.351741\n0.414835\n0.327681\n0.497939\n0.0\n1.0\n0.588811\n0.345238\n\n\n301\n2023-10-29\n0.382516\n0.541667\n0.383028\n0.320682\n0.656798\n0.000125\n0.524789\n0.433750\n0.347637\n0.423230\n0.329390\n0.525324\n0.0\n1.0\n0.328934\n0.345238\n\n\n302\n2023-10-30\n0.441340\n0.583333\n0.396152\n0.435237\n0.562500\n0.000000\n0.583861\n0.181250\n0.403483\n0.478327\n0.373686\n0.611307\n0.0\n1.0\n0.340139\n0.416667\n\n\n303\n2023-10-31\n0.550163\n0.583333\n0.489424\n0.549559\n0.442982\n0.000000\n0.570675\n0.025417\n0.492537\n0.579976\n0.467140\n0.687132\n0.0\n1.0\n0.470286\n0.464286\n\n\n304\n2023-11-01\n0.510784\n0.583333\n0.561672\n0.445218\n0.505482\n0.000340\n0.599156\n0.480000\n0.458209\n0.543040\n0.435594\n0.685218\n0.0\n1.0\n0.467795\n0.404762\n\n\n\n\n245 rows × 17 columns\n\n\n\n\naltair_plot(df)\n\n\n\n\n\n\n\n\nweather_avg_df_plot = long_form(weather_avg_filtered_df)\naltair_plot(weather_avg_df_plot)\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nwide_df[\"y\"]\n\n0      0.355157\n1      0.291076\n2      0.498418\n3      0.418824\n4      0.561599\n         ...   \n240    0.295413\n241    0.096052\n242    0.190600\n243    0.244137\n244    0.131755\nName: y, Length: 245, dtype: float64\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=28, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=28, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(wide_df[\"y\"],  lags=14, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(wide_df[\"y\"],  lags=14, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(wide_df[\"y\"], model = \"additive\", period=28)\na.plot();"
  },
  {
    "objectID": "consumption_model.html#designing-models",
    "href": "consumption_model.html#designing-models",
    "title": "Consumption Model for a Month",
    "section": "Designing Models",
    "text": "Designing Models\n\nweather_avg_filtered_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n60\n2023-03-02\n0.644444\n0.541667\n0.294980\n0.153784\n0.344298\n0.001056\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n61\n2023-03-03\n0.603268\n0.541667\n0.483563\n0.386142\n0.403509\n0.002131\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n62\n2023-03-04\n0.561765\n0.541667\n0.574669\n0.433612\n0.425439\n0.000161\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n63\n2023-03-05\n0.598039\n0.541667\n0.372324\n0.350627\n0.427632\n0.000358\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n64\n2023-03-06\n0.629575\n0.541667\n0.313073\n0.290854\n0.417763\n0.000358\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n\n\n\n\nwide_df = pd.merge(wide_df, weather_avg_filtered_df, on='ds', how='outer')\n\n\nwide_df.head()\n\n\n\n\n\n\n\n\nds\nAircon\ny\nConsumptionNet\nExport\nFridge\nGarage\nHotwater\nHouseBoardOutside\nHouseBoardOutsideR\n...\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\n\n\n\n\n0\n2023-03-02\n0.007134\n0.355157\nNaN\n0.016595\n0.814609\n0.003794\n0.000287\nNaN\n0.356702\n...\n0.675633\n0.345417\n0.609826\n0.667888\n0.594374\n0.845112\n0.0\n1.0\n0.250332\n0.470238\n\n\n1\n2023-03-03\n0.178748\n0.291076\nNaN\n0.165011\n0.669709\n0.015031\n0.000143\nNaN\n0.292065\n...\n0.682489\n0.579167\n0.572388\n0.629426\n0.548107\n0.821555\n0.0\n1.0\n0.432022\n0.380952\n\n\n2\n2023-03-04\n0.338990\n0.498418\nNaN\n0.294620\n0.686509\n0.059559\n0.000460\nNaN\n0.496691\n...\n0.604430\n0.364583\n0.509204\n0.590659\n0.491982\n0.734982\n0.0\n1.0\n0.485309\n0.422619\n\n\n3\n2023-03-05\n0.173844\n0.418824\nNaN\n0.302607\n0.701288\n0.009916\n0.000256\nNaN\n0.419958\n...\n0.603903\n0.401250\n0.542537\n0.624542\n0.531677\n0.761042\n0.0\n1.0\n0.319804\n0.428571\n\n\n4\n2023-03-06\n0.363831\n0.561599\nNaN\n0.381220\n0.773714\n0.008798\n0.002112\nNaN\n0.562640\n...\n0.608650\n0.157500\n0.568284\n0.653999\n0.557965\n0.785925\n0.0\n1.0\n0.263363\n0.476190\n\n\n\n\n5 rows × 34 columns\n\n\n\n\nhorizon = 28 * 2\n\ntrain_size = len(wide_df) - horizon\ntrain, test = wide_df[:train_size], wide_df[train_size:]\n\n\nwide_df.shape, train.shape, test.shape\n\n((245, 34), (189, 34), (56, 34))\n\n\n\nfutr_df=weather_avg_filtered_df[train_size:]\nfutr_df['unique_id'] = 'Consumption'\nfutr_df.shape\n\n/tmp/ipykernel_1270693/3311167277.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n(56, 18)\n\n\n\nfutr_df.head()\n\n\n\n\n\n\n\n\nds\ntemp_c\nis_day\nwind_kph\nwind_degree\npressure_mb\nprecip_mm\nhumidity\ncloud\nfeelslike_c\nwindchill_c\nheatindex_c\ndewpoint_c\nchance_of_rain\nvis_km\ngust_kph\nuv\nunique_id\n\n\n\n\n249\n2023-09-07\n0.430882\n0.5\n0.251784\n0.298630\n0.699561\n0.000000\n0.748418\n0.322083\n0.397637\n0.468407\n0.371320\n0.703475\n0.0\n1.000000\n0.223274\n0.345238\nConsumption\n\n\n250\n2023-09-08\n0.433660\n0.5\n0.348114\n0.424559\n0.656798\n0.001612\n0.732595\n0.320417\n0.394776\n0.471001\n0.371845\n0.694052\n0.0\n0.989583\n0.321547\n0.345238\nConsumption\n\n\n251\n2023-09-09\n0.338889\n0.5\n0.389016\n0.465761\n0.732456\n0.000358\n0.443038\n0.115833\n0.308706\n0.378816\n0.283123\n0.415931\n0.0\n1.000000\n0.370020\n0.309524\nConsumption\n\n\n252\n2023-09-10\n0.292810\n0.5\n0.490571\n0.406685\n0.789474\n0.000000\n0.489979\n0.264167\n0.268159\n0.329060\n0.235410\n0.428445\n0.0\n1.000000\n0.432852\n0.267857\nConsumption\n\n\n253\n2023-09-11\n0.326471\n0.5\n0.583716\n0.429085\n0.822368\n0.000000\n0.456751\n0.294583\n0.298881\n0.366758\n0.262487\n0.430654\n0.0\n1.000000\n0.511288\n0.303571\nConsumption\n\n\n\n\n\n\n\n\nfrom neuralforecast import models\nmodels.__all__\n\n['RNN',\n 'GRU',\n 'LSTM',\n 'TCN',\n 'DeepAR',\n 'DilatedRNN',\n 'MLP',\n 'NHITS',\n 'NBEATS',\n 'NBEATSx',\n 'TFT',\n 'VanillaTransformer',\n 'Informer',\n 'Autoformer',\n 'PatchTST',\n 'FEDformer',\n 'StemGNN',\n 'HINT',\n 'TimesNet']\n\n\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import *\n\n\nmodel_list = [RNN,\n GRU,\n LSTM,\n TCN,\n #DeepAR,  # not good\n DilatedRNN,\n MLP,\n NHITS,\n NBEATS,\n NBEATSx,\n #TFT, # too much GPU\n #VanillaTransformer, # not good\n #Informer,\n #Autoformer,\n #PatchTST, # too much GPU\n #FEDformer, #taken too long\n #StemGNN, #need n_series\n #HINT, #need n_series\n #TimesNet # takes too long\n             ]\n\n\nwide_df.columns\n\nIndex(['ds', 'Aircon', 'y', 'ConsumptionNet', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'unique_id', 'temp_c', 'is_day', 'wind_kph', 'wind_degree',\n       'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n       'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km',\n       'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nmodels = [model(input_size=2 * horizon,\n                h=horizon,\n                max_steps=50,\n                hist_exog_list =['Aircon', 'Export', 'Fridge', 'Garage',\n                                 'Hotwater', \n                                 'HouseBoardR','Import', 'Lights', 'Net', 'OvenStove',\n                                 'Powerpoints', 'Production'\n                                ],\n                futr_exog_list = ['temp_c', 'is_day', 'wind_kph', 'wind_degree',\n                    'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n                    'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain',\n                    'vis_km', 'gust_kph', 'uv']\n               ) for model in model_list]\nmodels\n\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n[RNN, GRU, LSTM, TCN, DilatedRNN, MLP, NHITS, NBEATS, NBEATSx]\n\n\n\nnf = NeuralForecast(\n    models=models, \n    freq='D')\n\n\ntrain.columns\n\nIndex(['ds', 'Aircon', 'y', 'ConsumptionNet', 'Export', 'Fridge', 'Garage',\n       'Hotwater', 'HouseBoardOutside', 'HouseBoardOutsideR', 'HouseBoardR',\n       'Import', 'Lights', 'Net', 'OvenStove', 'Powerpoints', 'Production',\n       'unique_id', 'temp_c', 'is_day', 'wind_kph', 'wind_degree',\n       'pressure_mb', 'precip_mm', 'humidity', 'cloud', 'feelslike_c',\n       'windchill_c', 'heatindex_c', 'dewpoint_c', 'chance_of_rain', 'vis_km',\n       'gust_kph', 'uv'],\n      dtype='object')\n\n\n\nnf.fit(df=train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconsumption_predict = nf.predict(futr_df=futr_df).reset_index()\nconsumption_predict.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\nConsumption\n2023-09-07\n0.520451\n0.558734\n0.566012\n0.552208\n0.565061\n0.398763\n0.235965\n0.266845\n0.296138\n\n\n1\nConsumption\n2023-09-08\n0.493308\n0.514300\n0.395049\n0.494308\n0.495290\n0.387929\n0.224346\n0.191190\n0.212631\n\n\n2\nConsumption\n2023-09-09\n0.425334\n0.355697\n0.433629\n0.400758\n0.381633\n0.408256\n0.241371\n0.233107\n0.228564\n\n\n3\nConsumption\n2023-09-10\n0.443577\n0.498908\n0.442869\n0.366124\n0.378713\n0.407515\n0.233445\n0.234206\n0.226971\n\n\n4\nConsumption\n2023-09-11\n0.353572\n0.434181\n0.394170\n0.391079\n0.314454\n0.401142\n0.212357\n0.289439\n0.254854\n\n\n\n\n\n\n\n\nconsumption_predict.drop('unique_id', axis=1, inplace=True)\n\njust_consumption = df[df['unique_id'] == 'Consumption']\n\n\nconsumption_predict_plot = long_form(consumption_predict)\nconsumption_predict_plot = pd.concat([consumption_predict_plot, just_consumption]\n                                     , ignore_index=True)\nconsumption_predict_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-09-07\nRNN\n0.520451\n\n\n1\n2023-09-08\nRNN\n0.493308\n\n\n2\n2023-09-09\nRNN\n0.425334\n\n\n3\n2023-09-10\nRNN\n0.443577\n\n\n4\n2023-09-11\nRNN\n0.353572\n\n\n...\n...\n...\n...\n\n\n744\n2023-10-28\nConsumption\n0.295413\n\n\n745\n2023-10-29\nConsumption\n0.096052\n\n\n746\n2023-10-30\nConsumption\n0.190600\n\n\n747\n2023-10-31\nConsumption\n0.244137\n\n\n748\n2023-11-01\nConsumption\n0.131755\n\n\n\n\n749 rows × 3 columns\n\n\n\n\naltair_plot(consumption_predict_plot)"
  },
  {
    "objectID": "consumption_model.html#performance",
    "href": "consumption_model.html#performance",
    "title": "Consumption Model for a Month",
    "section": "Performance",
    "text": "Performance\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\nconsumption_predict.info(), consumption_predict.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 56 entries, 0 to 55\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   ds          56 non-null     datetime64[ns]\n 1   RNN         56 non-null     float64       \n 2   GRU         56 non-null     float64       \n 3   LSTM        56 non-null     float64       \n 4   TCN         56 non-null     float64       \n 5   DilatedRNN  56 non-null     float64       \n 6   MLP         56 non-null     float64       \n 7   NHITS       56 non-null     float64       \n 8   NBEATS      56 non-null     float64       \n 9   NBEATSx     56 non-null     float64       \ndtypes: datetime64[ns](1), float64(9)\nmemory usage: 4.5 KB\n\n\n(None,\n Index(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n        'NBEATS', 'NBEATSx'],\n       dtype='object'))\n\n\n\ndef evaluate_performace(y_hist, y_true, y_pred):\n    #y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    print(y_pred.columns)\n    for i, model in enumerate(y_pred.columns):\n        if i &lt; 1: continue\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                    y_pred[model].values, \n                                                    y_hist['y'].values, seasonality=12)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_pred[model].values)\n    return pd.DataFrame(evaluation)\n\n\nscore = evaluate_performace(train, test, consumption_predict)\nscore.style.background_gradient(cmap ='YlGn', axis = 1,low=0.5, high=0.6)\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\n\n\n\n\n \nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\nmase\n0.708069\n0.753612\n0.653206\n0.715462\n0.683945\n0.841424\n0.585120\n0.644987\n0.540987\n\n\nmae\n0.116573\n0.124071\n0.107540\n0.117790\n0.112601\n0.138528\n0.096331\n0.106187\n0.089065\n\n\nmape\n117.972952\n123.586528\n92.569209\n117.289240\n113.036232\n141.320639\n63.815475\n109.551444\n72.312216\n\n\nrmse\n0.144370\n0.158492\n0.138631\n0.148289\n0.142224\n0.161854\n0.127389\n0.136658\n0.117224\n\n\nsmape\n53.985201\n55.843691\n52.182055\n53.097440\n52.532795\n59.642131\n62.462127\n55.998253\n53.806687\n\n\n\n\n\n\nscore = data_scaler(score.T)\nscore.plot(kind='bar', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nresults = evaluate_performace(train, test, consumption_predict)\nresults.to_csv('Data/Neutral_results5.csv')\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')"
  },
  {
    "objectID": "consumption_model.html#cross_validate",
    "href": "consumption_model.html#cross_validate",
    "title": "Consumption Model for a Month",
    "section": "Cross_Validate",
    "text": "Cross_Validate\n\ncv_df = nf.cross_validation(train,\n                            n_windows=3,\n                            step_size= 1,\n                           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncv_df.columns\n\nIndex(['unique_id', 'ds', 'cutoff', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN',\n       'MLP', 'NHITS', 'NBEATS', 'NBEATSx', 'Aircon', 'y', 'ConsumptionNet',\n       'Export', 'Fridge', 'Garage', 'Hotwater', 'HouseBoardOutside',\n       'HouseBoardOutsideR', 'HouseBoardR', 'Import', 'Lights', 'Net',\n       'OvenStove', 'Powerpoints', 'Production', 'temp_c', 'is_day',\n       'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',\n       'cloud', 'feelslike_c', 'windchill_c', 'heatindex_c', 'dewpoint_c',\n       'chance_of_rain', 'vis_km', 'gust_kph', 'uv'],\n      dtype='object')\n\n\n\ncv_df.drop('cutoff', axis=1, inplace=True)\ncv_df.drop('unique_id', axis=1, inplace=True)\n\n\ncv_df['RNN']\n\n0      0.435481\n1      0.571911\n2      0.712023\n3      0.549668\n4      0.572973\n         ...   \n163    0.331751\n164    0.253077\n165    0.284961\n166    0.376310\n167    0.242600\nName: RNN, Length: 168, dtype: float32\n\n\n\nconsumption_predict.columns\n\nIndex(['ds', 'RNN', 'GRU', 'LSTM', 'TCN', 'DilatedRNN', 'MLP', 'NHITS',\n       'NBEATS', 'NBEATSx'],\n      dtype='object')\n\n\n\ncv_df_filter = cv_df[consumption_predict.columns]\ncv_df_filter\n\n\n\n\n\n\n\n\nds\nRNN\nGRU\nLSTM\nTCN\nDilatedRNN\nMLP\nNHITS\nNBEATS\nNBEATSx\n\n\n\n\n0\n2023-07-11\n0.435481\n0.441562\n0.498433\n0.483206\n0.500122\n0.580595\n0.490572\n0.561005\n0.524275\n\n\n1\n2023-07-12\n0.571911\n0.569870\n0.579989\n0.628650\n0.598320\n0.679515\n0.452369\n0.516658\n0.289955\n\n\n2\n2023-07-13\n0.712023\n0.640818\n0.719223\n0.676610\n0.605447\n0.670375\n0.338826\n0.413797\n0.194005\n\n\n3\n2023-07-14\n0.549668\n0.612540\n0.583302\n0.630741\n0.591179\n0.687667\n0.341010\n0.349947\n0.284295\n\n\n4\n2023-07-15\n0.572973\n0.515143\n0.499026\n0.550076\n0.550768\n0.460477\n0.225505\n0.478275\n0.333824\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n163\n2023-09-02\n0.331751\n0.273870\n0.252772\n0.315278\n0.250863\n0.163495\n0.206029\n0.343418\n0.262445\n\n\n164\n2023-09-03\n0.253077\n0.223345\n0.258903\n0.295469\n0.289489\n0.261679\n0.284491\n0.301202\n0.263286\n\n\n165\n2023-09-04\n0.284961\n0.200487\n0.221326\n0.324679\n0.294298\n0.254948\n0.422089\n0.328173\n0.222025\n\n\n166\n2023-09-05\n0.376310\n0.339444\n0.321259\n0.406183\n0.381957\n0.062036\n0.235567\n0.484579\n-0.211019\n\n\n167\n2023-09-06\n0.242600\n0.215008\n0.229029\n0.244884\n0.185244\n0.396192\n0.361900\n0.446674\n0.201929\n\n\n\n\n168 rows × 10 columns\n\n\n\n\ncv_df_plot = long_form(cv_df_filter)\n\n\ncv_df_plot\n\n\n\n\n\n\n\n\nds\nunique_id\ny\n\n\n\n\n0\n2023-07-11\nRNN\n0.435481\n\n\n1\n2023-07-12\nRNN\n0.571911\n\n\n2\n2023-07-13\nRNN\n0.712023\n\n\n3\n2023-07-14\nRNN\n0.549668\n\n\n4\n2023-07-15\nRNN\n0.572973\n\n\n...\n...\n...\n...\n\n\n1507\n2023-09-02\nNBEATSx\n0.262445\n\n\n1508\n2023-09-03\nNBEATSx\n0.263286\n\n\n1509\n2023-09-04\nNBEATSx\n0.222025\n\n\n1510\n2023-09-05\nNBEATSx\n-0.211019\n\n\n1511\n2023-09-06\nNBEATSx\n0.201929\n\n\n\n\n1512 rows × 3 columns\n\n\n\n\naltair_plot(cv_df_plot)"
  }
]